# GTC 2025 Overview & Takeaways

[GTC Conference](https://www.nvidia.com/gtc/)

# Content

> Chinese: 1-100

> English: 101-200

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [1. AI 创业企业在中国的发展与助力 [S73846]](#1-ai-s73846)
- [2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]](#2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942)
- [3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]](#3-mobile-agent-s72561)
- [4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]](#4-ai-s73910)
- [5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]](#5-s72500)
- [6. 大语言模型在智能座舱中的应用 [S72716]](#6-s72716)
- [7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]](#7-s72635)
- [8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]](#8-cpugpu-s71579)
- [9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]](#9-ufo-lite-s72498)
- [10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]](#10-vla-s72557)
- [11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]](#11-overlap-llm-s72643)
- [12. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]](#12-tensorrt-llm-s74181)
- [13. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]](#13-laiye-ai-foundry-nvidia-ai-enterprise-s72276)
- [14. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]](#14-s72639)
- [15. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]](#15-s74073)
- [16. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]](#16-tensorrt-llm-s72995)
- [101. Enable Intelligent Storage to Process Data for AI Applications [S71937]](#101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937)

<!-- TOC end -->

---

[Chinese]

<!-- TOC --><a name="1-ai-s73846"></a>
# 1. AI 创业企业在中国的发展与助力 [S73846]
DataMesh 
- 数字孪生，帮助企业低成本整合数据。制造业为主。
- 业务逻辑ground truth + AI Omniverse数字孪生=降本增效
- 时空一致性是关键

逐际动力Limx Dynamics
- 基于人类操作视频数据生成大模型的机器人具身操作算法。

途深智合
- 蛋白质多模态大模型辅助自动化蛋白质工程。
- 多模态自然语言对话+全流程业务模型

<!-- TOC --><a name="2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942"></a>
# 2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]

He Wang, Galbot

- Vision-Language-Action (VLA) Model. Recent progress: $\pi_0$ model, for robot control. Bottleneck: data collection.
- Galbot's alternative data approach: Synthetic data can be scalable. NaVid (RSS 2024). Incorporate Depth: NaVid-4D (ICRA 2025).
- GraspVLA. Knowledge can be shared. 3D modality enhances spatial intelligence. Emergent abilities to learn new skills.

<!-- TOC --><a name="3-mobile-agent-s72561"></a>
# 3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]

Ji Zhang, Alibaba Qwen

- GUI Agent: 理解UI、定位操作、多步操作规划反思、操作延时
- Multi agents (planning + decision + reflection) with knowledge injection + 端云协同模型架构。
- 知识增强：操作逻辑 tips（根据用户query加载知识） + 操作步骤增强 shortcuts（理解UI），可点餐订火车票机票酒店。自我进化：用两个experience reflectors根据操作记录和错误日志，优化更新tips & shortcuts。
- 复杂任务拆解：separate high level planning & low level action, improve long-horizon planning & error recovery.

<!-- TOC --><a name="4-ai-s73910"></a>
# 4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]

NVIDIA NeMo: Every enterprise needs their data flywheel. 

Model customization: prompt engineering/learning + fine tuning (LoRA, Adapter, IA3, SFT, RLHF)

爱动超越
- 工业车辆端侧大模型Agent工作流，ADAS智驾

Paxini
- 机器人ITPU多维触觉+视觉触觉双模态感知。

<!-- TOC --><a name="5-s72500"></a>
# 5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]

王光甫，长城汽车
- 多路并行召回，从不同维度获取候选文档，merge候选池后进行精排rank，根据基座大模型总结输出。
- Contrastive loss，拉近正确样本对的语义距离，扩大错误样本对的差异。
- 对基座模型进行蒸馏，将DeepSeek-R1教师模型迁移到学生模型上。

Advanced RAG Pipeline
- Multimodal PDF Data Extraction & Document Parsing.
- Finetune embedding model for improved retrieval. E.g., automatic training of contrastive learning loss between positive/negative chunks of user query sampling, via NeMO.
- Contextual retrieval, build context for each chunk to connect the relationship of each chunk and include global information.
- Semantic splitter, use LLM to segment documents for semantically coherent content grouping.
- Query decomposition, break down difficult query to subqueries.

<!-- TOC --><a name="6-s72716"></a>
# 6. 大语言模型在智能座舱中的应用 [S72716]

高杰，蔚来汽车

Emotional Intelligence, E2E multi-modal diaglog

<!-- TOC --><a name="7-s72635"></a>
# 7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]

王小刚，绝影

- 叛逆进化：不是听话的工具循规蹈矩的助手，而是提供主动温暖关怀的新成员。
- Flash Decoding. Increase concurrency and improve GPU utilization.
- Segment prefill. Change management of KV cache, reuse computation results.
- Continue batching.

<!-- TOC --><a name="8-cpugpu-s71579"></a>
# 8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]

周小华，DolphinDB

- 高性能分布式时序数据库
- Pipeline流水线优化，多线程拷贝，提升内存带宽
- 雪球期权定价，适合GPU

<img src="https://github.com/user-attachments/assets/6a71af90-5e76-4e1b-bc13-2896902bd045" width="50%" height="50%">

- RAG: VectorDB + TextDB

<!-- TOC --><a name="9-ufo-lite-s72498"></a>
# 9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]

Teng Xi, Baidu. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727321215230001Bhv2/FinalPresPDF/S72498_1741760325928001kU80.pdf)

多模态大语言模型（MLLMs）展示了卓越的能力和强大的泛化能力。然而，目前的 MLLMs 往往难以满足快速响应的需求，推理延迟成为其在现实应用中的一个重要的挑战。本讲座将介绍 UFO-Lite，这是 VIMER-UFO MLLMs 系列的最新快速版本，其针对现实场景中的高效化部署进行了优化。具体而言，UFO-Lite 引入了自推测解码机制，显著的减少了端到端的延迟，且准确性几乎没有损失。它采用了新颖的双 LLM 结构，将自回归生成任务中序列化的多次前向推理卸载到了快速分支（即草稿模型），并通过原始模型对草稿序列进行并行的验证，以保持精度。为了实现自推测解码，UFO-Lite 基于量化感知的知识蒸馏，有效地开发了双 LLM 的快速分支，确保其分布与原始模型相似且具备较高的推理速度。此外，UFO-Lite 还提出了基于置信度的自适应切换，利用动态验证窗口大小，而不是固定大小，并兼容短序列生成。UFO-Lite 在 AI2D 和 MathVista 数据集上展示了与 Qwen2-VL-7B 和 InternVL2-8B 相当的性能。在 MMMU 上，它实现了与 InterVL2-8B、MiniPCPMV2-2_6 和 GLM-4V-9B 相当的结果。与上述模型相比，UFO-lite 可以加速 2 倍以上。此演讲可以为进一步发展高效且有效的多模态大语言模型提供有价值的见解。


- First work on MLLMs-SPD (Speculative Decoding) and orthogonal to previous work on LLMs
- Implement based on TensorRT-LLM，极致推理速度
- Can be applied to any state-of-the-art MLLMs，任意大小tokenizer、对任何大模型都能加速且精度相近(量化+蒸馏)
- Enhanced with FP8 and deployed on L20
- 多模态：小模型自回归k次迭代，大模型一次并行处理
- Draft-then-Verification Paradigm of Speculative Decoding 大小模型协同，交替推理
- Confidence-based Adaptive switching strategy: 小模型做错/幻觉了被大模型修正，小模型不够自信让大模型验证。
- Moreover, can Quantization-Aware Knowledge Model-Distillation
- DeepSeek-V3/R1: Multi-Token Prediction (MTP)可以直接让小模型预测速度翻倍。

<!-- TOC --><a name="10-vla-s72557"></a>
# 10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]

Peng Jia, Li Auto. 

- VLA based on Fast and Slow thinking systems
- Sparse attention can reduce the reasoning burden, MoE can control the growth of params and increase model capability.

<!-- TOC --><a name="11-overlap-llm-s72643"></a>
# 11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]

Baichuan AI. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727598580103001ix9j/FinalPresPDF/S72643_1741684006928001iRRh.pdf)

- 投机采样：利用decode过程算力冗余，生成多个候选token输入给LLM并行验证，充分使用算力且不增加时延。难点是draft model为候选token的产生方式。
  - 使用投机采样优化 decode 阶段效率问题，通过设计高命中率低成本的模型结构及动态的候选 token tree 结构，提升投机采样有效性
  - 采用计算通信 overlap 优化通信占比大场景下 prefill 效率问题，通过创新的序列内 overlap 提升计算利用率，从而降低 prefill 阶段耗时。
- Clover2模型，类RNN架构，轻量。能超越Eagle。
  - 优化loss
  - 主模型预测token信息前置，提前加入transformer block帮助更远预测
  - regressive attention block output projector结构由medusa的ResBlock改为FC，提升后几个head预测能力
  - 增加augmenting block层数，提高信息提取能力
- 工程实现（推理两阶段）
  - Prefill阶段（计算密集型）：输出token给draft model，draft model循环每个head动态sample出tree，对tree给decode并行验证。需要多batch高效动态sample，构建tree mask。
  - Decode阶段（内存访问密集型）：并行推理给sample， 然后验证，最后一个验证无效的token重复给draft model生成token tree。需要attention支持tree mask，KV Cache管理。
- 工程优化
  - 探索ISO(序列内计算通信overlap)策略在LLM推理期间如何提升计算利用率，从而节省大量首token耗时

<!-- TOC --><a name="12-tensorrt-llm-s74181"></a>
# 12. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]

Kuaishou

- 引入Consistency Model通过蒸馏，推理仅需1步（diffusion model很慢推理要25步，加速了25倍）：One-step inference of PTM, Compute distillation loss, EMA weight updating. 

<!-- TOC --><a name="13-laiye-ai-foundry-nvidia-ai-enterprise-s72276"></a>
# 13. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]

[Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726190594557001j0HX/FinalPresPDF/S72276_Laiye%20AI%20Foundry_NVIDIA%20AI%20Enterprise_1741682513554001WKy5.pdf)

- Hybrid RAG = GraphRAG (Node Embedding/PageRank) + VectorRAG (semantic chunking)
- PEFT enhancements: Adaptive Rank on LoRA, Adapters Diversifying, Hybriding（学习专有知识+共有知识，每个专家专注自己领域知识）

<!-- TOC --><a name="14-s72639"></a>
# 14. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]

Alibaba Cloud

Stable Diffusion引入latent space在VAE减少计算，速度提高7倍。Pipeline三个部分：文本编码器CLIP、VAE潜空间图像翻译为输出图像、UNet多步推理模型（95%计算量，要算子性能优化：矩阵乘和卷积）。

<!-- TOC --><a name="15-s74073"></a>
# 15. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]

Meituan, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734091150868001dcjp/FinalPresPDF/S74073_1741683377945001YCts.pdf)

- 下一代推荐模型，Gennerative Recommenders (GR) Model: HSTU (Hierarchical Sequential Transduction Units)：把召回和排序都嵌入这个序列生成模型，将长序列输入Transformer进行信息交互。
- 美团模型工程优化：
  - 混合并行策略 = Sparse模型并行 + Dense模型并行
  - 功能支持：动态hash table支持动态扩容（如商品频繁上下线），梯度累积（大batch size训练）
  - 性能优化：自动合表，混合精度训练，kernel优化，变长序列负载均衡（用户序列明显长尾分布）。
  - Inference优化：User Cache 同一用户不同request之间序列特征基本相同可复用，先读用户KV Cache再计算attention。
- 未来：online learning场景、Embedding压缩 + 低精度训练优化性能、模型并行支持更大规模GR、AOT compile支持PyTorch2

<!-- TOC --><a name="16-tensorrt-llm-s72995"></a>
# 16. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]

JD, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727962469507001ov1v/FinalPresPDF/S72995_1741644906381001kPbt.pdf)

单节点，算力释放
- TensorRT-LLM最佳实践，耗时约束下吞吐量最优化
- 基于广告场域特性优化，有限表征下吞吐优化

分布式，软硬协同
- 异构硬件分布式推理，CPU&GPU异构部署
- KV Memory Tool，KV中心，三级缓存，异步加载，增量更新

# 17. 



---

[English]

<!-- TOC --><a name="101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937"></a>
# 101. Enable Intelligent Storage to Process Data for AI Applications [S71937]

Vincent Hsu, IBM

- Content-aware Storage (CAS) GenAI semantic search: Unstructured data is unsearchable, but we can integrate GenAI into our storage, so as not to repeatedly copying data from vector store all the time with lots of money.

# 102. Mistral AI: Placing Frontier AI in Your Hands [S73942]

Arthur Mensch, Mistral AI CEO

Enterprise AI strategy
- Private architecture:
  - SaaS-like simplicity on-premises and in-VPC. Everything is stateful.
  - Deep observability and robust tooling.
  - Continuous infra optimization.
- E2E customizability pipeline:
  - Custom models (full/continued pretraining).
  - Rigorous post-training (SFT, DPO, alignment). 
  - Plug-n-play platform modules.
<img src="https://github.com/user-attachments/assets/d58400a0-c272-4e14-9ef0-bbcc75339227" width="80%" height="80%">

- Enterprise context:
  - Integrations to enterprise knowledge.
  - Continually updating context.
  - Easy-to-build agents.




