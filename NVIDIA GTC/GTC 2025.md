# GTC 2025 Overview & Takeaways

[GTC Conference 2025](https://www.nvidia.com/gtc/)

## Content

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [Chinese: 1-100](#chinese-1-100)
   * [1. AI 创业企业在中国的发展与助力 [S73846]](#1-ai-s73846)
   * [2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]](#2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942)
   * [3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]](#3-mobile-agent-s72561)
   * [4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]](#4-ai-s73910)
   * [5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]](#5-s72500)
   * [6. 大语言模型在智能座舱中的应用 [S72716]](#6-s72716)
   * [7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]](#7-s72635)
   * [8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]](#8-cpugpu-s71579)
   * [9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]](#9-ufo-lite-s72498)
   * [10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]](#10-vla-s72557)
   * [11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]](#11-overlap-llm-s72643)
   * [12. 构建以 Megatron-Core 为核心的大语言模型训练加速生态 [S72580]](#12-megatron-core-s72580)
   * [13. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]](#13-tensorrt-llm-s74181)
   * [14. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]](#14-laiye-ai-foundry-nvidia-ai-enterprise-s72276)
   * [15. LLM 2-bit 后量化的加速与部署实践 [S72647]](#15-llm-2-bit-s72647)
   * [16. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]](#16-s72639)
   * [17. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]](#17-s74073)
   * [18. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]](#18-tensorrt-llm-s72995)
- [English: 101-200](#english-101-200)
   * [101. Enable Intelligent Storage to Process Data for AI Applications [S71937]](#101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937)
   * [102. Mistral AI: Placing Frontier AI in Your Hands [S73942]](#102-mistral-ai-placing-frontier-ai-in-your-hands-s73942)
   * [103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]](#103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619)
   * [104. GTC 2025 Keynote [S72484]](#104-gtc-2025-keynote-s72484)
   * [105. AI for Safe and Efficient Trading in Electronic Markets [S72692]](#105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692)
   * [106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]](#106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694)
   * [107. Google AI: Research, Progress, and the Future of Ads [S73303]](#107-google-ai-research-progress-and-the-future-of-ads-s73303)
   * [108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]](#108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208)
   * [109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]](#109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854)
   * [110. Distributed Agentic Multi-Modal LLM Deployments [S72654]](#110-distributed-agentic-multi-modal-llm-deployments-s72654)
   * [111. AI Agents in Production: Insights and Future Directions [S72884]](#111-ai-agents-in-production-insights-and-future-directions-s72884)
   * [112. Insights From NVIDIA Research [S73202]](#112-insights-from-nvidia-research-s73202)
      + [GPU performance bottlenecks for LLM workloads with Optimizations for Memory](#gpu-performance-bottlenecks-for-llm-workloads-with-optimizations-for-memory)
      + [Hybrid-head LLMs and Hymba Architecture](#hybrid-head-llms-and-hymba-architecture)
   * [113. Scaling Vision LLMs to extract and deploy targeted metadata for Catalog Management [S73701]](#113-scaling-vision-llms-to-extract-and-deploy-targeted-metadata-for-catalog-management-s73701)
   * [114. Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]](#114-productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818)
   * [115. An Introduction to Building Humanoid Robots [S72590]](#115-an-introduction-to-building-humanoid-robots-s72590)
   * [116. Leveraging Large Model-Based Embodied Intelligence to Enhance Financial Service Robots [S71267]](#116-leveraging-large-model-based-embodied-intelligence-to-enhance-financial-service-robots-s71267)
   * [117. AI for Humanoid Robots [S73182]](#117-ai-for-humanoid-robots-s73182)
   * [118. Jane Street: How an Early AI Adopter Thinks About Infrastructure (Presented by CoreWeave) [S74219]](#118-jane-street-how-an-early-ai-adopter-thinks-about-infrastructure-presented-by-coreweave-s74219)
   * [119. The Promise of Humanoid Robots: Research vs. the Real World [S72592]](#119-the-promise-of-humanoid-robots-research-vs-the-real-world-s72592)
   * [120. A New Era of Generalist Robotics: The Rise of Humanoids [S72543]](#120-a-new-era-of-generalist-robotics-the-rise-of-humanoids-s72543)
   * [121. Tencent HunYuan: Building a High-Performance Inference Engine for Large Models Based on NVIDIA TensorRT-LLM [S71563]](#121-tencent-hunyuan-building-a-high-performance-inference-engine-for-large-models-based-on-nvidia-tensorrt-llm-s71563)
   * [122. Accelerate Super Long-Context LLM Inference [S72568]](#122-accelerate-super-long-context-llm-inference-s72568)
   * [123. Scaling AI Platform at LinkedIn: LLMs, Agents, GPUs, Kernels, and More [S72963]](#123-scaling-ai-platform-at-linkedin-llms-agents-gpus-kernels-and-more-s72963)
   * [124. Streamlining Investment Insights for Wealth Management with Generative AI [S71653]](#124-streamlining-investment-insights-for-wealth-management-with-generative-ai-s71653)
   * [125. Unlocking the Future of LLMs: High-Efficiency xLSTM Architectures for Scalable Performance [S71812]](#125-unlocking-the-future-of-llms-high-efficiency-xlstm-architectures-for-scalable-performance-s71812)
   * [126. The Future of AI: Scaling Intelligence, Open-Source Innovation, and Human-AI Collaboration [S73863]](#126-the-future-of-ai-scaling-intelligence-open-source-innovation-and-human-ai-collaboration-s73863)
   * [Unlocking High-Performance AI Applications at Airbnb [S73265]](#unlocking-high-performance-ai-applications-at-airbnb-s73265)
   * [FlashAttention-3: Fast and Accurate Attention With Asynchrony and Low Precision [S71368]](#flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision-s71368)
   * [Toward Multidisciplinary Scientific Foundation Models [S72256]](#toward-multidisciplinary-scientific-foundation-models-s72256)
   * [Scaling Distributed AI Applications with Ray [S71766]](#scaling-distributed-ai-applications-with-ray-s71766)

<!-- TOC end -->

---

<!-- TOC --><a name="chinese-1-100"></a>
# Chinese: 1-100

<!-- TOC --><a name="1-ai-s73846"></a>
## 1. AI 创业企业在中国的发展与助力 [S73846]
> DataMesh 
- 数字孪生，帮助企业低成本整合数据。制造业为主。
- 业务逻辑ground truth + AI Omniverse数字孪生=降本增效
- 时空一致性是关键

> Limx Dynamics
- 基于人类操作视频数据生成大模型的机器人具身操作算法。

> 途深智合
- 蛋白质多模态大模型辅助自动化蛋白质工程。
- 多模态自然语言对话+全流程业务模型

<!-- TOC --><a name="2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942"></a>
## 2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]

> He Wang, Galbot

- Vision-Language-Action (VLA) Model. Recent progress: $\pi_0$ model, for robot control. Bottleneck: data collection.
- Galbot's alternative data approach: Synthetic data can be scalable. NaVid (RSS 2024). Incorporate Depth: NaVid-4D (ICRA 2025).
- GraspVLA. Knowledge can be shared. 3D modality enhances spatial intelligence. Emergent abilities to learn new skills.

<!-- TOC --><a name="3-mobile-agent-s72561"></a>
## 3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]

> Ji Zhang, Alibaba Qwen

- GUI Agent: 理解UI、定位操作、多步操作规划反思、操作延时
- Multi agents (planning + decision + reflection) with knowledge injection + 端云协同模型架构。
- 知识增强：操作逻辑 tips（根据用户query加载知识） + 操作步骤增强 shortcuts（理解UI），可点餐订火车票机票酒店。自我进化：用两个experience reflectors根据操作记录和错误日志，优化更新tips & shortcuts。
- 复杂任务拆解：separate high level planning & low level action, improve long-horizon planning & error recovery.

<!-- TOC --><a name="4-ai-s73910"></a>
## 4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]

- NVIDIA NeMo: Every enterprise needs their data flywheel (continuous learning loops for AI agents). 
- Model customization: prompt engineering/learning + fine tuning (LoRA, Adapter, IA3, SFT, RLHF)

> 爱动超越
- 工业车辆端侧大模型Agent工作流，ADAS智驾

> Paxini
- 机器人ITPU多维触觉+视觉触觉双模态感知。

<!-- TOC --><a name="5-s72500"></a>
## 5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]

> 王光甫, Great Wall Motor
- 多路并行召回，从不同维度获取候选文档，merge候选池后进行精排rank，根据基座大模型总结输出。
- Contrastive loss，拉近正确样本对的语义距离，扩大错误样本对的差异。
- 对基座模型进行蒸馏，将DeepSeek-R1教师模型迁移到学生模型上。

Advanced RAG Pipeline
- Multimodal PDF Data Extraction & Document Parsing.
- Finetune embedding model for improved retrieval. E.g., automatic training of contrastive learning loss between positive/negative chunks of user query sampling, via NeMO.
- Contextual retrieval, build context for each chunk to connect the relationship of each chunk and include global information.
- Semantic splitter, use LLM to segment documents for semantically coherent content grouping.
- Query decomposition, break down difficult query to subqueries.

<!-- TOC --><a name="6-s72716"></a>
## 6. 大语言模型在智能座舱中的应用 [S72716]

> 高杰, NIO

Emotional Intelligence, E2E multi-modal diaglog

<!-- TOC --><a name="7-s72635"></a>
## 7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]

> 王小刚，SenseAuto

- 叛逆进化：不是听话的工具循规蹈矩的助手，而是提供主动温暖关怀的新成员。
- Flash Decoding. Increase concurrency and improve GPU utilization.
- Segment prefill. Change management of KV cache, reuse computation results.
- Continue batching.

<!-- TOC --><a name="8-cpugpu-s71579"></a>
## 8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]

> 周小华，DolphinDB

- 高性能分布式时序数据库
- Pipeline流水线优化，多线程拷贝，提升内存带宽
- 雪球期权定价，适合GPU

<img src="https://github.com/user-attachments/assets/6a71af90-5e76-4e1b-bc13-2896902bd045" width="50%" height="50%">

- RAG: VectorDB + TextDB

<!-- TOC --><a name="9-ufo-lite-s72498"></a>
## 9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]

> Teng Xi, Baidu. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727321215230001Bhv2/FinalPresPDF/S72498_1741760325928001kU80.pdf)

多模态大语言模型（MLLMs）展示了卓越的能力和强大的泛化能力。然而，目前的 MLLMs 往往难以满足快速响应的需求，推理延迟成为其在现实应用中的一个重要的挑战。本讲座将介绍 UFO-Lite，这是 VIMER-UFO MLLMs 系列的最新快速版本，其针对现实场景中的高效化部署进行了优化。具体而言，UFO-Lite 引入了自推测解码机制，显著的减少了端到端的延迟，且准确性几乎没有损失。它采用了新颖的双 LLM 结构，将自回归生成任务中序列化的多次前向推理卸载到了快速分支（即草稿模型），并通过原始模型对草稿序列进行并行的验证，以保持精度。为了实现自推测解码，UFO-Lite 基于量化感知的知识蒸馏，有效地开发了双 LLM 的快速分支，确保其分布与原始模型相似且具备较高的推理速度。此外，UFO-Lite 还提出了基于置信度的自适应切换，利用动态验证窗口大小，而不是固定大小，并兼容短序列生成。UFO-Lite 在 AI2D 和 MathVista 数据集上展示了与 Qwen2-VL-7B 和 InternVL2-8B 相当的性能。在 MMMU 上，它实现了与 InterVL2-8B、MiniPCPMV2-2_6 和 GLM-4V-9B 相当的结果。与上述模型相比，UFO-lite 可以加速 2 倍以上。此演讲可以为进一步发展高效且有效的多模态大语言模型提供有价值的见解。


- First work on MLLMs-SPD (Speculative Decoding) and orthogonal to previous work on LLMs
- Implement based on TensorRT-LLM，极致推理速度
- Can be applied to any state-of-the-art MLLMs，任意大小tokenizer、对任何大模型都能加速且精度相近(量化+蒸馏)
- Enhanced with FP8 and deployed on L20
- 多模态：小模型自回归k次迭代，大模型一次并行处理
- Draft-then-Verification Paradigm of Speculative Decoding 大小模型协同，交替推理
- Confidence-based Adaptive switching strategy: 小模型做错/幻觉了被大模型修正，小模型不够自信让大模型验证。
- Moreover, can Quantization-Aware Knowledge Model-Distillation
- DeepSeek-V3/R1: Multi-Token Prediction (MTP)可以直接让小模型预测速度翻倍。

<!-- TOC --><a name="10-vla-s72557"></a>
## 10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]

> Peng Jia, Li Auto. 

- VLA based on Fast and Slow thinking systems
- Sparse attention can reduce the reasoning burden, MoE can control the growth of params and increase model capability.

<!-- TOC --><a name="11-overlap-llm-s72643"></a>
## 11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]

> Baichuan AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727598580103001ix9j/FinalPresPDF/S72643_1741684006928001iRRh.pdf)

- 投机采样：利用decode过程算力冗余，生成多个候选token输入给LLM并行验证，充分使用算力且不增加时延。难点是draft model为候选token的产生方式。
  - 使用投机采样优化 decode 阶段效率问题，通过设计高命中率低成本的模型结构及动态的候选 token tree 结构，提升投机采样有效性
  - 采用计算通信 overlap 优化通信占比大场景下 prefill 效率问题，通过创新的序列内 overlap 提升计算利用率，从而降低 prefill 阶段耗时。
- Clover2模型，类RNN架构，轻量。能超越Eagle。
  - 优化loss
  - 主模型预测token信息前置，提前加入transformer block帮助更远预测
  - regressive attention block output projector结构由medusa的ResBlock改为FC，提升后几个head预测能力
  - 增加augmenting block层数，提高信息提取能力
- 工程实现（推理两阶段）
  - Prefill阶段（计算密集型）：输出token给draft model，draft model循环每个head动态sample出tree，对tree给decode并行验证。需要多batch高效动态sample，构建tree mask。
  - Decode阶段（内存访问密集型）：并行推理给sample， 然后验证，最后一个验证无效的token重复给draft model生成token tree。需要attention支持tree mask，KV Cache管理。
- 工程优化
  - 探索ISO(序列内计算通信overlap)策略在LLM推理期间如何提升计算利用率，从而节省大量首token耗时

<!-- TOC --><a name="12-megatron-core-s72580"></a>
## 12. 构建以 Megatron-Core 为核心的大语言模型训练加速生态 [S72580]

> Alibaba Cloud, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727454091122001Qxbv/FinalPresPDF/S72580_1741685427734001itcO.pdf)

Megatron-Core 是 NVIDIA 开发的用于训练超大规模 Transformer 模型的分布式框架，具有出色的分布式性能，是训练具有数千亿或更多参数的大语言模型的必备工具。Pai-Megatron-patch 是阿里云旗下 PAI 平台开发的大语言模型训练工具包，包含基于 Megatron-Core构建高效 LLM 训练系统的关键组件，如 mcore 和 Huggingface 之间的双向 ckpt 转换，弥合 mcore 和 Huggingface 生态系统之间的差距；实现了 Distributed Optimizer CPU 卸载技术，进一步降低了大模型训练的成本；还开发了给定硬件资源条件下的自动超参数优化工具，提高了框架的可用性等功能。在此基础上，它提供了训练各种开源大语言模型的最佳实践。

- 填补了 Megatron 优化器在 GPU 资源有限时无法卸载的空白。 我们实现了具有高效 CPU 卸载的分布式优化器，这是一种用于训练的 GPU 显存优化技术，可在 GPU 资源有限的情况下训练长序列
- 支持 HuggingFace 和 MCore 模型之间的高精度双向 ckpt 转换。 更具体地说，我们可以对复杂的稠密 MoE 模型 (例如 Llama 3、DeepSeek-V2-MoE 或 Qwen-MoE 等) 进行 3D (张量并行/流水线并行/专家并行) 转换
- 针对热门的稠密 MoE 模型升级和简化训练最佳实践，提供易于使用并且非常强大的加速技术，例如 FlashAttention-3、TP-Comm-Overlapping



<!-- TOC --><a name="13-tensorrt-llm-s74181"></a>
## 13. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]

> Kuaishou

引入Consistency Model通过蒸馏，推理仅需1步（diffusion model很慢推理要25步，加速了25倍）：One-step inference of PTM, Compute distillation loss, EMA weight updating. 

<!-- TOC --><a name="14-laiye-ai-foundry-nvidia-ai-enterprise-s72276"></a>
## 14. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]

> Laiye AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726190594557001j0HX/FinalPresPDF/S72276_Laiye%20AI%20Foundry_NVIDIA%20AI%20Enterprise_1741682513554001WKy5.pdf)

- Hybrid RAG = GraphRAG (Node Embedding/PageRank) + VectorRAG (semantic chunking)
- PEFT enhancements: Adaptive Rank on LoRA, Adapters Diversifying, Hybriding（学习专有知识+共有知识，每个专家专注自己领域知识）

<!-- TOC --><a name="15-llm-2-bit-s72647"></a>
## 15. LLM 2-bit 后量化的加速与部署实践 [S72647]

> ByteDance, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727624187326001lFnY/FinalPresPDF/S72647_1741682615101001YdeF.pdf)

- LLM 高精度 2-bit 权重压缩。将模型参数解耦为整数和浮点部分，并通过对权重和 scale/zp 的交替迭代进行优化，求解局部极小值，最终在 Llama-1/2 7B~70B 的 2-bit 后量化上实现了 sota 精度。
- 2-bit 内存访问优势的新技术，并基于 TensorRT-LLM 中的 w4a16 Gemm 运算符开发 Gemm CUDA 内核，该内核可以高效加速 w2a16 模型的推理，并在 NVIDIA GPU 上实现 1.4 倍至 1.7 倍的加速。

<!-- TOC --><a name="16-s72639"></a>
## 16. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]

> Alibaba Cloud

Stable Diffusion引入latent space在VAE减少计算，速度提高7倍。Pipeline三个部分：文本编码器CLIP、VAE潜空间图像翻译为输出图像、UNet多步推理模型（95%计算量，要算子性能优化：矩阵乘和卷积）。

<!-- TOC --><a name="17-s74073"></a>
## 17. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]

> Meituan, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734091150868001dcjp/FinalPresPDF/S74073_1741683377945001YCts.pdf)

- 下一代推荐模型，Gennerative Recommenders (GR) Model: HSTU (Hierarchical Sequential Transduction Units)：把召回和排序都嵌入这个序列生成模型，将长序列输入Transformer进行信息交互。
- 美团模型工程优化：
  - 混合并行策略 = Sparse模型并行 + Dense模型并行
  - 功能支持：动态hash table支持动态扩容（如商品频繁上下线），梯度累积（大batch size训练）
  - 性能优化：自动合表，混合精度训练，kernel优化，变长序列负载均衡（用户序列明显长尾分布）。
  - Inference优化：User Cache 同一用户不同request之间序列特征基本相同可复用，先读用户KV Cache再计算attention。
- 未来：online learning场景、Embedding压缩 + 低精度训练优化性能、模型并行支持更大规模GR、AOT compile支持PyTorch2

<!-- TOC --><a name="18-tensorrt-llm-s72995"></a>
## 18. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]

> JD, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727962469507001ov1v/FinalPresPDF/S72995_1741644906381001kPbt.pdf)

单节点，算力释放
- TensorRT-LLM最佳实践，耗时约束下吞吐量最优化
- 基于广告场域特性优化，有限表征下吞吐优化

分布式，软硬协同
- 异构硬件分布式推理，CPU&GPU异构部署
- KV Memory Tool，KV中心，三级缓存，异步加载，增量更新

---

<!-- TOC --><a name="english-101-200"></a>
# English: 101-200

<!-- TOC --><a name="101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937"></a>
## 101. Enable Intelligent Storage to Process Data for AI Applications [S71937]

> Vincent Hsu, IBM Storage CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726155198693001lYd6/FinalPresPDF/S71937_1741204556803001g36M.pdf)

Content-aware Storage (CAS) GenAI semantic search: Unstructured data is unsearchable, but we can integrate GenAI into our storage, so as not to repeatedly copying data from vector store all the time with lots of money.

<img src="https://github.com/user-attachments/assets/0fdbfdff-55f5-44dc-a807-87016653ce9b" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/5d41a8af-31ff-48de-b095-ff4fa955e260" width="80%" height="80%">


<!-- TOC --><a name="102-mistral-ai-placing-frontier-ai-in-your-hands-s73942"></a>
## 102. Mistral AI: Placing Frontier AI in Your Hands [S73942]

> Arthur Mensch, Mistral AI CEO

Enterprise AI strategy
- Private architecture:
  - SaaS-like simplicity on-premises and in-VPC. Everything is stateful.
  - Deep observability and robust tooling.
  - Continuous infra optimization.
<img src="https://github.com/user-attachments/assets/bad09fb3-6651-4505-b975-f4ab65958b23" width="80%" height="80%">

- E2E customizability pipeline:
  - Custom models (full/continued pretraining).
  - Rigorous post-training (SFT, DPO, alignment). 
  - Plug-n-play platform modules.
<img src="https://github.com/user-attachments/assets/d58400a0-c272-4e14-9ef0-bbcc75339227" width="80%" height="80%">

- Enterprise context: connectors
  - Integrations to enterprise knowledge.
  - Continually updating context.
  - Easy-to-build agents.
<img src="https://github.com/user-attachments/assets/3afe719a-17d6-4b28-9906-3575eb4f0116" width="80%" height="80%">

Mistral Small 3.1, OCR and SOTA embedding models 
- Clever routing between Small and Large allows to bring the best user experience
- AI app building is about stitching bricks together: bring these bricks to customers and help them stitch it.
<img src="https://github.com/user-attachments/assets/196ed814-396e-4099-8713-1c95d5651c42" width="80%" height="80%">

<!-- TOC --><a name="103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619"></a>
## 103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]

> Joe Stam, Jump Trading, Head of Research Technology. [Slides1](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/DraftPres/S73619_draft.pdf_1738621036627001aBsx.pdf), [Slides2](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/supmat/MatMulSlides_old_for_v100_1729878858947001rxoU.pdf).

Key Takeaways
- Explore computation of expected speed-of-light performance of an algorithm on a particular GPU
- Optimization strategies for matrix multiplication
- Proper use of GPU memory hierarchy

Summary
- Many $O(n^3)$ problems are actually $AO(n^3) + BO(n)$, where $B \gg A$. Must fuse operations, eliminate memory trips.
- Solution: Use Shared Memory.
  - Shared memory is divided into 32 banks.
  - If multiple threads within a warp access the same bank, the warp will serialize.
  - Important when we need to handle different transpose orientations
- We need to use SMEM to batch incoming data. To saturate FMA, we need to batch multiple computations and with `.reuse`, maybe want multiple CTAs per SM. We want to use asynchronous copies.
- Performance keys: All GMEM coalesced. Use SMEM, no bank conflicts. Use Async Load to SMEM LDGSTS, 128 bit where possible. Free warp scheduler to issue FMAs - use 128 bit loads to registers. User register banking. Repeat Registers.

<!-- TOC --><a name="104-gtc-2025-keynote-s72484"></a>
## 104. GTC 2025 Keynote [S72484]

> Jensen Huang, NVIDIA CEO

- Every industry and company in the future that has factories will have two factories: factory for what they built, and factory for mathematics/AI.
- Use AI to recreate AI: Distillation process bootstraps a policy model, but complex scenarios require further tuning. Closed-loop training enables fine-tuning of policy models, digital twin and synthetic data generation enhances adaptability to diverse problems.
- Inference at scale is extreme computing, to maximize the Pareto Frontier curve of smart AI fast response + tokens per second throughput. Traditional LLMs are fast but wrong, reasoning models like DeepSeek-R1 produce more tokens thinking with good result.
- NVIDIA Dynamo, operating system of AI factory: Prefill is busy when doing deep research, and it's not generating many tokens. Decode is busy when chatting. So depending on workload, we dynamically put GPU to prefill/decode more or less, by pipeline parallel, expert parallel, tensor parallel, in-flight batching, disaggregated inferencing, and KV Cache (to route to the right GPU and manage through all the memory hierarchies).
- Pareto Frontier curve (flops * bandwidth): We need a programmable architecture that's as homogeneously fungible as possible, because the workload changes dramatically across the entire frontier.
- Storage will be reinvented from retrieval-based to semantic-based, and GPU-accelerated.
- Groot N1 for robots: Dual-system architecture for thinking fast and slow.

<!-- TOC --><a name="105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692"></a>
## 105. AI for Safe and Efficient Trading in Electronic Markets [S72692]

> Iain Dunning, Hudson River Trading, Head of AI Lab

- Provide liquidity and price discovery. Market data, beyond tick-level data order books, there's news feeds, announcements, alternative data.
- Optimize money rather than prediction results, so we have RL. RL targets real thing, but has lower signal to noise, and less compute efficiency. RL trains and tests the same thing, no need for splitting training and testing datasets.
- Build market digital twins, factory of market data, training models that learn intuitively to diverse datasets to be robust.
- LLM to predict next token (market price in next second, ~170B price tokens for 3K stocks 10 years tick data), generate multiple possible future trajectories by sample-backin sample-backin repeatedly.
- Desirable properties: probabilistic, non-degenerate, capture tails, likelihood estimation, understandable.
- Challenges: Alt data, Coherence, Tails (tail output with human-driven signals of how model behaves), Calibration, Actions, Inference.
- Massive parallel compute, on model training, model inference, simulation and generation.
- Backtest how different actions would affect market if taking them. Time horizon from intraday to single days.

<!-- TOC --><a name="106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694"></a>
## 106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]

> Jim Fan, NVIDIA Principal Research Scientist. Justin Johnson, World Labs Co-founder. Misha Laskin, Reflection AI CEO. Saad Godil, Hippocratic CTO. Pete Florence, Generalist AI CEO. Eric Jang, 1x Technologies VP of AI.

- World Labs: Allow people to generate worlds by inputting single image, and generate 3D consistent world for agents to move around and interact with visually.
- Reflection AI: Today's agent is still driven by person, how to get systems that autonomously reason and backtrack on their own, and self improve to solve tasks. Coding agent is important as embodied factor for agent computer to solve general problems.
- Hippocratic: Future is vertical models, copilot is not full potential of GenAI, we need Autopilot to provide abundance in scale. Another bet is voice.
- 1x Technologies: Let AI to think harder why they fail.

<!-- TOC --><a name="107-google-ai-research-progress-and-the-future-of-ads-s73303"></a>
## 107. Google AI: Research, Progress, and the Future of Ads [S73303]

> James Johnsrud, Google Research. Tris Warkentin, Google DeepMind.

- Use RL for cold-start challenge to help advertisers better optimize bids
  - Online Bidding under RoS Constraints without Knowing the Value
- Pushing the envelope of what's possible with LLMs, modeling architectures inspired by biology (store new info as long-term memory in neuron)
  - Titans, Learning to Memorize at Test Time

<!-- TOC --><a name="108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208"></a>
## 108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]

> Yann LeCun, Meta Chief AI Scientist, NYU. Bill Dally, NVIDIA Chief Scientist.

- 4 interesting problems: Understand physical world, have persistent memory, reason and plan.
- Token = finite set of discrete possibilities, but we don't know how to produce tokens with video (high dimensional and continuous), plus lots of things are unpredictable by pixel level (raw representation) but rather in latent space.
- Plan and Reason: We need a predictor, given state of the world, an action we imagine, can predict the next stage of the world, so we can plan a sequence of actions to arrive at outcome.
- Foundation models will be open-sourced, trained in distributed fashion around the world, having access to different subsets of data, and training a consensus model.
- Business model for AI startups: build specialized system for vertical applications, download Llama and fine-tune on proprietary data not to upload.
- Tradeoffs between training time and inference time: We expect a new model for reasoning in abstract space, because some reasoning thoughts are not related to language (token space) but rather done in abstract mental space.
  - JEPA, Joint Embedding Predictive Architecture, world models that learn abstract representations, capable of manipulating abstractions, reason and produce sequences of actions.
  - Use world model to do task, if zero-shot never encountered such task before, you don't have to be trained for this task, you can just do it without learning anything but just basic understanding of the world and planning abilities. If you do the tasks many times, it gets compiled into policy in your reactive system.
  - Two reasoning systems:
    - 1. Automatic subconscious reactive policy (LLM can do).
    - 2. Understand physical world. (We need a different architecture, not generative architecture, because language is discreet and simple, but the world is complicated) 
- Masked Autoencoder (MAE) reconstructs the full image from the corrupted/masked transformed version, taking both versions run through encoders and we train the representation of the full image from the representation of the corrupted one. It's joint embedding predictive architecture, with 2 latent spaces for 2 input clusters.
  - Cheaper for photos but failure for videos. VJEPA: predict on video, but at representation level, take a sliding window of 16 frames on video, can predict next few frames by minimizing prediction error (infant baby cognition).

<!-- TOC --><a name="109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854"></a>
## 109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]

> Pedro Vicente, BlackRock, Aladdin Principal AI Engineer. Shawn Simpson, BlackRock, AI Labs Senior Data Science Team Director. 

- Portfolio analysis with Aladdin Copilot, Agentic platform

<img src="https://github.com/user-attachments/assets/e5eb2335-0a66-4234-8075-9cc06b591dc2" width="80%" height="80%">

- Agents Orchestration Evaluation: Configuration layer can simulate user's environment up to the application, its context, history and expected response. Solution layer can solve the task with multiple steps separate non-related threads.
- E2E Eval is a fundamental part of CI/CD pipeline, can diagnose issues in day-to-day dev.
- NVIDIA NIM - Llama 3.3 70B. NeMo Guardrails offers easy management of prompts and configs, LangGraph creates complex workflows (full control over flow and state + custom communication protocol makes interacting with multi-agents workflows in any framework).

<!-- TOC --><a name="110-distributed-agentic-multi-modal-llm-deployments-s72654"></a>
## 110. Distributed Agentic Multi-Modal LLM Deployments [S72654]

> Jeff White, Dell Product CTO.

- Combine Generative, Discriminative, Casual techniques.
- Architecture focuses on federated learning to improve the performance of distributed system.
- Continuous federated learning reduces anomalies, and take response with perfect context to RAG database, which is hard for heuristic methods without federated learning.
- Future enterprise architectures for edge AI inference will be multi-modal. With core foundation model (FM)/LLM functions centralized (attention, MLP-FFNN, layer normalization, residual connection) and some functions distributed to edge (retrieval-augmented generation, input processing-tokenization/vector embedding/position encoding [maybe], guardrails, caching, inference optimization-sampling/beam search). In addition, in multi-modal architectures, convolutional neural networks (CNNs) will be integrated with the FM/LLM.

<img src="https://github.com/user-attachments/assets/c052e6d5-b90a-43b9-82ca-b156576e4b92" width="80%" height="80%">

<!-- TOC --><a name="111-ai-agents-in-production-insights-and-future-directions-s72884"></a>
## 111. AI Agents in Production: Insights and Future Directions [S72884]

> Harrison Chase, LangChain CEO

- Key Concepts
  - Chain = predetermined control flow, sequence of steps.
  - ReAct Agent = LLM running in a loop (e.g. with for loop, tool calling, observe response)
  - Planning step: Tree of Thoughts. Reflection step: Verbal RL.
  - Flow Engineering = Pre-processing + code iterations, with reasoning models
- LangGraph: Balancing agent control with agency.
  - Controllability: to define both explicit and implicit workflows
  - Persistence: to enhance human-agent/multi-agent interactions & fault tolerance
  - Human-in-the-loop: to facilitate human guidance
  - Streaming: to expose any event or token as it occurs
- LangGraph tools
  - LangGraph platform: enterprise use for multiple teams across the org.
  - LangGraph builder: generate scaffolding and implement business logics
  - LangGraph templates
  - LangSmith: unified testing & observability platform for improving visibility and monitoring/debugging LLM apps.
    
<img src="https://github.com/user-attachments/assets/6405ec66-46ce-4e4e-868b-f01ce47febee" width="80%" height="80%">

- Memory is different from knowledge: knowledge is fixed source of docs/info, memory is updates through user interactions.
  - 3 types
    - Semantic facts (things I learned in school)
    - Episodic experiences (thigns I did)
    - Procedural instructions (instincts or motor skills)
  - Memory is not just RAG, it uses LLM to reflect on user interactions. Agent UX is tightly coupled to memory in need of human feedback. 
  - Memory and prompt optimization are two sides of the same coin.
  - Agent UX is tightly coupled to memory in need of human feedback.
  - Long-term memory is nice-to-have and we still want low-level control over memory.
- Agent Protocol: interoperatability for LLM multi-agents, compatibility for:
  - Agents built with LangGraph or other frameworks
  - Homegrown agents
  - Vertical off the shelf vendor provided agents
- Multi Agent Prebuilt Architectures. Prebuilt packages: Supervisor and Swarm
- Blueprint: Structured report generation. Stacks to overcome challenges with agents in enterprise
  - NVIDIA NIMs: cheaply access best open source models
  - LangGraph: controllable agent framework, to perform well
  - LangGraph Platform: Agent-specific infra for deployment and management
  - LangSmith: Observability and evaluation for agents 
  
<img src="https://github.com/user-attachments/assets/ffd34136-bad1-4495-8430-16f217b89ba0" width="80%" height="80%">


<!-- TOC --><a name="112-insights-from-nvidia-research-s73202"></a>
## 112. Insights From NVIDIA Research [S73202]

> Bill Dally, NVIDIA Chief Scientist.

<!-- TOC --><a name="gpu-performance-bottlenecks-for-llm-workloads-with-optimizations-for-memory"></a>
### GPU performance bottlenecks for LLM workloads with Optimizations for Memory
- LLM training and inference prefill phase (parallel processing on input prompt tokens) are bound by computing large GEMM kernels in FFN layers
  - Solution: Cash-Aware GEMM kernels (CAGE), reordering dataflows in GEMM kernels to maximize data locality in L2 cache, to make better use of L2 cache leads to better power efficiency & performance for large GEMM kernels
- LLM inference decode phase (sequentially generating output tokens) is bound by Key-Value (KV) cache loarding in attention layers
  - Solution: RocketKV for training-free KV cache sparsity, for accurate estimate on critical KV cache tokens via multi-stage (prefill phase to coarse-grain permanent KV cache eviction, decode phase to fine-grain dynamic KV token selection), multi-dimensional approximation. 

<!-- TOC --><a name="hybrid-head-llms-and-hymba-architecture"></a>
### Hybrid-head LLMs and Hymba Architecture
<img src="https://github.com/user-attachments/assets/b8be9674-375e-4732-963c-6ef3b747fb2b" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/1749172b-3ffa-4d3d-9151-af2d1b3f9c04" width="80%" height="80%">

- Hybrid LLM: Hymba Architecture
  - Combined attention and state-space heads. Better results by combining layers in parallel, take state space model in pink, and regular transformer, we have sliding window layer (less expensive transformer), sharing the KV-Cache.

<img src="https://github.com/user-attachments/assets/237f6afb-08ac-41c2-bb83-db69682cde85" width="80%" height="80%">

- Attention much more efficient. Sliding window attention with state space head, we have much more full attention.

<img src="https://github.com/user-attachments/assets/b548836b-d7c5-4c64-b407-8f8c10a61c1d" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/eec6f61f-da52-473b-b252-bb96a93f0271" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/d7d84588-ae74-423f-a76b-0f05bbcb064c" width="80%" height="80%">


<!-- TOC --><a name="113-scaling-vision-llms-to-extract-and-deploy-targeted-metadata-for-catalog-management-s73701"></a>
## 113. Scaling Vision LLMs to extract and deploy targeted metadata for Catalog Management [S73701]

> Shopify

- Two stages of serving in production (classification then extract): low latency real time prediction, high throughput/volume near real time prediction (less strict SLA with ability for in-flight batching, and LLM as classifier).

<img src="https://github.com/user-attachments/assets/78f75cd9-34b1-4c42-a107-790dd0bf13a4" width="80%" height="80%">

<!-- TOC --><a name="114-productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818"></a>
## 114. Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]

> Dimitris Emmanoulopoulos, Barclays, Head of Machine Learning Technologies

- Experiment: Meta-Llama-3-70B, input tokens: 2048, output tokens: 1024, FP16, NVIDIA NIM: TensorRT-LLM
- Risk Workflow: Can use any LLM, OCR, Retriever, because NIMs containerize them.
<img src="https://github.com/user-attachments/assets/98865447-d920-49a2-80f2-761eb636a7d6" width="50%" height="50%">
<img src="https://github.com/user-attachments/assets/def1aa26-c761-4876-baff-5638743d0c2c" width="50%" height="50%">

<!-- TOC --><a name="115-an-introduction-to-building-humanoid-robots-s72590"></a>
## 115. An Introduction to Building Humanoid Robots [S72590]

> Jim Fan, Yuke Zhu. NVIDIA

- Groot N1: combination of continuous diffusion and discrete LLM, and combination of Dual Systems: System 1 (low-level closed-loop sensorimotor control), System 2 (high-level cognition, reason and plan).

<img src="https://github.com/user-attachments/assets/ea3b26f4-734a-4b72-8b7c-e6044ce94df3" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/7f39497c-c872-4f43-96cf-205da7f94248" width="80%" height="80%">


<!-- TOC --><a name="116-leveraging-large-model-based-embodied-intelligence-to-enhance-financial-service-robots-s71267"></a>
## 116. Leveraging Large Model-Based Embodied Intelligence to Enhance Financial Service Robots [S71267]

> Jianzong Wang, Ping An Technology Deputy Chief Engineer. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1725419593222001E00c/FinalPresPDF/S_71267_1741673499511001z8i1.pdf)

<img src="https://github.com/user-attachments/assets/e2c40869-f53e-404b-98f0-28adf2fd63d9" width="80%" height="80%">


<!-- TOC --><a name="117-ai-for-humanoid-robots-s73182"></a>
## 117. AI for Humanoid Robots [S73182]

> Pieter Abbeel, UC Berkeley

Body Transformer (BoT), Embodiment-aware architecture for policy learning
- A spatially modular architecture in place of a monolithic NN, can capture spatial dependencies in high-dimensional settings
  - inductive bias
  - computationally efficient
  - enable multi-frequency reasoning
  - localized "credit assignment"
- By exploiting the sparse nature of architecture, BoT achieves 2-3x reduction in computation cost. Improves imitation learning performance and scalability, reinforcement learning and transfers to real-world. 
<img src="https://github.com/user-attachments/assets/3ff646fc-c224-4d2a-a380-d00f541a4740" width="80%" height="80%">

- HumanoidBench
- Addressing high-dimensional action space: Hierarchical learning for long-horizon reasoning can solve complex tasks.

MuJoCo
- Can we train RL policies in simulation for safe and cheap data? MuJoCo playground, full-stack robotics suite, open-source library for GPU-accelerated robot learning and sim-to-real transfer, can even train on Colab notebook.
- Training process: Define task and (many) rewards. Starting training in simulation (PPO) > tune the rewards. Define learning curriculum. Add domain randomization. Transfer to real world.
- Limiations: JAX computation time scales with # possible contacts. JIT compilation slow. Reward shaping is still important.

<!-- TOC --><a name="118-jane-street-how-an-early-ai-adopter-thinks-about-infrastructure-presented-by-coreweave-s74219"></a>
## 118. Jane Street: How an Early AI Adopter Thinks About Infrastructure (Presented by CoreWeave) [S74219]

> Adam Canady, Jane Street. Peter Salanki, CoreWeave CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734382333688001wR6T/FinalPresPDF/S74219_1741645197269001Gqhs.pdf)

- Harnessing Cutting-Edge GPUs for Financial Innovation: CoreWeave’s infrastructure delivers Jane Street reliable access to the latest NVIDIA GPUs, enabling rapid training and fine-tuning of AI models to drive innovation in financial markets
- Scaling Infrastructure for Demanding AI Workloads: CoreWeave proactively builds out GPU capacity and optimizes data center designs, including liquid cooling and RDMA networking, to meet the unique challenges of large-scale workloads
- Ensuring Reliability in Mission-Critical Applications: CoreWeave’s automated node health checks, robust architecture, and continuous monitoring provide Jane Street with the stability needed to maintain uptime in the high-stakes world of quantitative trading

<!-- TOC --><a name="119-the-promise-of-humanoid-robots-research-vs-the-real-world-s72592"></a>
## 119. The Promise of Humanoid Robots: Research vs. the Real World [S72592]

> Aaron Saunders, Boston Dynamics CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727465141102001g41h/DraftPres/NVIDA_GTC_2025_Saunders_1741990990349001f3fa.pdf)

<img src="https://github.com/user-attachments/assets/fc9b6a73-994e-4207-9701-6e5396d5fe17" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/ce42b2b0-455c-4316-885f-a501d9c44f57" width="80%" height="80%">


<!-- TOC --><a name="120-a-new-era-of-generalist-robotics-the-rise-of-humanoids-s72543"></a>
## 120. A New Era of Generalist Robotics: The Rise of Humanoids [S72543]

> Deepak Pathak, Skild AI CEO. Tiffany Janzen, Tiffintech Founder. Jim Fan, NVIDIA Principal Research Scientist. Aaron Saunders, Boston Dynamics CTO. Pras Velagapudi, Agility Robotics CTO. Bernt Børnich, 1X CEO.

- Jim Fan on Groot's strategy: Model should be simple, while data pipeline (all that surrounded by model) should be complicated. All complicated data strategies, we compress them into one clean artifact suffices for a wide range of tasks.
- Bernt Børnich: Intelligence comes from diversity, which is important for robots to understand tasks. Robots can learn its own dynamics history.
- Deepak Pathak: It's not that language leads to intelligence, but it's infrastructure exists in our brain came to physical reasoning. Let robots learn by experience, rather than control robots. Robot is learning agent, learning on the fly, which is different from train-deploy schemes in other AI/LLMs. 
- Jim Fan on future 5-20 years: Embodied scaling law for robotics. Robotics-accelerated and -automated science. Robotics automating robotics itself and self-improve recursively (like LLM's AutoML, prompt LLMs to deeper research finding next Transformer).

<!-- TOC --><a name="121-tencent-hunyuan-building-a-high-performance-inference-engine-for-large-models-based-on-nvidia-tensorrt-llm-s71563"></a>
## 121. Tencent HunYuan: Building a High-Performance Inference Engine for Large Models Based on NVIDIA TensorRT-LLM [S71563]

> Yifu Sun, Tencent Applied Researcher. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726046708673001W9i7/FinalPresPDF/GTC-yifu-Tencent%20HunYuan%20Building%20a%20High-Performance%20Inference%20Engine%20for%20Large%20Models%20Leveraging%20NVIDIA%20TensorRT-LLM_1741670208137001HVLE.pdf)

- Performance Optimization: Strategies for effectively tuning TensorRT-LLM’s out-of-the-box performance to the maximum, along with customized optimization approaches
- Model Compression Techniques: An exploration of various methods for model compression, and the tangible benefits derived from their implementation
- Parallel Inference: Strategies for achieving high-performance inference of ultra-large mixture-of-experts models across multiple GPUs and nodes
- KV-cache Optimization: Efficient generation and loading of KV-cache, in conjunction with algorithmic customizations
- Service Scheduling Strategies: Implementation of flexible service scheduling strategies based on NVIDIA Triton, enabling both continuous batching and prefill/decode separation

<img src="https://github.com/user-attachments/assets/5f256ab0-d284-4e13-bf66-c8eb72877eee" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/87347234-3d7f-49d0-8771-9171e95e9a7a" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/b12ef26d-8c3c-4504-bcee-4bc2fe1ac961" width="80%" height="80%">

<!-- TOC --><a name="122-accelerate-super-long-context-llm-inference-s72568"></a>
## 122. Accelerate Super Long-Context LLM Inference [S72568]

> Boyuan Huang, Alibaba Cloud Product Director. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727451588919001blpQ/FinalPresPDF/GTC2025_v1.1_1741657177860001TsJU.pdf)

- 

<!-- TOC --><a name="123-scaling-ai-platform-at-linkedin-llms-agents-gpus-kernels-and-more-s72963"></a>
## 123. Scaling AI Platform at LinkedIn: LLMs, Agents, GPUs, Kernels, and More [S72963]

> Animesh Singh, LinkedIn AI Executive Director

<!-- TOC --><a name="124-streamlining-investment-insights-for-wealth-management-with-generative-ai-s71653"></a>
## 124. Streamlining Investment Insights for Wealth Management with Generative AI [S71653]

> Orest Xherija, UBS Director


<!-- TOC --><a name="125-unlocking-the-future-of-llms-high-efficiency-xlstm-architectures-for-scalable-performance-s71812"></a>
## 125. Unlocking the Future of LLMs: High-Efficiency xLSTM Architectures for Scalable Performance [S71812]

> Sepp Hochreiter, NXAI Chief Scientist. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726132188712001KFs0/FinalPresPDF/S71812_1741601965571001LoNm.pdf)

- LSTM limitations
  - Can't revise storage decisions, can only do nearest neighbor search. Solution: exponential gating
  - Limited storage capacities, rare token predictions. Solution: Matrix memory & covariance update
  - No parallelization for training. Solution: update is memory independent.
- xLSTM (7B model) can beat Transformer, and is the fastest model, can deal with long context processing.
- Can be used for predictive analysis, robotics and real-time simulations.
- Chunk-wise kernel is faster than flash-attention.

<!-- TOC --><a name="126-the-future-of-ai-scaling-intelligence-open-source-innovation-and-human-ai-collaboration-s73863"></a>
## 126. The Future of AI: Scaling Intelligence, Open-Source Innovation, and Human-AI Collaboration [S73863]

> Ali Farhadi, Allen Institute for AI CEO

<!-- TOC --><a name="unlocking-high-performance-ai-applications-at-airbnb-s73265"></a>
## Unlocking High-Performance AI Applications at Airbnb [S73265]

> Airbnb

<!-- TOC --><a name="flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision-s71368"></a>
## FlashAttention-3: Fast and Accurate Attention With Asynchrony and Low Precision [S71368]

> Tri Dao, Together AI Chief Scientist

<!-- TOC --><a name="toward-multidisciplinary-scientific-foundation-models-s72256"></a>
## Toward Multidisciplinary Scientific Foundation Models [S72256]

> National Center for Scientific Research (CNRS)

<!-- TOC --><a name="scaling-distributed-ai-applications-with-ray-s71766"></a>
## Scaling Distributed AI Applications with Ray [S71766]

> Stephanie Wang, Anyscale Founding Engineer













