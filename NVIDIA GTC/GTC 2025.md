# GTC 2025 Overview & Takeaways

[GTC Conference](https://www.nvidia.com/gtc/)

## Content

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [Chinese: 1-100](#chinese-1-100)
   * [1. AI 创业企业在中国的发展与助力 [S73846]](#1-ai-s73846)
   * [2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]](#2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942)
   * [3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]](#3-mobile-agent-s72561)
   * [4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]](#4-ai-s73910)
   * [5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]](#5-s72500)
   * [6. 大语言模型在智能座舱中的应用 [S72716]](#6-s72716)
   * [7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]](#7-s72635)
   * [8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]](#8-cpugpu-s71579)
   * [9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]](#9-ufo-lite-s72498)
   * [10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]](#10-vla-s72557)
   * [11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]](#11-overlap-llm-s72643)
   * [12. 构建以 Megatron-Core 为核心的大语言模型训练加速生态 [S72580]](#12-megatron-core-s72580)
   * [13. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]](#13-tensorrt-llm-s74181)
   * [14. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]](#14-laiye-ai-foundry-nvidia-ai-enterprise-s72276)
   * [15. LLM 2-bit 后量化的加速与部署实践 [S72647]](#15-llm-2-bit-s72647)
   * [16. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]](#16-s72639)
   * [17. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]](#17-s74073)
   * [18. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]](#18-tensorrt-llm-s72995)
- [English: 101-200](#english-101-200)
   * [101. Enable Intelligent Storage to Process Data for AI Applications [S71937]](#101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937)
   * [102. Mistral AI: Placing Frontier AI in Your Hands [S73942]](#102-mistral-ai-placing-frontier-ai-in-your-hands-s73942)
   * [103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]](#103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619)
   * [104. GTC 2025 Keynote [S72484]](#104-gtc-2025-keynote-s72484)
   * [105. AI for Safe and Efficient Trading in Electronic Markets [S72692]](#105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692)
   * [106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]](#106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694)
   * [107. Google AI: Research, Progress, and the Future of Ads [S73303]](#107-google-ai-research-progress-and-the-future-of-ads-s73303)
   * [108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]](#108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208)
   * [109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]](#109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854)
   * [110. Distributed Agentic Multi-Modal LLM Deployments [S72654]](#110-distributed-agentic-multi-modal-llm-deployments-s72654)
   * [Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]](#productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818)
   * [AI for Humanoid Robots [S73182]](#ai-for-humanoid-robots-s73182)

<!-- TOC end -->

---

<!-- TOC --><a name="chinese-1-100"></a>
# Chinese: 1-100

<!-- TOC --><a name="1-ai-s73846"></a>
## 1. AI 创业企业在中国的发展与助力 [S73846]
> DataMesh 
- 数字孪生，帮助企业低成本整合数据。制造业为主。
- 业务逻辑ground truth + AI Omniverse数字孪生=降本增效
- 时空一致性是关键

> 逐际动力Limx Dynamics
- 基于人类操作视频数据生成大模型的机器人具身操作算法。

> 途深智合
- 蛋白质多模态大模型辅助自动化蛋白质工程。
- 多模态自然语言对话+全流程业务模型

<!-- TOC --><a name="2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942"></a>
## 2. The embodied end-to-end VLA model driven by synthetic big data 合成大数据驱动的具身端到端 VLA 大模型 [S71942]

> He Wang, Galbot

- Vision-Language-Action (VLA) Model. Recent progress: $\pi_0$ model, for robot control. Bottleneck: data collection.
- Galbot's alternative data approach: Synthetic data can be scalable. NaVid (RSS 2024). Incorporate Depth: NaVid-4D (ICRA 2025).
- GraspVLA. Knowledge can be shared. 3D modality enhances spatial intelligence. Emergent abilities to learn new skills.

<!-- TOC --><a name="3-mobile-agent-s72561"></a>
## 3. Mobile-Agent: 探索基于多模态智能体的汽车座舱助手新技术 [S72561]

> Ji Zhang, Alibaba Qwen

- GUI Agent: 理解UI、定位操作、多步操作规划反思、操作延时
- Multi agents (planning + decision + reflection) with knowledge injection + 端云协同模型架构。
- 知识增强：操作逻辑 tips（根据用户query加载知识） + 操作步骤增强 shortcuts（理解UI），可点餐订火车票机票酒店。自我进化：用两个experience reflectors根据操作记录和错误日志，优化更新tips & shortcuts。
- 复杂任务拆解：separate high level planning & low level action, improve long-horizon planning & error recovery.

<!-- TOC --><a name="4-ai-s73910"></a>
## 4. 创业企业在生成式 AI 及机器人方向的实践与分享 [S73910]

NVIDIA NeMo: Every enterprise needs their data flywheel. 

Model customization: prompt engineering/learning + fine tuning (LoRA, Adapter, IA3, SFT, RLHF)

> 爱动超越
- 工业车辆端侧大模型Agent工作流，ADAS智驾

> Paxini
- 机器人ITPU多维触觉+视觉触觉双模态感知。

<!-- TOC --><a name="5-s72500"></a>
## 5. 应用于汽车行业聊天机器人的多模态检索增强生成方案 [S72500]

> 王光甫，长城汽车
- 多路并行召回，从不同维度获取候选文档，merge候选池后进行精排rank，根据基座大模型总结输出。
- Contrastive loss，拉近正确样本对的语义距离，扩大错误样本对的差异。
- 对基座模型进行蒸馏，将DeepSeek-R1教师模型迁移到学生模型上。

Advanced RAG Pipeline
- Multimodal PDF Data Extraction & Document Parsing.
- Finetune embedding model for improved retrieval. E.g., automatic training of contrastive learning loss between positive/negative chunks of user query sampling, via NeMO.
- Contextual retrieval, build context for each chunk to connect the relationship of each chunk and include global information.
- Semantic splitter, use LLM to segment documents for semantically coherent content grouping.
- Query decomposition, break down difficult query to subqueries.

<!-- TOC --><a name="6-s72716"></a>
## 6. 大语言模型在智能座舱中的应用 [S72716]

> 高杰，蔚来汽车

Emotional Intelligence, E2E multi-modal diaglog

<!-- TOC --><a name="7-s72635"></a>
## 7. 激发通用人工智能的创造力，引领智能汽车走向新的未来 [S72635]

> 王小刚，绝影

- 叛逆进化：不是听话的工具循规蹈矩的助手，而是提供主动温暖关怀的新成员。
- Flash Decoding. Increase concurrency and improve GPU utilization.
- Segment prefill. Change management of KV cache, reuse computation results.
- Continue batching.

<!-- TOC --><a name="8-cpugpu-s71579"></a>
## 8. 加速指标计算：CPU/GPU 异构实时计算平台 [S71579]

> 周小华，DolphinDB

- 高性能分布式时序数据库
- Pipeline流水线优化，多线程拷贝，提升内存带宽
- 雪球期权定价，适合GPU

<img src="https://github.com/user-attachments/assets/6a71af90-5e76-4e1b-bc13-2896902bd045" width="50%" height="50%">

- RAG: VectorDB + TextDB

<!-- TOC --><a name="9-ufo-lite-s72498"></a>
## 9. UFO-Lite: 基于自推测解码的低延迟多模态大模型 [S72498]

> Teng Xi, Baidu. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727321215230001Bhv2/FinalPresPDF/S72498_1741760325928001kU80.pdf)

多模态大语言模型（MLLMs）展示了卓越的能力和强大的泛化能力。然而，目前的 MLLMs 往往难以满足快速响应的需求，推理延迟成为其在现实应用中的一个重要的挑战。本讲座将介绍 UFO-Lite，这是 VIMER-UFO MLLMs 系列的最新快速版本，其针对现实场景中的高效化部署进行了优化。具体而言，UFO-Lite 引入了自推测解码机制，显著的减少了端到端的延迟，且准确性几乎没有损失。它采用了新颖的双 LLM 结构，将自回归生成任务中序列化的多次前向推理卸载到了快速分支（即草稿模型），并通过原始模型对草稿序列进行并行的验证，以保持精度。为了实现自推测解码，UFO-Lite 基于量化感知的知识蒸馏，有效地开发了双 LLM 的快速分支，确保其分布与原始模型相似且具备较高的推理速度。此外，UFO-Lite 还提出了基于置信度的自适应切换，利用动态验证窗口大小，而不是固定大小，并兼容短序列生成。UFO-Lite 在 AI2D 和 MathVista 数据集上展示了与 Qwen2-VL-7B 和 InternVL2-8B 相当的性能。在 MMMU 上，它实现了与 InterVL2-8B、MiniPCPMV2-2_6 和 GLM-4V-9B 相当的结果。与上述模型相比，UFO-lite 可以加速 2 倍以上。此演讲可以为进一步发展高效且有效的多模态大语言模型提供有价值的见解。


- First work on MLLMs-SPD (Speculative Decoding) and orthogonal to previous work on LLMs
- Implement based on TensorRT-LLM，极致推理速度
- Can be applied to any state-of-the-art MLLMs，任意大小tokenizer、对任何大模型都能加速且精度相近(量化+蒸馏)
- Enhanced with FP8 and deployed on L20
- 多模态：小模型自回归k次迭代，大模型一次并行处理
- Draft-then-Verification Paradigm of Speculative Decoding 大小模型协同，交替推理
- Confidence-based Adaptive switching strategy: 小模型做错/幻觉了被大模型修正，小模型不够自信让大模型验证。
- Moreover, can Quantization-Aware Knowledge Model-Distillation
- DeepSeek-V3/R1: Multi-Token Prediction (MTP)可以直接让小模型预测速度翻倍。

<!-- TOC --><a name="10-vla-s72557"></a>
## 10. VLA：迈向自动驾驶物理智能体的关键一步 [S72557]

> Peng Jia, Li Auto. 

- VLA based on Fast and Slow thinking systems
- Sparse attention can reduce the reasoning burden, MoE can control the growth of params and increase model capability.

<!-- TOC --><a name="11-overlap-llm-s72643"></a>
## 11. 使用投机采样和计算通信 Overlap 提升 LLM 推理效率 [S72643]

> Baichuan AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727598580103001ix9j/FinalPresPDF/S72643_1741684006928001iRRh.pdf)

- 投机采样：利用decode过程算力冗余，生成多个候选token输入给LLM并行验证，充分使用算力且不增加时延。难点是draft model为候选token的产生方式。
  - 使用投机采样优化 decode 阶段效率问题，通过设计高命中率低成本的模型结构及动态的候选 token tree 结构，提升投机采样有效性
  - 采用计算通信 overlap 优化通信占比大场景下 prefill 效率问题，通过创新的序列内 overlap 提升计算利用率，从而降低 prefill 阶段耗时。
- Clover2模型，类RNN架构，轻量。能超越Eagle。
  - 优化loss
  - 主模型预测token信息前置，提前加入transformer block帮助更远预测
  - regressive attention block output projector结构由medusa的ResBlock改为FC，提升后几个head预测能力
  - 增加augmenting block层数，提高信息提取能力
- 工程实现（推理两阶段）
  - Prefill阶段（计算密集型）：输出token给draft model，draft model循环每个head动态sample出tree，对tree给decode并行验证。需要多batch高效动态sample，构建tree mask。
  - Decode阶段（内存访问密集型）：并行推理给sample， 然后验证，最后一个验证无效的token重复给draft model生成token tree。需要attention支持tree mask，KV Cache管理。
- 工程优化
  - 探索ISO(序列内计算通信overlap)策略在LLM推理期间如何提升计算利用率，从而节省大量首token耗时

<!-- TOC --><a name="12-megatron-core-s72580"></a>
## 12. 构建以 Megatron-Core 为核心的大语言模型训练加速生态 [S72580]

> Alibaba Cloud, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727454091122001Qxbv/FinalPresPDF/S72580_1741685427734001itcO.pdf)

Megatron-Core 是 NVIDIA 开发的用于训练超大规模 Transformer 模型的分布式框架，具有出色的分布式性能，是训练具有数千亿或更多参数的大语言模型的必备工具。Pai-Megatron-patch 是阿里云旗下 PAI 平台开发的大语言模型训练工具包，包含基于 Megatron-Core构建高效 LLM 训练系统的关键组件，如 mcore 和 Huggingface 之间的双向 ckpt 转换，弥合 mcore 和 Huggingface 生态系统之间的差距；实现了 Distributed Optimizer CPU 卸载技术，进一步降低了大模型训练的成本；还开发了给定硬件资源条件下的自动超参数优化工具，提高了框架的可用性等功能。在此基础上，它提供了训练各种开源大语言模型的最佳实践。

- 填补了 Megatron 优化器在 GPU 资源有限时无法卸载的空白。 我们实现了具有高效 CPU 卸载的分布式优化器，这是一种用于训练的 GPU 显存优化技术，可在 GPU 资源有限的情况下训练长序列
- 支持 HuggingFace 和 MCore 模型之间的高精度双向 ckpt 转换。 更具体地说，我们可以对复杂的稠密 MoE 模型 (例如 Llama 3、DeepSeek-V2-MoE 或 Qwen-MoE 等) 进行 3D (张量并行/流水线并行/专家并行) 转换
- 针对热门的稠密 MoE 模型升级和简化训练最佳实践，提供易于使用并且非常强大的加速技术，例如 FlashAttention-3、TP-Comm-Overlapping



<!-- TOC --><a name="13-tensorrt-llm-s74181"></a>
## 13. 重塑短视频视觉体验，基于 TensorRT-LLM 加速的智能视频质量评价与处理大模型 [S74181]

> Kuaishou

引入Consistency Model通过蒸馏，推理仅需1步（diffusion model很慢推理要25步，加速了25倍）：One-step inference of PTM, Compute distillation loss, EMA weight updating. 

<!-- TOC --><a name="14-laiye-ai-foundry-nvidia-ai-enterprise-s72276"></a>
## 14. Laiye AI Foundry - NVIDIA AI Enterprise 在中国的最佳实践 [S72276]

> Laiye AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726190594557001j0HX/FinalPresPDF/S72276_Laiye%20AI%20Foundry_NVIDIA%20AI%20Enterprise_1741682513554001WKy5.pdf)

- Hybrid RAG = GraphRAG (Node Embedding/PageRank) + VectorRAG (semantic chunking)
- PEFT enhancements: Adaptive Rank on LoRA, Adapters Diversifying, Hybriding（学习专有知识+共有知识，每个专家专注自己领域知识）

<!-- TOC --><a name="15-llm-2-bit-s72647"></a>
## 15. LLM 2-bit 后量化的加速与部署实践 [S72647]

> ByteDance, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727624187326001lFnY/FinalPresPDF/S72647_1741682615101001YdeF.pdf)

深入研究了用于 LLM 的高精度 2-bit 权重压缩。将模型参数解耦为整数和浮点部分，并通过对权重和 scale/zp 的交替迭代进行优化，求解局部极小值，最终在 Llama-1/2 7B~70B 的 2-bit 后量化上实现了 sota 精度。此外，我们还探索利用 2-bit 内存访问优势的新技术，并基于 TensorRT-LLM 中的 w4a16 Gemm 运算符开发 Gemm CUDA 内核，该内核可以高效加速 w2a16 模型的推理，并在 NVIDIA GPU 上实现 1.4 倍至 1.7 倍的加速。

<!-- TOC --><a name="16-s72639"></a>
## 16. 面向海量模型业务场景的文生图高效推理加速解决方案 [S72639]

> Alibaba Cloud

Stable Diffusion引入latent space在VAE减少计算，速度提高7倍。Pipeline三个部分：文本编码器CLIP、VAE潜空间图像翻译为输出图像、UNet多步推理模型（95%计算量，要算子性能优化：矩阵乘和卷积）。

<!-- TOC --><a name="17-s74073"></a>
## 17. 下一代生成式推荐模型训推引擎的建设和落地实践 [S74073]

> Meituan, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734091150868001dcjp/FinalPresPDF/S74073_1741683377945001YCts.pdf)

- 下一代推荐模型，Gennerative Recommenders (GR) Model: HSTU (Hierarchical Sequential Transduction Units)：把召回和排序都嵌入这个序列生成模型，将长序列输入Transformer进行信息交互。
- 美团模型工程优化：
  - 混合并行策略 = Sparse模型并行 + Dense模型并行
  - 功能支持：动态hash table支持动态扩容（如商品频繁上下线），梯度累积（大batch size训练）
  - 性能优化：自动合表，混合精度训练，kernel优化，变长序列负载均衡（用户序列明显长尾分布）。
  - Inference优化：User Cache 同一用户不同request之间序列特征基本相同可复用，先读用户KV Cache再计算attention。
- 未来：online learning场景、Embedding压缩 + 低精度训练优化性能、模型并行支持更大规模GR、AOT compile支持PyTorch2

<!-- TOC --><a name="18-tensorrt-llm-s72995"></a>
## 18. 基于 TensorRT-LLM 的广告场景生成式推理加速方案 [S72995]

> JD, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727962469507001ov1v/FinalPresPDF/S72995_1741644906381001kPbt.pdf)

单节点，算力释放
- TensorRT-LLM最佳实践，耗时约束下吞吐量最优化
- 基于广告场域特性优化，有限表征下吞吐优化

分布式，软硬协同
- 异构硬件分布式推理，CPU&GPU异构部署
- KV Memory Tool，KV中心，三级缓存，异步加载，增量更新

---

<!-- TOC --><a name="english-101-200"></a>
# English: 101-200

<!-- TOC --><a name="101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937"></a>
## 101. Enable Intelligent Storage to Process Data for AI Applications [S71937]

> Vincent Hsu, IBM Storage CTO

- Content-aware Storage (CAS) GenAI semantic search: Unstructured data is unsearchable, but we can integrate GenAI into our storage, so as not to repeatedly copying data from vector store all the time with lots of money.

<!-- TOC --><a name="102-mistral-ai-placing-frontier-ai-in-your-hands-s73942"></a>
## 102. Mistral AI: Placing Frontier AI in Your Hands [S73942]

> Arthur Mensch, Mistral AI CEO

Enterprise AI strategy
- Private architecture:
  - SaaS-like simplicity on-premises and in-VPC. Everything is stateful.
  - Deep observability and robust tooling.
  - Continuous infra optimization.
<img src="https://github.com/user-attachments/assets/bad09fb3-6651-4505-b975-f4ab65958b23" width="80%" height="80%">

- E2E customizability pipeline:
  - Custom models (full/continued pretraining).
  - Rigorous post-training (SFT, DPO, alignment). 
  - Plug-n-play platform modules.
<img src="https://github.com/user-attachments/assets/d58400a0-c272-4e14-9ef0-bbcc75339227" width="80%" height="80%">

- Enterprise context: connectors
  - Integrations to enterprise knowledge.
  - Continually updating context.
  - Easy-to-build agents.
<img src="https://github.com/user-attachments/assets/3afe719a-17d6-4b28-9906-3575eb4f0116" width="80%" height="80%">

Mistral Small 3.1, OCR and SOTA embedding models 
- Clever routing between Small and Large allows to bring the best user experience
- AI app building is about stitching bricks together: bring these bricks to customers and help them stitch it.
<img src="https://github.com/user-attachments/assets/196ed814-396e-4099-8713-1c95d5651c42" width="80%" height="80%">

<!-- TOC --><a name="103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619"></a>
## 103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]

> Joe Stam, Jump Trading, Head of Research Technology. [Slides1](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/DraftPres/S73619_draft.pdf_1738621036627001aBsx.pdf), [Slides2](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/supmat/MatMulSlides_old_for_v100_1729878858947001rxoU.pdf).

Key Takeaways
- Explore computation of expected speed-of-light performance of an algorithm on a particular GPU
- Optimization strategies for matrix multiplication
- Proper use of GPU memory hierarchy

Summary
- Many $O(n^3)$ problems are actually $AO(n^3) + BO(n)$, where $B \gg A$. Must fuse operations, eliminate memory trips.
- Solution: Use Shared Memory.
  - Shared memory is divided into 32 banks.
  - If multiple threads within a warp access the same bank, the warp will serialize.
  - Important when we need to handle different transpose orientations
- We need to use SMEM to batch incoming data. To saturate FMA, we need to batch multiple computations and with `.reuse`, maybe want multiple CTAs per SM. We want to use asynchronous copies.
- Performance keys: All GMEM coalesced. Use SMEM, no bank conflicts. Use Async Load to SMEM LDGSTS, 128 bit where possible. Free warp scheduler to issue FMAs - use 128 bit loads to registers. User register banking. Repeat Registers.

<!-- TOC --><a name="104-gtc-2025-keynote-s72484"></a>
## 104. GTC 2025 Keynote [S72484]

> Jensen Huang, NVIDIA CEO

<!-- TOC --><a name="105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692"></a>
## 105. AI for Safe and Efficient Trading in Electronic Markets [S72692]

> Iain Dunning, Hudson River Trading, Head of AI Lab

- Provide liquidity and price discovery. Market data, beyond tick-level data order books, there's news feeds, announcements, alternative data.
- Optimize money rather than prediction results, so we have RL. RL targets real thing, but has lower signal to noise, and less compute efficiency. RL trains and tests the same thing, no need for splitting training and testing datasets.
- Build market digital twins, factory of market data, training models that learn intuitively to diverse datasets to be robust.
- LLM to predict next token (market price in next second, ~170B price tokens for 3K stocks 10 years tick data), generate multiple possible future trajectories by sample-backin sample-backin repeatedly.
- Desirable properties: probabilistic, non-degenerate, capture tails, likelihood estimation, understandable.
- Challenges: Alt data, Coherence, Tails (tail output with human-driven signals of how model behaves), Calibration, Actions, Inference.
- Massive parallel compute, on model training, model inference, simulation and generation.
- Backtest how different actions would affect market if taking them. Time horizon from intraday to single days.

<!-- TOC --><a name="106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694"></a>
## 106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]

> Jim Fan, NVIDIA. Justin Johnson, World Labs. Misha Laskin, Reflection AI. Saad Godil, Hippocratic. Pete Florence, Generalist AI. Eric Jang, 1x Technologies.

- World Labs: Allow people to generate worlds by inputting single image, and generate 3D consistent world for agents to move around and interact with visually.
- Reflection AI: Today's agent is still driven by person, how to get systems that autonomously reason and backtrack on their own, and self improve to solve tasks. Coding agent is important as embodied factor for agent computer to solve general problems.
- Hippocratic: Future is vertical models, copilot is not full potential of GenAI, we need Autopilot to provide abundance in scale. Another bet is voice.
- 1x Technologies: Let AI to think harder why they fail.

<!-- TOC --><a name="107-google-ai-research-progress-and-the-future-of-ads-s73303"></a>
## 107. Google AI: Research, Progress, and the Future of Ads [S73303]

<!-- TOC --><a name="108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208"></a>
## 108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]

<!-- TOC --><a name="109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854"></a>
## 109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]

> Pedro Vicente, BlackRock, Aladdin Principal AI Engineer. Shawn Simpson, BlackRock, AI Labs Senior Data Science Team Director

- Portfolio analysis with Aladdin Copilot, Agentic platform

<img src="https://github.com/user-attachments/assets/e5eb2335-0a66-4234-8075-9cc06b591dc2" width="80%" height="80%">

- Agents Orchestration Evaluation: Configuration layer can simulate user's environment up to the application, its context, history and expected response. Solution layer can solve the task with multiple steps separate non-related threads.
- E2E Eval is a fundamental part of CI/CD pipeline, can diagnose issues in day-to-day dev.
- NVIDIA NIM - Llama 3.3 70B. NeMo Guardrails offers easy management of prompts and configs, LangGraph creates complex workflows (full control over flow and state + custom communication protocol makes interacting with multi-agents workflows in any framework).

<!-- TOC --><a name="110-distributed-agentic-multi-modal-llm-deployments-s72654"></a>
## 110. Distributed Agentic Multi-Modal LLM Deployments [S72654]


<!-- TOC --><a name="productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818"></a>
## Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]

> Dimitris Emmanoulopoulos, Barclays, Head of Machine Learning Technologies

- Experiment: Meta-Llama-3-70B, input tokens: 2048, output tokens: 1024, FP16, NVIDIA NIM: TensorRT-LLM
- Risk Workflow: Can use any LLM, OCR, Retriever, because NIMs containerize them.
<img src="https://github.com/user-attachments/assets/98865447-d920-49a2-80f2-761eb636a7d6" width="50%" height="50%">
<img src="https://github.com/user-attachments/assets/def1aa26-c761-4876-baff-5638743d0c2c" width="50%" height="50%">

<!-- TOC --><a name="ai-for-humanoid-robots-s73182"></a>
## AI for Humanoid Robots [S73182]

> Pieter Abbeel, UC Berkeley

Body Transformer (BoT), Embodiment-aware architecture for policy learning
- A spatially modular architecture in place of a monolithic NN, can capture spatial dependencies in high-dimensional settings
  - inductive bias
  - computationally efficient
  - enable multi-frequency reasoning
  - localized "credit assignment"
- By exploiting the sparse nature of architecture, BoT achieves 2-3x reduction in computation cost. Improves imitation learning performance and scalability, reinforcement learning and transfers to real-world. 
<img src="https://github.com/user-attachments/assets/3ff646fc-c224-4d2a-a380-d00f541a4740" width="80%" height="80%">

- HumanoidBench
- Addressing high-dimensional action space: Hierarchical learning for long-horizon reasoning can solve complex tasks.

MuJoCo
- Can we train RL policies in simulation for safe and cheap data? MuJoCo playground, full-stack robotics suite, open-source library for GPU-accelerated robot learning and sim-to-real transfer, can even train on Colab notebook.
- Training process: Define task and (many) rewards. Starting training in simulation (PPO) > tune the rewards. Define learning curriculum. Add domain randomization. Transfer to real world.
- Limiations: JAX computation time scales with # possible contacts. JIT compilation slow. Reward shaping is still important.

