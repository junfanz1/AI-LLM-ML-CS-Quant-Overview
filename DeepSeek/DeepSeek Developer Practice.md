# Contents


- 项目不使用 Agent，而是会采用去年 9 月份，Anthropic 公司（Claude 模型的母公司）推出的模型上下文协议（MCP）来实现。通过 MCP，可以让大模型连接本地文件系统、数据库、调用工具等等。
- 在模型蒸馏中，思维方式有一个专业名称，叫做软标签，软标签并不是直接告诉小模型“这是对的”，而是通过大模型的输出，给出每个类别的概率分布。小模型通过学习这些概率分布，能够理解不同类别之间的微妙区别。
- 公司想要部署私有化大模型，你就可以站出来承接这个工作了。公司从业务角度考虑，特别希望在公司内自己部署微调蒸馏 DeepSeek 模型；再比如做一些 RAG 对话之类的，70B 就足够的场景。
- ReAct=思维链 + 外部工具调用。ReAct 思想会让大模型把大问题拆分成小问题，一步步地解决，每一步都会尝试调用外部工具来解决问题，并且还会根据工具的反馈结果，思考工具调用是否出了问题。如果判断出问题了，大模型会尝试重新调用工具。这样经过一系列的工具调用后，最终完成目标。Agent 会将问题拆分成多个子问题，之后一个个的解决，因此从 Thought 到 Observation 的过程会执行 N 次，直到大模型认为得到了最终的答案。Agent 处理问题会将大问题拆分成一个个的小问题，分别选择相应的工具去解决问题。因此作为实际工具调用者的我们，就需要配合大模型完成多轮工具的调用，直到大模型反馈 Final Answer，因此这是一个多轮对话的模式。AI 应用开发，写好了 prompt，就至少成功了一半

# DeepSeek-R1 模型的私有化部署
- Ollama：Ollama 致力于让用户可以以极简的方式快速部署运行开源模型。为了减少对于显存的占用，Ollama 对模型进行了量化处理，因此我们才可以在一张 16GB 显存的 T4 卡上，体验 DeepSeek-R1:32B 模型。除此之外，Ollama 还对各个大模型进行了统一的 API 封装，API 兼容 OpenAI 数据格式，因此用户可以直接通过访问 OpenAI 模型的方式去访问 Ollama 拉起的模型，非常方便。
  - Ollama 是一个专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计的开源框架，它可以用简单的命令行快捷部署多种大模型，例如 DeepSeek、Qwen、Llama3 等等模型。
  - Ollama 自身还会通过权重量化技术，调整模型权重，并通过分块加载与缓存机制以及 GPU/CPU 灵活调度等技术，使得模型能够降低对硬件的要求，提高资源利用率。以 DeepSeek-R1 的蒸馏模型 DeepSeek-R1-Distill-Qwen-7B 为例，最小也需要 14 G 的显存。但 Ollama 通过对模型的量化，可以显著降低对于显存的占用。
  - Ollama 为所有支持的模型封装了统一的 API，并且兼容 OpenAI 数据格式。这一点至关重要，由于模型是由不同公司或团队训练的，每种模型原本都提供各自的开发接口。因此，Ollama 进行统一封装后，用户在使用时就变得极为便捷。
- Ollama 部署 DeepSeek-R1 实战：引入AI 网关 Higress 以及对话前端 LobeChat。通过这两个组件，我们组成了一个带有可视化自然语言对话前端的高可用高可控的大模型集群。
  - 使用 Ollama 启动 DS 模型：在服务器上安装 Ollama 工具，直接将模型下载到本地，启动容器，将模型挂载进去
  - 网关选型：传统的类似 Nginx 的网关已经无法应对 AI 时代的长连接、高延时和大带宽的要求。Higress 是以 istio 和 envoy 为核心开发的，天然对于这些特性就有完美的契合。Higress 部署与测试
  - API Key二次分组：如果我们想将 Ollama 集群面向多用户提供服务，这种场景该如何满足呢？比如面向公司内部员工使用，每个员工一个 API Key，或者面向外部客户，按 API Key 使用量收费等。Higress 提供了 API Key 二次分租的功能。我们可以这样理解这个功能—— Ollama 是房东，Higress 租下了 Ollama 的房子，然后又把房子租给了别人，自己变成了二房东。
  - 接入 对话前端LobeChat：curl命令只是我们研发人员测试使用的，如果让用户也使用 curl，就太不像话了。因此最后，我们将网关接入到一个可视化的自然语言前端，为我们的整个方案画上圆满的句号。在 Github 上，开源的自然语言前端有很多，热度比较高的有 OpenWebUI、LobeChat
