# Contents


- 项目不使用 Agent，而是会采用去年 9 月份，Anthropic 公司（Claude 模型的母公司）推出的模型上下文协议（MCP）来实现。通过 MCP，可以让大模型连接本地文件系统、数据库、调用工具等等。
- 在模型蒸馏中，思维方式有一个专业名称，叫做软标签，软标签并不是直接告诉小模型“这是对的”，而是通过大模型的输出，给出每个类别的概率分布。小模型通过学习这些概率分布，能够理解不同类别之间的微妙区别。
- 公司想要部署私有化大模型，你就可以站出来承接这个工作了。公司从业务角度考虑，特别希望在公司内自己部署微调蒸馏 DeepSeek 模型；再比如做一些 RAG 对话之类的，70B 就足够的场景。
- ReAct=思维链 + 外部工具调用。ReAct 思想会让大模型把大问题拆分成小问题，一步步地解决，每一步都会尝试调用外部工具来解决问题，并且还会根据工具的反馈结果，思考工具调用是否出了问题。如果判断出问题了，大模型会尝试重新调用工具。这样经过一系列的工具调用后，最终完成目标。Agent 会将问题拆分成多个子问题，之后一个个的解决，因此从 Thought 到 Observation 的过程会执行 N 次，直到大模型认为得到了最终的答案。Agent 处理问题会将大问题拆分成一个个的小问题，分别选择相应的工具去解决问题。因此作为实际工具调用者的我们，就需要配合大模型完成多轮工具的调用，直到大模型反馈 Final Answer，因此这是一个多轮对话的模式。AI 应用开发，写好了 prompt，就至少成功了一半

# DeepSeek-R1 模型的私有化部署
- Ollama：Ollama 致力于让用户可以以极简的方式快速部署运行开源模型。为了减少对于显存的占用，Ollama 对模型进行了量化处理，因此我们才可以在一张 16GB 显存的 T4 卡上，体验 DeepSeek-R1:32B 模型。除此之外，Ollama 还对各个大模型进行了统一的 API 封装，API 兼容 OpenAI 数据格式，因此用户可以直接通过访问 OpenAI 模型的方式去访问 Ollama 拉起的模型，非常方便。
  - Ollama 是一个专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计的开源框架，它可以用简单的命令行快捷部署多种大模型，例如 DeepSeek、Qwen、Llama3 等等模型。
  - Ollama 自身还会通过权重量化技术，调整模型权重，并通过分块加载与缓存机制以及 GPU/CPU 灵活调度等技术，使得模型能够降低对硬件的要求，提高资源利用率。以 DeepSeek-R1 的蒸馏模型 DeepSeek-R1-Distill-Qwen-7B 为例，最小也需要 14 G 的显存。但 Ollama 通过对模型的量化，可以显著降低对于显存的占用。
  - Ollama 为所有支持的模型封装了统一的 API，并且兼容 OpenAI 数据格式。这一点至关重要，由于模型是由不同公司或团队训练的，每种模型原本都提供各自的开发接口。因此，Ollama 进行统一封装后，用户在使用时就变得极为便捷。
- Ollama 部署 DeepSeek-R1 实战：引入AI 网关 Higress 以及对话前端 LobeChat。通过这两个组件，我们组成了一个带有可视化自然语言对话前端的高可用高可控的大模型集群。
  - 使用 Ollama 启动 DS 模型：在服务器上安装 Ollama 工具，直接将模型下载到本地，启动容器，将模型挂载进去
  - 网关选型：传统的类似 Nginx 的网关已经无法应对 AI 时代的长连接、高延时和大带宽的要求。Higress 是以 istio 和 envoy 为核心开发的，天然对于这些特性就有完美的契合。Higress 部署与测试
  - API Key二次分组：如果我们想将 Ollama 集群面向多用户提供服务，这种场景该如何满足呢？比如面向公司内部员工使用，每个员工一个 API Key，或者面向外部客户，按 API Key 使用量收费等。Higress 提供了 API Key 二次分租的功能。我们可以这样理解这个功能—— Ollama 是房东，Higress 租下了 Ollama 的房子，然后又把房子租给了别人，自己变成了二房东。
  - 接入 对话前端LobeChat：curl命令只是我们研发人员测试使用的，如果让用户也使用 curl，就太不像话了。因此最后，我们将网关接入到一个可视化的自然语言前端，为我们的整个方案画上圆满的句号。在 Github 上，开源的自然语言前端有很多，热度比较高的有 OpenWebUI、LobeChat

# 分布式部署

Kubernetes架构
- 通过设置策略，让 K8s 自动完成容器在服务器之间的调度，这个过程就叫做容器编排。
- 容器 A 的服务压力过载了，我需要再搞一个容器 A 的副本，然后建立一个负载均衡来进行分流。此时我就可以将容器 A 的副本数调成 2，K8s 就会自动创建一个副本，并自动调度到资源合适的服务器上去。这个过程就叫扩容。反之，如果压力降下来，容器 A 可以再将副本数调成 1，此时服务器 3 上的容器 A 就会自动销毁，这个过程叫缩容。
- 这些服务器之间是如何关联的呢？这会有一个主程序进行统一的管理。通常主程序是单独部署到一台服务器上的，也就是所谓的 master 节点，剩余的跑容器应用的服务器叫做 worker 节点。在 worker 节点上会安装一个客户端程序，学名叫 kubelet，主程序不能直接和容器应用建立联系，而是通过与 kubelet 建立联系来实现对容器应用的编排等操作。
- 主程序由多个组件组成，有负责将容器应用调度到哪个节点的，学名叫调度器 kube-scheduler；有负责存储各个节点的状态的，学名叫 etcd；还有负责控制容器应用行为的，比如副本扩张等等，学名叫控制器 kube-controller。同时为了方便 Kubernetes 集群外的用户或者程序对集群的访问，主程序还做了一个 HTTP Server，暴露出了很多 API 接口，这个 Server 叫做 API Server。
- K8s 对容器应用又做了一层封装，名字叫做 Pod，Pod 是 K8s 中业务容器最基本的单元。一个 Pod 内可以有多个容器。
- Docker 和 containerd 的关系，就如同斯太尔卡车整车和车头的关系。两个都能跑，但是光车头更轻便。
- 部署到 K8s 上的服务，默认只能在集群内部访问，如果想要在集群外部访问，需要进行服务暴露。服务在 K8s 中也是一种资源，叫做 Service。Service 的服务暴露方式有两种，第一种是直接通过端口暴露，这种方式叫 NodePort；第二种是通过负载均衡，需要服务器有一个负载均衡器才可以。
- operator 包含两个部分，分别是自定义资源 CRD 和自定义控制器，这句话的重点在于自定义。也就是说，我可以自己编写一个资源，注册到 K8s 中，从而完成一些自定义的功能。比如，我可以做一个 nginx operator，当我 apply nginx operator 的自定义资源时，K8s 就会自动帮我把 nginx Deployment 和 service 都创建出来，这就是自定义的含义。

vLLM分布式部署DeepSeek
- vLLM 是一个快速且易于使用的库，专为大型语言模型 (LLM) 的推理和部署而设计，可以无缝集成 HuggingFace、Modelscope 上的模型。在性能优化上，vLLM 通过引入创建的架构和算法，例如 Paged Attention、动态张量并行等，减少计算开销，提高吞吐量，实现推理过程的高效，从而加速大模型在推理阶段的性能。
- 离线推理的意思是一次性给大模型发送多条 prompt，让大模型针对每条 prompt 分别给出回答。使用 LLM 类和 DeepSeek-R1-Distill-Qwen-7B 初始化 vLLM 引擎以进行离线推理。调用 llm.generate 生成输出。它将输入提示添加到 vLLM 引擎的等待队列中，并执行 vLLM 引擎来生成高吞吐量的输出。输出作为 RequestOutput 对象列表返回，其中包括所有输出的 tokens。
- 在线推理是我们日常使用大模型时的方式，比如与大模型进行实时对话等，都属于在线推理。
- vLLM 凭借 Paged Attention、动态张量并行等创新技术，为分布式推理提供了工业级解决方案。

分布式推理
- 单 GPU （无分布式推理）的情况，只有我们 GPU 卡的显存能够承载模型的运行时，这种策略才适用。例如上文中的例子，我的 T4 卡 显存是 16 G，而 DeepSeek-R1-Distill-Qwen-7B 所需要的显存是 14 G，单卡可以满足，因此就需要做分布式。
- 单节点多 GPU（张量并行推理）的情况，这种情况适用于一张卡无法承载模型的运行。例如，如果我要运行一个 32 B 的模型，需要显存为 64 G，则我至少需要 5 张 T4 卡（4 张卡是 64 G，考虑到通常要设置显存只能占用 95% 的阈值，防止推理时将显存压爆，造成模型运行崩溃，不能可丁可卯），才能运行。vLLM 提供了一个参数，–tensor-parallel-size，用于设置张量并行数量。例如 5 张卡，就设置 --tensor-parallel-size 为 5。NVlink 相当于用一条高速公路，将多张卡串在了一起，保证了多卡之间数据交换的效率。
- 多节点单（多） GPU（张量并行加管道并行推理）的情况，当我们的单台节点上没有这么多卡了，就需要多个节点来凑。例如，我要部署一个 DeepSeek-R1 671B，需要 16 张 A100，则此时一般会使用两个节点，每个节点上 8 张卡。我们把多节点的并行推理，叫做管道并行。

vLLM + Ray集群，分布式部署计算框架。 llama.cpp，模型轻量化部署开箱即用。Llama-factory，一站式微调和评估平台。

# MCP

- MCP，统一LLM与外部数据源和工具之间的通信协议，AI连接万物的接口。MCP 把工具调用程序做成了一个 C-S 架构，工具的实际调用由 MCP Server 来完成。
- 当大模型选择了合适的能力后，MCP Hosts 会调用 MCP Cient 与 MCP Server 进行通信，由 MCP Server 调用工具或者读取资源后，反馈给 MCP Client，然后再由 MCP Hosts 反馈给大模型，由大模型判断是否能解决用户的问题。如果解决了，则会生成自然语言响应，最终由 MCP Hosts 将响应展示给用户。
- MCP Server 的 Python SDK，分为 FastMCP SDK 和 Low-Lever SDK 两种。FastMCP 是在 Low-Level 的基础上又做了一层封装，不论是写代码，还是项目依赖等，操作起来都更加简单。

MCP能力
- Tool，是直接对接大模型的，可以由大模型自主选择工具，无需人类进行干涉，整个过程是全自动的。Tool 更偏标准化接入，而且供应和消费分离，别人写了某个提供 Tool 的 MCP Server，其他人也可以直接用。
- Resource，定义了大模型可以只读访问的数据源，可以用于为大模型提供上下文。这个功能类似于文件对话功能，Resource 对接的是 MCP Hosts，需要 MCP Hosts 额外开发与 Resouce 的交互功能，并且由用户进行选择，才能直接使用。

通信方式
- 标准输入输出（Standard Input/Output, stdio）：客户端通过启动服务器子进程并使用标准输入（stdin）和标准输出（stdout）建立双向通信，一个服务器进程只能与启动它的客户端通信（1:1 关系）。stdio 适用于本地快速集成的场景。
- 服务器发送事件（Server-Sent Events, SSE）：服务器作为独立进程运行，客户端和服务器代码完全解耦，支持多个客户端随时连接和断开。是一种基于 HTTP 协议的技术，允许服务器向客户端单向、实时地推送数据。在 SSE 模式下，客户端通过创建一个  EventSource  对象与服务器建立持久连接，服务器则通过该连接持续发送数据流，而无需客户端反复发送请求。

![image](https://github.com/user-attachments/assets/f24f7b3e-b6b7-4b73-812d-7d18fb20cd8e)

# Job Seeking Project

- 无头浏览器，比较常用的框架，叫做 Selenium，它是一个自动化测试和浏览器自动化的开源框架。它允许开发人员编写脚本，并借助浏览器和浏览器的驱动，来模拟在浏览器中的行为，自动执行一些列的操作，比如点击按钮、填写表单、导航到不同的页面等。
- uv项目管理配置

```bash
.
|-- src
| |-- jobsearch_mcp_server
| | |-- llm
| | | |-- llm.py
| | |-- prompt
| | | |-- prompt.py
| | |-- tools
| | | |-- job.py
| | |-- server.py
| | |-- __init__.py
|-- .env
|-- pyproject.toml
|-- LICENSE
```

- 求职助手 MCP Server。通过该 MCP Server，我们只需将简历丢给大模型，大模型就会帮我们匹配到合适的工作，并给出求职建议。

# Acknowledgements

- [DeepSeek 应用开发实战](https://time.geekbang.org/column/intro/100995901)
