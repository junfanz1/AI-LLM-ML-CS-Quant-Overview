# Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [1. Concepts](#1-concepts)
   * [Evaluation Metrics for GenAI Systems](#evaluation-metrics-for-genai-systems)
   * [Parallelism in GenAI Models with Distributed Machine Learning](#parallelism-in-genai-models-with-distributed-machine-learning)
   * [Inference Optimization in GenAI Models](#inference-optimization-in-genai-models)
- [2. Design Framework](#2-design-framework)
- [3. Text-to-Text System design](#3-text-to-text-system-design)
   * [High Level](#high-level)
   * [Design](#design)
      + [Prompt processing system](#prompt-processing-system)
      + [Long-term memory system](#long-term-memory-system)
      + [Model host and content moderation system](#model-host-and-content-moderation-system)
- [4. Text-to-Speech](#4-text-to-speech)
   * [High Level](#high-level-1)
   * [Design](#design-1)
- [5. Text-to-Image](#5-text-to-image)
   * [High Level](#high-level-2)
   * [Design](#design-2)
      + [Prompt processing and embedding system](#prompt-processing-and-embedding-system)
      + [Dynamic contextualizer system](#dynamic-contextualizer-system)
      + [Model hosting and content moderation system](#model-hosting-and-content-moderation-system)
      + [Inference strategies](#inference-strategies)
- [6. Text-to-Video](#6-text-to-video)
   * [High Level](#high-level-3)
   * [Design](#design-3)
- [Acknowledgements](#acknowledgements)

<!-- TOC end -->

---

<!-- TOC --><a name="1-concepts"></a>
# 1. Concepts

<!-- TOC --><a name="evaluation-metrics-for-genai-systems"></a>
## Evaluation Metrics for GenAI Systems

- Fréchet inception distance: FID compares the mean and covariance of feature embeddings from real and generated images.
- BLEU score: BLEU (bilingual evaluation understudy) measures the similarity between generated and reference texts by comparing n-grams.
- ROUGE score: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) focuses on recall and evaluates how much of the reference text’s content is captured by the generated text.
- Perplexity score: Perplexity measures how well a language model predicts a sequence of words. Lower perplexity indicates greater fluency and confidence in generation. It is most effective when paired with qualitative evaluation to assess coherence.
- CLIP score: CLIP (Contrastive Language–Image Pre-training) score evaluates alignment between text and images using embeddings generated by OpenAI’s CLIP model, which is an open-source pretrained model. This model has been trained on image-text pairs.

<!-- TOC --><a name="parallelism-in-genai-models-with-distributed-machine-learning"></a>
## Parallelism in GenAI Models with Distributed Machine Learning
- Peer-to-peer (P2P) synchronization: is a decentralized approach where servers (nodes) work collaboratively to synchronize the model. Each server communicates with its peers to gather and share updates, ensuring everyone stays on the same page. There are many sub-types of P2P model synchronization, including:
  - AllReduce: The simplest approach in P2P data parallelism is where every node contributes its local gradients to its peers, and a global average is calculated. This guarantees that all workers operate on identical updated gradients after synchronization. This is efficient for small clusters and eliminates the single point of failure, but its communication and complexity overhead quickly adds up.
  - Ring AllReduce: This is a specialized implementation of AllReduce, which organizes workers in a virtual ring topology to reduce communication overhead. Each worker communicates only with its two neighbors, passing gradients sequentially. This scales efficiently with more training servers and has lower bandwidth requirements than basic AllReduce. However, this sequential information passing can sometimes be slower, especially if one server lags.
  - Hierarchical AllReduce: When scaling to thousands of GPUs, the communication demands of basic AllReduce or ring AllReduce become overwhelming. Hierarchical AllReduce introduces intermediate aggregation steps by grouping workers into smaller subgroups.
    - Cluster coordinators: The GPUs are divided into smaller groups, each managed by a coordinator. Think of these coordinators as team leaders.
    - Within-cluster aggregation: A simpler method like AllReduce or ring AllReduce combines updates inside each cluster. It’s easier to manage things within smaller teams.
    - Coordinator communication: The coordinators then use AllReduce to communicate and combine the aggregated updates from each group, creating a streamlined flow of information.
- Model parallelism
  - Layer-wise partitioning: This strategy divides the model into distinct layers, assigning each layer to a different device. For example, the input, hidden, and output layers could be placed on separate GPUs in a neural network. This straightforward approach can lead to communication bottlenecks if layers have strong dependencies.
  - Operator-wise partitioning: This finer-grained strategy breaks down individual operations within a layer across multiple devices. For example, a matrix multiplication operation within a layer could be split across several GPUs. This can improve efficiency for computationally intensive operations but requires more careful management of data flow and synchronization.

<!-- TOC --><a name="inference-optimization-in-genai-models"></a>
## Inference Optimization in GenAI Models
- Pruning
- Quantization
- Knowledge distillation (KD): is a technique for transferring the knowledge of a larger model to a smaller, compact model while maintaining validity. The larger model is usually called the parent, while the smaller one is called the student. After knowledge transfer, the student model mimics the parent model to maintain a similar accuracy level.
- Caching: Key-value (KV) cache: KV cache enables models to generate new tokens quickly by storing context from earlier tokens. Thus, models do not need to recompute already processed data. KV caching is valuable in autoregressive (generative) models because they generate tokens sequentially based on past tokens. While KV caches help reduce inference latency, they typically require much memory to store the context.
- Batching: involves combining multiple requests and processing them all at once.


<!-- TOC --><a name="2-design-framework"></a>
# 2. Design Framework

- System Requirements:
- Choose AI model
- Acquire and Prepare for data
- Leverage AI model 
- Estimate resources for System Design
- Design system and evaluate requirements

<!-- TOC --><a name="3-text-to-text-system-design"></a>
# 3. Text-to-Text System design

<!-- TOC --><a name="high-level"></a>
## High Level

We demonstrated how to design a conversational AI, from training a Llama 3.2 3B model focusing on data preprocessing and bias mitigation to deploying it using a scalable System Design that prioritizes personalization and context awareness. We covered key aspects like security and modular subsystems for prompt embedding, long-term memory, and dynamic contextualization, offering a practical guide to developing a robust conversational AI.

- Llama 3.2 3B is chosen for its efficiency and open-source nature, enabling customization for optimal conversational performance.
- Vector databases efficiently store and retrieve embeddings, facilitating real-time context retrieval for relevant and personalized responses.
- A modular design ensures the system is futureproofed and readily hackable.

<!-- TOC --><a name="design"></a>
## Design

<!-- TOC --><a name="prompt-processing-system"></a>
### Prompt processing system

This subsystem transforms user prompts into numerical vectors called embeddings, which capture semantic and contextual meaning.
  - Vector embedding service: This service generates a numerical representation of the text.
  - Semantic cache: This component caches embeddings of frequently encountered phrases and topics to reduce latency during inference. An effective caching strategy for this component is a least recently used (LRU) caching strategy, as it ensures frequently accessed embeddings remain in the cache while evicting the least recently used ones. This aligns well with the service’s need to prioritize commonly used phrases and topics for faster retrieval.
  - Vector database: A specialized database like Pinecone stores and indexes embeddings, allowing for rapid retrieval of contextually relevant data (vectors) using similarity searches based on user query embeddings.
  - Data filter: Cleans and refines retrieved data to ensure that only relevant and high-quality context information is passed to the long-term memory system.
<!-- TOC --><a name="long-term-memory-system"></a>
### Long-term memory system
This subsystem stores user interactions, feedback, and preferences that enhance contextual accuracy and personalization of the prompts. It refines input prompts and processes them to align with the user’s intent and prior context.
  - Context retrieval engine: This system identifies and retrieves relevant data from long-term storage. When the input prompt is received, the context retrieval engine searches the stored data to identify the most relevant past interactions, user preferences, or general knowledge based on the highest similarity scores using algorithms such as cosine similarity, Euclidean distance, etc. This process ensures the system retrieves contextually accurate and personalized information to enrich the final output (prompt) with relevant data. It also ensures that the retrieved context aligns well with the user’s intent and the overall conversational flow. It scans and adjusts context-aware information, focusing on relevancy and clarity. This process also involves weighing the importance of different pieces of information, resolving contradictions, or adding missing details, depending on the user’s history and preferences.
  - Data privacy controls: It incorporates compliance features to ensure transparency in data usage and adherence to regulatory requirements. An important role of this service is to ensure that user data remains private.
  - Data stores: To facilitate the long-term memory system, a number of storage services are required to store and maintain various forms of data:
    - User profile and feedback data: This database collects and stores user-specific data such as preferences and feedback. The purpose of this service is to enable personalization and continuous improvement of the prompt and, finally, the output generated by the model. A NoSQL database like MongoDB or Cassandra is ideal for storing user profile and feedback data due to its flexibility in handling unstructured or semi-structured data and scalability.
    - Session storage and cache: This keeps the context within a session for accurate and coherent interactions. A key-value store like Redis is often used as a session cache because it provides fast, in-memory data storage. It is also ideal for temporarily storing session data that needs to be quickly accessible and frequently updated, such as user login states, session IDs, or recent interactions in real-time applications.
    - Long-term storage: This stores historical data related to users' interactions over a period of time. This system helps the AI provide a personalized experience over multiple sessions with a user. A long-term memory is a system of databases such as SQL, NoSQL, and vector databases. The raw data is stored in SQL or NoSQL databases and is processed using encoding models like BERT or word2vec, which captures the semantic meaning. The encoded (embedded) data using these algorithms are stored in the vector database.
    - Knowledge base: Often referred to as a knowledge graph, this optional component is useful for applications requiring general knowledge reference. It captures structured relationships, concepts, and facts that the system may need to retain. A graph database is used for knowledge graph (knowledge base), with popular options like Neo4j or Amazon Neptune.
<!-- TOC --><a name="model-host-and-content-moderation-system"></a>
### Model host and content moderation system
This service hosts the core model and generates results using the enhanced prompt. It also ensures that the generated content is clear, appropriate, and relevant.
  - Model loader: This service is responsible for loading and updating model weights in the model host, supporting versioning and efficient swapping for continuous deployment.
  - Model host: This is where the core model runs. This service takes the optimized prompt and executes it to produce a response.
  - Model storage: A repository of various models, their files, and configurations. It ensures quick access to various model versions and configurations for dynamic loading and facilitates updates and rollbacks. A combination of blob storage and distributed file systems is typically used for model storage.
  - Real-time content moderation service:


<!-- TOC --><a name="4-text-to-speech"></a>
# 4. Text-to-Speech

<!-- TOC --><a name="high-level-1"></a>
## High Level

We designed a text-to-speech system using the efficient and multilingual SOTA Fish-Speech model, trained on the extensive common voice corpus. Our system prioritizes high-quality audio generation and user customization, employing a modular architecture with subsystems for prompt processing, linguistic feature extraction, acoustic representation, and speech generation. We optimized for performance and scalability using techniques like model quantization and horizontal scaling while ensuring high availability through redundancy and fault tolerance mechanisms.

- Dual-AR structure for efficient handling of complex linguistic features, polyphonic words, and multilingual inputs.
- Emphasis on prosody and intonation for emotionally resonant speech output.
- Inference optimizations like quantization to achieve minimal latency

<!-- TOC --><a name="design-1"></a>
## Design

- The Fish Speech architecture begins with an LLM that processes the input text and extracts linguistic features. These features are fed into a slow transformer, which generates token logits. The first AR model uses RMSNorm and slow transformer layers to generate hidden states. Another AR model with fast embedding and fast transformer layers uses these hidden states to generate codebook logits. These logits are then passed to a decoder as quantized mel tokens.
- Finally, these codebook logits produce the output speech waveform. This Dual-AR structure allows Fish Speech to effectively model both the linguistic and acoustic aspects of speech, resulting in high-quality and natural-sounding synthesized speech.
- In Fish Speech’s architecture, the slow transformer focuses on processing the linguistic features extracted by the LLM. It operates at a slower pace due to the complexity of language modeling. On the other hand, the “fast” transformer handles the acoustic information (mel spectrograms) and operates more quickly to generate the final speech waveform. A Firefly-GAN (FF-GAN) is used as the decoder.

<!-- TOC --><a name="5-text-to-image"></a>
# 5. Text-to-Image

<!-- TOC --><a name="high-level-2"></a>
## High Level

We went through a comprehensive journey designing, training, and deploying a text-to-image generation system using the Stable Diffusion 3.5 Large model. We emphasized data quality and preprocessing and utilized a distributed training approach with a peer-to-peer architecture. The deployment System Design focuses on inference optimization, scalability, and user experience, incorporating techniques like efficient denoising schedulers, resolution scaling, and multi-region deployment. The study offers a practical understanding of the infrastructure and design decisions required for a robust and efficient text-to-image generation system.

- CLIP text encoder for detailed, prompt understanding and accurate image matching.
- Peer-to-peer distributed training is used to efficiently train the computationally intensive model.
- Various techniques, such as efficient denoising schedulers (DDIM or DPM-Solver), half-precision floating-point format (FP16), and attention layer optimization, are employed to enhance performance and user experience by reducing inference time and memory consumption.

<!-- TOC --><a name="design-2"></a>
## Design

<!-- TOC --><a name="prompt-processing-and-embedding-system"></a>
### Prompt processing and embedding system
This subsystem is the foundational component that transforms user inputs into meaningful data for text-to-image generation. It processes and enhances prompts to extract semantic meanings and expressive styles, providing encoded inputs for subsequent stages.
<!-- TOC --><a name="dynamic-contextualizer-system"></a>
### Dynamic contextualizer system
This subsystem combines dynamic prompt interpretation with user feedback, enabling the model to generate contextually rich and detailed outputs. This system ensures user intent is balanced across concepts, styles, and aesthetics while the different databases retain relevant knowledge for informed image creation.
<!-- TOC --><a name="model-hosting-and-content-moderation-system"></a>
### Model hosting and content moderation system
<!-- TOC --><a name="inference-strategies"></a>
### Inference strategies
We will use efficient denoising schedulers like the denoising diffusion implicit model (DDIM) or diffusion probabilistic model solver (DPM-Solver) to reduce steps and speed up image generation without sacrificing quality. As we have seen, the half-precision floating point is ideal for fast computation (and reducing the number of GPUs). Attention layers can be optimized with libraries like xFormers for better memory and speed performance. Using batching and parallelization can maximize GPU usage, especially in high-throughput scenarios.

<!-- TOC --><a name="6-text-to-video"></a>
# 6. Text-to-Video

<!-- TOC --><a name="high-level-3"></a>
## High Level

We built a text-to-video system using the Mochi 1 model, trained on a diverse dataset of videos and text prompts. Our system emphasizes accurate, prompt interpretation, scene continuity, and asset diversity, employing a modular architecture with subsystems for prompt processing, content planning, asset generation, motion synthesis, rendering, and quality control. This robust design enables our system to generate high-quality, stylized videos that accurately reflect user prompts.

- Diffusion-based model for high-quality stylized video generation.
- Modular system with specialized subsystems for prompt processing, content planning, asset generation, motion synthesis, rendering, and quality control.
- Specialized evaluation metrics that work with video generation.

<!-- TOC --><a name="design-3"></a>
## Design

- Mochi 1, a diffusion-based model that generates high-quality stylized videos for our text-to-video generation system. Mochi 1 uses ~10 B parameters in the diffusion process and ~300 M for the VAE to ensure detail, temporal consistency, and customization flexibility. We will keep it simple and assume a model with 10 B parameters.
- The Mochi 1 model starts with two main inputs: a text prompt (processed by the T5-XXL language model) and video input. The text prompt is encoded into text tokens and passed to the text processing stream with a smaller hidden dimension. Simultaneously, the video input is compressed by the video VAE, reducing its size through spatial compression (8x8) and temporal compression (6x) to a compact 12-channel latent space. This output is then processed by the visual processing stream, which has a larger hidden dimension because video data requires more capacity to handle complexity.
- The text and video streams come together in the multimodal self-attention module, where the system learns to unify and relate information from both modalities. The model uses an asymmetric diffusion transformer (AsymmDiT) with specialized layers like non-square QKV and projection layers to efficiently manage the differences between text and video data. This design ensures efficient memory usage and balances processing power.
- After this, the architecture applies full 3D attention to process a large number of video tokens (44,520) within a single context window, enhancing spatial and temporal coherence. It leverages 3D positional embeddings to position each token accurately in space and time. The model then uses techniques like SwiGLU feedforward layers, query-key normalization, and sandwich normalization to stabilize and refine the generation process. Finally, the output is the generated video, which integrates the features from the input text.

<!-- TOC --><a name="acknowledgements"></a>
# Acknowledgements

[Educative Certificate](https://www.educative.io/verify-certificate/RgxzXQFQkKyYgKrGjTX1RQpE9J3vT6)

![image](https://github.com/user-attachments/assets/5ca7557b-ef9a-4e9e-be21-8d2c623aa89c)



































