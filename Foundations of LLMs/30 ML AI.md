30 Essential Questions and Answers on Machine Learning and AI - by Sebastian Raschka, 2025 

<img src="https://github.com/user-attachments/assets/05c0f747-7448-4d4e-b338-aae832a86b8b" width="34%" height="34%">


# Contents
<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [2. 自监督学习](#2-)
- [4. 彩票假设](#4-)
- [6. 减少过拟合](#6-)
- [7. 多GPU训练](#7-gpu)
- [9. GenAI](#9-genai)
- [10. 随机性](#10-)
- [11. 计算参数](#11-)
- [13. ViT](#13-vit)
- [18. 参数高效微调](#18-)
- [19. 评测模型](#19-)
- [22. 加速推理](#22-)
- [23. 数据分布偏移](#23-)
- [30. 有限的有标签数据](#30-)

<!-- TOC end -->


<!-- TOC --><a name="2-"></a>
## 2. 自监督学习

孪生网络：同一个模型对原始样本和施加扰动的样本生成嵌入向量。

<!-- TOC --><a name="4-"></a>
## 4. 彩票假设

彩票假设：随机权重初始化，训练网络达到收敛和最佳性能，权重剪枝，将权重重置为较小随机权重值，并重新训练（反复步骤），直到网络缩减到10%大小（中奖彩票），甚至比原来大模型有更好泛化性。

<!-- TOC --><a name="6-"></a>
## 6. 减少过拟合

- 知识蒸馏（KL散度）、集成（k-fold）
- BatchNorm、LayerNorm让训练更稳定且有正则化效果，WeightNorm也能提高泛化性

<!-- TOC --><a name="7-gpu"></a>
## 7. 多GPU训练

- 模型并行：第一层在GPU1，第二层在GPU2。GPU之间需要相互等待和依赖，不高效
- 数据并行：小批量数据划分成微批量数据，让GPU分别处理一个微批量数据，并计算模型权重的损失和梯度。所有GPU处理完后，梯度被汇总平均，来计算下一轮权重更新。
  - 优势：GPU并行运行，但如果模型太大就不能放进GPU显存。模型必须适配单GPU。
- 张量并行：模型大到无法装进单GPU显存。权重和激活矩阵拆分到各设备，而非将模型层分散到各设备。矩阵拆分可以把矩阵乘法分布到多GPU。
  - 绕过内存限制，又能并行操作。缺点：多GPU间通信开销大。
- 流水线并行：在前向传播时将激活值传递下去。但在反向传播时，输入张量的梯度被传回来，防止设备空闲。
  - 模型并行（最小化串行计算的性能瓶颈，增强部署在不同设备上的模型层的并行性）+数据并行（将小批量数据拆分成微批量数据）。
- 序列并行：Transformer架构大模型处理长序列任务的计算瓶颈，自注意力机制和输入序列的规模是二次方复杂度。缺点：长序列分解子序列可能降低准确性。

<!-- TOC --><a name="9-genai"></a>
## 9. GenAI

- 流模型（归一化流）：用可逆变换，将概率分布转变为更复杂分布。NICE (Non-linear Independent Components Estimation）的解耦层比VAE的卷积神经网络编码器更线性。
- 扩散模型：用随机扩散过程。

<!-- TOC --><a name="10-"></a>
## 10. 随机性

- Dropout：以概率p随机剔除一些单元
- 卷积：基于快速傅里叶、Winograd卷积（3x3尺寸滤波器）

<!-- TOC --><a name="11-"></a>
## 11. 计算参数

两个卷积层（卷积层1=3个输入通道+5个输出通道+核尺寸5；最大池化层=核尺寸5+步长2；卷积层2=5个输入通道+12个输出通道+核尺寸3；平均池化层=核尺寸3+步长2）+两个全连接层（192个输入单元+128个输出单元）
- 卷积核26个参数 = 5*5权重+1偏置单元
- 卷积层76个参数 = 3组权重*25 +1
- 5个输出通道的5个卷积核380个参数= 5 * 76
- 卷积层1有380个参数 = 5 * （5 * 5 * 3）+5 = 380
- 卷积层2有552个参数 = 12 * （3 * 3 * 5）+12 = 552
- 池化层没有训练参数，卷积总参数= 380 + 552 = 932
- 全连接层1有24704个参数 = 192 * 128 + 128
- 全连接层2输出层有1290个参数 = 128 * 10 + 10
- 全连接层总参数 25994 = 24704 + 1290
- 网络总参数26926 = 25994全连接 + 932卷积

<!-- TOC --><a name="13-vit"></a>
## 13. ViT

- CNN由卷积层构成，ViT由多头注意力模块组成。CNN有很多归纳偏置（领域知识），需要的训练数据比ViT少，而ViT有更多自由度。
- CNN局部连接：隐藏层单元仅与前一层的部分神经元连接，因为邻近像素关联性大。权值共享：整个图像使用同一组权重卷积核，因为相同的核对于检测图像不同部分的相同模式有用。分层处理：多个卷积层让低级特征整合成复杂特征。空间不变：可以平移。
- ViT没有空间不变性的归纳偏置，两个不同位置的相同物体会产生不同输出，因此添加相对位置编码。分块归纳偏置：用patchify独立处理每个块，用自注意力捕捉全局让每个块都能关注别的块，可以处理大图像。
- ViT自注意力是侧重全局形状和平滑曲率的低通滤波器，CNN卷积层是关注细节纹理的高通滤波器。

<!-- TOC --><a name="18-"></a>
## 18. 参数高效微调

```py
def transformer_block_with_prefix(x):
    soft_prompt = FullyConnectedLayers(soft_prompt) # prefix 
    x = concatenate([soft_prompt, x], dim=seq_len)
    residual = x 
    x = SelfAttention(x)
    x = LayerNorm(x + residual)
    residual = x 
    x = FullyConnectedLayers(x)
    x = LayerNorm(x + residual)
    return x 

def transformer_block_with_adapter(x):
    residual = x 
    x = SelfAttention(x)
    x = FullyConnectedLayers(x) # adapter 
    x = LayerNorm(x + residual)
    residual = x 
    x = FullyConnectedLayers(x)
    x = FullyConnectedLayers(x) # adapter 
    x = LayerNorm(x + residual)
    return x 
```

<!-- TOC --><a name="19-"></a>
## 19. 评测模型

- Perplexity（不同语言模型性能的评测）：预测下一词的平均不确定性。
- BLEU（精确度）：机器翻译与人工翻译的n-gram重叠程度。适合作为模型选择的工具，而不是模型评测的工具，翻译任务，可以评估流畅度。
- ROUGE（召回率）：生成的摘要与参考摘要之间的重合度。是召回率（参考文本有多少词在生成文本中出现）和精确度（候选文本中有多少次在参考文本中出现）的调和平均。但不能考虑到同义词。
- BERTScore：用BERT生成的上下文嵌入来衡量候选文本与参考文本的相似性。更好捕捉语义相似性。

<!-- TOC --><a name="22-"></a>
## 22. 加速推理

- 向量化：对整个矩阵操作，万恶不赦for循环迭代，需要底层优化如BLAS (Basic Linear Algebra Subprograms)
- 循环分块：增加数据局部性
- 算子融合：多个循环合并为单个循环。重新参数化：多个操作简化为单一操作（如多分支架构网络在推理时将其重新参数化为单流架构，RepVGG训练期间每个分支都是许多卷积，训练完成后模型被重新参数化为单序列卷积）。


<!-- TOC --><a name="23-"></a>
## 23. 数据分布偏移

标签偏移：训练数据集50%是垃圾邮件、50%是非垃圾邮件，实际生活中只有10%是垃圾邮件，因此要用加权损失函数更新模型。

<!-- TOC --><a name="30-"></a>
## 30. 有限的有标签数据

多任务学习：垃圾邮件分类是主要任务，邮件分类是辅助任务。多任务多损失函数需要同时优化，辅助任务作为归纳偏置，引导模型优先考虑可解释多任务的假设，可在未见数据表现好。
- 硬参数共享：只有输出层是任务特定，所有任务共享相同的隐藏层和主网络。
- 软参数共享：每个任务用独立神经网络，但会用参数层距离最小化等归一化技术提升网络间相似性。

多模态学习：图像和文本一起输入，用图像-文本匹配损失迫使图像和文本的嵌入向量相似，用两个编码器的嵌入合并为联合嵌入来计算损失。

模型选择：训练模型，模型性能评测，准确度是否高？高，结束；不高，画学习曲线。增加数据是否有帮助？无，选择更大模型；有，收集更多数据（有标注的数据集：迁移学习，没有标注的数据集：小样本学习）。若没有更多数据：人工打标+主动学习，若无法人工打标，让算法打标更多数据（弱监督学习），若不能算法打标：如果是深度神经网络，则自监督学习，否则半监督学习。








