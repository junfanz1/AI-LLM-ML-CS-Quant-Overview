
## 2. 自监督学习

孪生网络：同一个模型对原始样本和施加扰动的样本生成嵌入向量。

## 4. 彩票假设

彩票假设：随机权重初始化，训练网络达到收敛和最佳性能，权重剪枝，将权重重置为较小随机权重值，并重新训练（反复步骤），直到网络缩减到10%大小（中奖彩票），甚至比原来大模型有更好泛化性。

## 6. 减少过拟合

- 知识蒸馏（KL散度）、集成（k-fold）
- BatchNorm、LayerNorm让训练更稳定且有正则化效果，WeightNorm也能提高泛化性

## 7. 多GPU训练

- 模型并行：第一层在GPU1，第二层在GPU2。GPU之间需要相互等待和依赖，不高效
- 数据并行：小批量数据划分成微批量数据，让GPU分别处理一个微批量数据，并计算模型权重的损失和梯度。所有GPU处理完后，梯度被汇总平均，来计算下一轮权重更新。
  - 优势：GPU并行运行，但如果模型太大就不能放进GPU显存。模型必须适配单GPU。
- 张量并行：模型大到无法装进单GPU显存。权重和激活矩阵拆分到各设备，而非将模型层分散到各设备。矩阵拆分可以把矩阵乘法分布到多GPU。
  - 绕过内存限制，又能并行操作。缺点：多GPU间通信开销大。
- 流水线并行：在前向传播时将激活值传递下去。但在反向传播时，输入张量的梯度被传回来，防止设备空闲。
  - 模型并行（最小化串行计算的性能瓶颈，增强部署在不同设备上的模型层的并行性）+数据并行（将小批量数据拆分成微批量数据）。
- 序列并行：Transformer架构大模型处理长序列任务的计算瓶颈，自注意力机制和输入序列的规模是二次方复杂度。缺点：长序列分解子序列可能降低准确性。

## 9. GenAI

- 流模型（归一化流）：用可逆变换，将概率分布转变为更复杂分布。NICE (Non-linear Independent Components Estimation）的解耦层比VAE的卷积神经网络编码器更线性。
- 扩散模型：用随机扩散过程。

## 10. 随机性

- Dropout：以概率p随机剔除一些单元
- 卷积：基于快速傅里叶、Winograd卷积（3x3尺寸滤波器）

## 11. 计算参数

两个卷积层（卷积层1=3个输入通道+5个输出通道+核尺寸5；最大池化层=核尺寸5+步长2；卷积层2=5个输入通道+12个输出通道+核尺寸3；平均池化层=核尺寸3+步长2）+两个全连接层（192个输入单元+128个输出单元）
- 卷积核26个参数 = 5*5权重+1偏置单元
- 卷积层76个参数 = 3组权重*25 +1
- 5个输出通道的5个卷积核380个参数= 5 * 76
- 卷积层1有380个参数 = 5 * （5 * 5 * 3）+5 = 380
- 卷积层2有552个参数 = 12 * （3 * 3 * 5）+12 = 552
- 池化层没有训练参数，卷积总参数= 380 + 552 = 932
- 全连接层1有24704个参数 = 192 * 128 + 128
- 全连接层2输出层有1290个参数 = 128 * 10 + 10
- 全连接层总参数 25994 = 24704 + 1290
- 网络总参数26926 = 25994全连接 + 932卷积

## 12. 全连接层、卷积层
















