# Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [1. AI Agent ](#1-ai-agent)
   * [1.1 技术框架](#11-)
   * [1.2 核心思想](#12-)
   * [1.3 微调](#13-)
- [2. 大模型API封装](#2-api)
- [3. NLP](#3-nlp)
   * [3.1 梯度消失](#31-)
   * [3.2 梯度爆炸](#32-)
   * [3.3 Word2Vec ](#33-word2vec)
   * [3.4 Seq2Seq](#34-seq2seq)
- [4. Transformer](#4-transformer)
   * [4.1 多头注意力Multi-head Attention](#41-multi-head-attention)
   * [4.2 自注意层处理（编码器）](#42-)
   * [4.3 前馈层处理（编码器）](#43-)
   * [4.4 自注意力层处理（解码器）](#44-)
   * [4.5 编码 - 解码注意力层（解码器）](#45-)
   * [4.6 Softmax ](#46-softmax)
   * [4.7优势](#47)
- [5. 训练](#5-)
   * [5.1 分布式训练](#51-)
   * [5.2 推理](#52-)
   * [5.3 压缩](#53-)
   * [5.4 RLHF对齐](#54-rlhf)
- [6. 模型轻量化](#6-)
   * [6.1 参数剪枝](#61-)
   * [6.2 量化](#62-)
   * [6.3 知识蒸馏](#63-)
- [7. 上下文长度](#7-)
- [8. 架构设计](#8-)
- [9. Mamba](#9-mamba)
   * [9.1 Mamba](#91-mamba)
   * [9.2 Mamba架构状态空间模型（SSM）](#92-mambassm)
   * [9.3 SSM与S4](#93-ssms4)
- [10. 机器人](#10-)
- [11. Sora](#11-sora)
   * [11.1 Transformer](#111-transformer)
   * [11.2 扩散模型](#112-)
- [Acknowledgements](#acknowledgements)

<!-- TOC end -->

---

<!-- TOC --><a name="1-ai-agent"></a>
# 1. AI Agent 

<!-- TOC --><a name="11-"></a>
## 1.1 技术框架
- LangChain：大模型应用开发框架。
- LangSmith：统一的 DevOps 平台，用于开发、协作、测试、部署和监控大模型应用程序，同时，LangSmith 是一套 Agent DevOps 规范，不仅可以用于 LangChain 应用程序，还可以用在其他框架下的应用程序中。
- LangServe：部署 LangChain 应用程序，并提供 API 管理能力，包含版本回退、升级、数据处理等。
- LangGraph：一个用于使用大模型构建有状态、多参与者应用程序的库，是 2024 年 1 月份推出的。
- 放到我们传统软件开发场景中，我认为 LangChain 就类似于 SpringCloud；LangSmith 类似于 Jenkins + Docker + K8s + Prometheus；LangServe 类似于 API 网关；LangGraph 类似于 Nacos；当然只是简单比拟。

LangChain 框架本身包含三大模块。
- LangChain-Core：基础抽象和 LangChain 表达式语言。
- LangChain-Community：第三方集成。
- LangChain：构成应用程序认知架构的链、代理和检索策略。

<!-- TOC --><a name="12-"></a>
## 1.2 核心思想
- 大模型是核心控制器，所有的操作都是围绕大模型的输入和输出在进行。链的概念，可以将一系列组件串起来进行功能叠加，这对于逻辑的抽象和组件复用是非常关键的。
- Chains：链条就是各种各样的顺序调用，类似于 Linux 命令里的管道。可以是文件处理链条、SQL 查询链条、搜索链条等等。LangChain 技术体系里链条主要通过 LCEL（LangChain 表达式）实现。既然是主要使用 LCEL 实现，那说明还有一部分不是使用 LCEL 实现的链条，也就是 LegacyChain，一些底层的链条，没有通过 LCEL 实现。
- LCEL：链和 Linux 里的管道很像，通过特殊字符 | 来连接不同组件，构成复杂链条，以实现特定的功能。每个组件的输出会作为下一个组件的输入，直到最后一个组件执行完。当然，我们也可以通过 LCEL 将多个链关联在一起。
- Callback：LangChain 针对各个组件提供回调机制，包括链、模型、代理、工具等。回调的原理和普通开发语言里的回调差不多，就是在某些事件执行后唤起提前设定好的调用。LangChain 回调有两种：构造函数回调和请求回调。构造函数回调只适用于对象本身，而请求回调适用于对象本身及其所有子对象。
- 微调是一种让预先训练好的模型适应特定任务或数据集的方案，成本相对较低，这种情况下，模型会学习训练者提供的微调数据，并且具备一定的理解能力。知识库使用向量数据库或者其他数据库存储数据，为大语言模型提供信息来源外挂。API 和知识库类似，为大语言模型提供信息来源外挂。微调相当于让大模型去学习一门新的学科，在回答的时候进行闭卷考试，知识库和 API 相当于为大模型提供了新学科的课本，回答的时候进行开卷考试。

<!-- TOC --><a name="13-"></a>
## 1.3 微调
- trainable params 指的是在模型训练过程中可以被优化或更新的参数数量。在深度学习模型中，这些参数通常是网络的权重和偏置。它们是可训练的，因为在训练过程中，通过反向传播算法这些参数会根据损失函数的梯度不断更新，以减小模型输出与真实标签之间的差异。通过调整 lora.yaml 配置文件里 peft_config 下面的参数 r 来改变可训练参数的数量，r 值越大，trainable params 越大。微调 trainable params 为 1.9M（190 万），整个参数量是 6B（62 亿），训练比为 3%。
- Loss：损失函数衡量模型预测的输出与实际数据之间的差异或误差。在训练过程中，目标是最小化这个损失值，从而提高模型的准确性。
- Grad_norm（梯度范数）：在训练深度学习模型时，通过反向传播算法计算参数（如权重）的梯度，以便更新这些参数。梯度范数是这些梯度向量的大小或长度，它提供了关于参数更新幅度的信息。如果梯度范数非常大，可能表示模型在训练过程中遇到了梯度爆炸问题；如果梯度范数太小，可能表示存在梯度消失问题。
- Learning_rate（学习率）：学习率是一个控制参数更新幅度的超参数。在优化算法中，学习率决定了在反向传播期间参数更新的步长大小。太高的学习率可能导致训练过程不稳定，而太低的学习率可能导致训练进展缓慢或陷入局部最小值。
- epoch（周期）：一个 epoch 指的是训练算法在整个训练数据集上的一次完整遍历。通常需要多个 epochs 来训练模型，以确保模型能够充分学习数据集中的模式。每个 epoch 后，通常会评估模型在验证集上的表现，以监控和调整训练过程。

<!-- TOC --><a name="2-api"></a>
# 2. 大模型API封装
引入一个类似于 SpringBoot 的框架，用来做接口服务化，在 Python 技术体系里，有一个框架叫 FastAPI，可以很方便地实现接口注册，所以我们这节课会基于 FastAPI 对大模型的接口进行封装。

提供 Web API 服务需要两个技术组件：Uvicorn 和 FastAPI。
- Uvicorn 作为 Web 服务器，类似 Tomcat，但是比 Tomcat 轻很多。允许异步处理 HTTP 请求，所以非常适合处理并发请求。基于 uvloop 和 httptools，所以具备非常高的性能，适合高并发请求的现代 Web 应用。
- FastAPI 作为 API 框架，和 SpringBoot 差不多，同样比 SpringBoot 轻很多，只是形式上类似于 SpringBoot 的角色。结合使用 Uvicorn 和 FastAPI，你可以构建一个高性能、易于扩展的异步 Web 应用程序或 API。Uvicorn 作为服务器运行你的 FastAPI 应用，可以提供优异的并发处理能力，而 FastAPI 则让你的应用开发得更快、更简单、更安全。
- 实际开发过程中，接口输入可能是多个字段，和 Java 接口一样，需要定义一个 Request 实体类来承接 HTTP 请求参数，Python 里使用 Pydantic 模型来定义数据结构，Pydantic 是一个数据验证和设置管理的库，它利用 Python 类型提示来进行数据验证。类似 Java 里的 Validation。
- 这里引入了一个 BaseModel 类，类似于 Java 里的 Object 类，但是又不完全是 Object，Object 是所有 Java 类的基类，Java 中所有类会默认集成 Object 类的公共方法，比如 toString()、equals()、hashcode() 等，而 BaseModel 是为了数据验证和管理而设计的。当你创建一个继承自 BaseModel 的类时，比如上面的 ChatSession 和 Message 类，将自动获得数据验证、序列化和反序列化的功能。
- 实际开发过程中，也不可能把所有 API 的定义和 Pydantic 类放在最外层，按照 Java 工程化的最佳实践，Web 应用我们一般会进行分层，比如 controller、service、model、tool 等，Python 工程化的时候，为了方便管理代码，也会进行分层
- FastAPI 的 include_router 方法就是用来将不同的路由集成到主应用中的，有助于组织和分离代码，特别是在构建大型工程化应用时，非常好用。

```python
project_name/
│
├── app/                         # 主应用目录
│   ├── main.py                  # FastAPI 应用入口
│   └── controller/              # API 特定逻辑
│       └── chat.py
│   └── common/                  # 通用API组件
│       └── errors.py            # 错误处理和自定义异常
│
├── services/                    # 服务层目录
│   ├── chat_service.py          # 聊天服务相关逻辑
│
├── schemas/                     # Pydantic 模型（请求和响应模式）
│   ├── chat_schema.py           # 聊天数据模式
│
├── database/                    # 数据库连接和会话管理
│   ├── session.py               # 数据库会话配置
│   └── engine.py                # 数据库引擎配置
│
├── tools/                       # 工具和实用程序目录
│   ├── data_migration.py        # 数据迁移工具
│
├── tests/                       # 测试目录
│   ├── conftest.py              # 测试配置和夹具
│   ├── test_services/           # 服务层测试
│   │   ├── test_chat_service.py
│   └── test_controller/                
│       ├── test_chat_controller.py
│
├── requirements.txt             # 项目依赖文件
└── setup.py                     # 安装、打包、分发配置文件
```

接口封装
- 我们在 service 层进行模型对话的封装。定义一个 ModelManager 类进行大模型的懒加载。
- 因为在使用 FastAPI 的过程中，会有大量的异步操作，和 Java 的处理方式有点差异，需要注意下。

流式输出接口调用
- Java 调用 Python 接口：主要用到了 okhttp3 框架，需要组装参数、发起流式请求，事件监听处理三步。
- 前端调用 Java 接口：使用 JS 原生 EventSource 的 API 就可以。

<!-- TOC --><a name="3-nlp"></a>
# 3. NLP

<!-- TOC --><a name="31-"></a>
## 3.1 梯度消失

两个原因
- 深层网络中的连乘效应：在深层网络中，梯度是通过链式法则进行反向传播的。如果每一层的梯度都小于 1，那么随着层数的增加，这些小于 1 的值会连乘在一起，导致最终的梯度非常小。
- 激活函数的选择：使用某些激活函数，如 tanh，函数的取值范围是 -1～1，小于 1 的数进行连乘，也会快速降低梯度值。

解决方法
- 长短期记忆（LSTM）和门控循环单元（GRU）是专门为了避免梯度消失问题而设计的。它们通过引入门控机制来调节信息的流动，保留长期依赖信息，从而避免梯度在反向传播过程中消失。
- 使用 ReLU 及其变体激活函数，在正区间内的梯度保持恒定，不会随着输入的增加而减少到 0，这有助于减轻梯度消失问题。

<!-- TOC --><a name="32-"></a>
## 3.2 梯度爆炸
三个原因
- 深层网络的连乘效应：在深层网络中，梯度是通过链式法则进行反向传播的。如果每一层的梯度都大于 1，那么随着层数的增加，这些大于 1 的值会连乘在一起，导致最终的梯度非常大。
- 权重初始化不当：如果网络的权重初始化得太大，那么在前向传播过程中信号的大小会迅速增加，同样，反向传播时梯度也会迅速增加。
- 使用不恰当的激活函数：某些激活函数（如 ReLU）在正区间的梯度为常数。如果网络架构设计不当，使用这些激活函数也可能导致梯度爆炸。

解决方法
- 梯度爆炸和梯度消失基本相反，解决方法一样，要么使用长短期记忆和门控循环单元调整网络结构，要么替换激活函数，还有一种办法是进行梯度裁剪，梯度裁剪意思是在训练过程中，通过限制梯度的最小 / 大值来防止梯度消失 / 爆炸，间接地保持梯度的稳定性。

<!-- TOC --><a name="33-word2vec"></a>
## 3.3 Word2Vec 
两种主要架构：
- 连续词袋（Continuous Bag of Words, CBOW），CBOW 尝试从一个词的“上下文”来预测这个词本身。
- 跳字模型（Skip-Gram），与 CBOW 模型相反，Skip-Gram 每次接收一个词作为输入，并预测它周围的词，这使其在处理较大数据集和捕获罕见词或短语时表现更出色。

<!-- TOC --><a name="34-seq2seq"></a>
## 3.4 Seq2Seq
如果说 Word2Vec 是让我们的机器学会了理解词汇的话，那 Seq2Seq 则是教会了机器如何理解句子并进行相应地转化。在这个过程中，我们会遇到两个核心的角色：编码器（Encoder）和解码器（Decoder）。编码器的任务是理解和压缩信息，就像是把一封长信函整理成一个精简的摘要；而解码器则需要将这个摘要展开，翻译成另一种语言或形式的完整信息。

<!-- TOC --><a name="4-transformer"></a>
# 4. Transformer
![image](https://github.com/user-attachments/assets/061d494d-3b22-4dc6-a8b1-9b402572db0c)

<!-- TOC --><a name="41-multi-head-attention"></a>
## 4.1 多头注意力Multi-head Attention
Transformer 模型中的一个关键创新。它的核心思想是将注意力机制“分头”进行，即在相同的数据上并行地运行多个注意力机制，然后将它们的输出合并。这种设计允许模型在不同的表示子空间中捕获信息，从而提高了模型处理信息的能力。Transformer 默认 8 个头，其工作过程如下：
- 分割：对于每个输入，多头注意力首先将查询、键和值矩阵分割成多个“头”。这是通过将每个矩阵分割成较小的矩阵来实现的，每个较小的矩阵对应一个注意力“头”。假设原始矩阵的维度是 d_model​，那么每个头的矩阵维度将是 d_model​/h ，其中 h 是头的数量。
- 并行注意力计算：对每个头分别计算自注意力。由于这些计算是独立的，它们可以并行执行。这样每个头都能在不同的表示子空间中捕获输入序列的信息。
- 拼接和线性变换：所有头的输出再被拼接起来，形成一个与原始矩阵维度相同的长矩阵。最后，通过一个线性变换调整维度，得到多头注意力的最终输出。

<!-- TOC --><a name="42-"></a>
## 4.2 自注意层处理（编码器）
数据经过 Add & Norm 操作进入前馈处理层（Feed Forward）
- Add 表示残差连接，是指在自注意力层后把这一层处理过的数据和这一层的原始输入相加，这种方式允许模型在增加额外处理层的同时，保留输入信息的完整性，从而在不损失重要信息的前提下，学习到输入数据的复杂特征。具体来说，如果某一层的输入是 x，层的函数表示为 f(x)，那么这层的输出就是 x+f(x)。这样做主要是为了缓解深层网络中的梯度消失或梯度爆炸问题，使深度模型更容易训练。
- 可以缓解梯度消失问题。因为 x+f(x)，而不仅仅是 f(x)，这样在反向传播过程中，可以有多条路径，可以减轻连续连乘导致梯度减少到 0 的问题。
- Norm 表示归一化（Normalization），数据在经过 Add 操作后，对每一个样本的所有特征进行标准化，即在层内部对每个样本的所有特征计算均值和方差，并使用这些统计信息来标准化特征值。这有助于避免训练过程中的内部协变量偏移问题，即保证网络的每一层都在相似的数据分布上工作，从而提高模型训练的稳定性和速度。

<!-- TOC --><a name="43-"></a>
## 4.3 前馈层处理（编码器）
- 通过两次线性映射和一个中间的 ReLU 激活函数，FFN 引入了必要的非线性处理，使模型能够捕捉更复杂的数据特征。FFN(x)=max(0,xW1​+b1​)W2​+b2​
- Transformer输入和编码器4 个要点：1+2+1。1 表示位置编码，方便处理序列顺序。2 表示两层：自注意力层和前馈网络层。1 表示每一层进入下一层前都需要进行 Add & Norm 操作。

<!-- TOC --><a name="44-"></a>
## 4.4 自注意力层处理（解码器）
与编码器的自注意力层不同，解码器的自注意力层需要处理额外的约束，即保证在生成序列的每一步仅依赖于之前的输出，而不是未来的输出。这是通过一个特定的掩蔽（masking）技术来实现的。
- 处理序列依赖关系，解码器的自注意力层使每个输出位置可以依赖于到目前为止在目标序列中的所有先前位置。这允许模型在生成每个新词时，综合考虑已生成的序列的上下文信息。
- 掩蔽未来信息，为了确保在生成第 t 个词的时候不会使用到第 t+1 及之后的词的信息，自注意力层使用一个上三角掩蔽矩阵，在实现中通常填充为负无穷或非常大的负数。这保证了在计算 Softmax 时未来位置的贡献被归零，从而模型无法“看到”未来的输出。
- 动态调整注意力焦点，通过学习的注意力权重，模型可以动态地决定在生成每个词时应更多地关注目标序列中的哪些部分。
- 解码器中的自注意力层至关重要，因为它不仅提供了处理序列内依赖关系的能力，还确保了生成过程的自回归性质，即在生成当前词的时候，只依赖于之前已经生成的词。这种机制使 Transformer 模型非常适合各种序列生成任务，如机器翻译、文本摘要等。之所以有这种机制，是因为自注意力机制允许当前位置的输出与未来位置的输入产生关联，从而导致数据泄露和信息泄露的问题。而推理阶段，是不可能读到未来信息的，这样可能会导致模型在训练和推断阶段表现不一致，以及模型预测结果的不稳定性。

<!-- TOC --><a name="45-"></a>
## 4.5 编码 - 解码注意力层（解码器）
- 编码 - 解码注意力层（Encoder-Decoder Attention Layer）是一种特殊的注意力机制，用于在解码器中对输入序列（编码器的输出）进行注意力计算。这个注意力层有助于解码器在生成输出序列时对输入序列的信息进行有效整合和利用，注意，这个注意力层关注的是全局的注意力计算，包括编码器输出的信息序列和解码器内部的自注意力计算。
- 与上面的解码器自注意力层的区别
  - 信息来源不同：编码 - 解码注意力层用在解码器（Decoder）中，将解码器当前位置的查询向量与编码器（Encoder）的输出进行注意力计算，而解码自注意力层用于解码器自身内部，将解码器当前位置的查询向量与解码器之前生成的位置的输出进行注意力计算。
  - 计算方式不同：编码 - 解码注意层计算当前解码器位置与编码器输出序列中所有位置的注意力分数。这意味着解码器在生成每个输出位置时，都可以综合考虑整个输入序列的信息。解码自注意力层计算当前解码器位置与之前所有解码器位置的输出的注意力分数。这使得解码器可以自我关注并利用先前生成的信息来生成当前位置的输出。
- 编码 - 解码注意层关注整个编码器输出序列，将编码器的信息传递给解码器，用于帮助解码器生成目标序列，解码自注意力层关注解码器自身先前生成的位置的信息，用于帮助解码器维护上下文并生成连贯的输出序列。

<!-- TOC --><a name="46-softmax"></a>
## 4.6 Softmax 
核心就是一句话：将一组任意实数转换成一个概率分布。

为什么需要通过 Softmax 函数进行计算？意义在哪里？
- 将得分转换成概率后，模型能够更加明确地选择哪些输入的部分是最相关的。
- 在神经网络中，直接处理非常大或非常小的数值可能会导致数值不稳定，出现梯度消失或爆炸等问题。通过 Softmax 函数处理后，数据将被规范化到一个固定的范围，从 0 到 1 之间，可以缓解这些问题。
- Softmax 函数的输出是概率分布，这使得模型的行为更加透明，可以直接解释为“有多少比例的注意力被分配到特定的输入上”。这有助于调试和优化模型，以及理解模型的决策过程。

将任意实数转化为概率分布，数据的意义将会发生变化，会不会对效果产生影响？
- 原始的得分只表达了相对大小关系，即一个得分比另一个高，但不清楚这种差异有多大。而通过 Softmax 转换后，得到的概率值不仅反映出哪些得分较高，还具体表达了它们相对于其他选项的重要性。这种转换让模型可以做出更精确的决策。
- 原始得分可能因范围广泛或分布不均而难以直接操作，而概率形式的输出更标准化、更规则，适合进一步的处理和决策，如分类决策、风险评估等。

<!-- TOC --><a name="47"></a>
## 4.7优势
- 并行处理能力：与传统的循环神经网络（RNN）和长短期记忆网络（LSTM）不同，Transformer 完全依赖于自注意力机制，消除了序列处理中的递归结构，允许模型在处理输入数据时实现高效的并行计算。这使得训练过程大大加速，特别是在使用现代 GPU 和 TPU 等硬件时。你可以回顾一下 Position Encoding 的作用，就是为序列添加位置编码，以便在并行处理完以后，进行合并。
- 捕捉长距离依赖：Transformer 通过自注意力机制能够捕捉序列中的长距离依赖关系。在自然语言处理中，这意味着模型可以有效地关联文本中相隔很远的词汇，提高对上下文的理解。
- 灵活的注意力分布：多头注意力机制允许 Transformer 在同一个模型中同时学习数据的不同表示。每个头可以专注于序列的不同方面，例如一个头关注语法结构，另一个头关注语义内容。
- 可扩展性：Transformer 模型可以很容易地扩展到非常大的数据集和非常深的网络结构。这一特性是通过模型的简单可堆叠的架构实现的，使其在训练非常大的模型时表现出色。

<!-- TOC --><a name="5-"></a>
# 5. 训练
总参数量 = 嵌入层参数量 + 位置编码参数量 + 解码器层参数量 + 线性输出层参数量。
- 嵌入层参数量 = vocab_size * embed_size
- 位置编码参数量 = embed_size
- 解码器层参数量 =（自注意力机制参数量 + 前向馈网络参数量）* 层数
- 线性输出层参数量 = vocab_size * embed_size
模型文件，也可以叫模型权重，里面大部分空间存放的是模型的参数：权重（Weights）和偏置（Biases），当然也有一些其他信息，比如优化器状态、其他元数据，比如 epoch 数等。我们使用的是 PyTorch 框架，生成的模型权重文件格式是.pth，如果使用 TensorFlow 或者 Hugging Face Transformers 等框架，也有可能是.bin 格式的文件。模型预训练完成后，我们可以调用下面的代码保存模型。模型的训练过程就是不断调整权重的过程，调整权重的依据就是根据损失函数计算损失，通过反向传播不断调整权重使损失最小，达到理想损失值后，把权重记录下来保存。我们可以使用 Netron、torchviz 等工具可视化模型结构，辅助理解。

二元分类问题，最常用的损失函数是二元交叉熵损失（Binary Cross-Entropy Loss）。当输出是一个概率值，并且标签是 0 或 1 的时候，这种方法非常合适。

<!-- TOC --><a name="51-"></a>
## 5.1 分布式训练

DeepSpeed 是由微软开发的一个非常优秀的分布式训练库，专为大规模和高效的深度学习训练设计，在分布式训练领域提供了多项创新的技术，比如并行训练、并行推理、模型压缩等。

- DeepSpeed 灵活组合三种并行方法：数据并行性、管道并行性和模型并行性，简称 3D 并行性，可适应不同工作负载的需求。目前已经支持超过一万亿超大参数的模型，实现了近乎完美的内存扩展和吞吐量扩展效率。
- 数据并行性：DeepSpeed 提供了一种高效的数据并行方法，能够在不使用模型并行技术的情况下进行模型训练。例如通过一个单独的 GPU，就能够处理多达 130 亿参数的模型。相比而言，传统的框架，比如 PyTorch 的分布式数据并行处理技术，最多在处理 14 亿参数时就可能会出现内存溢出。这里主要使用的是 ZeRO 技术，通过对模型状态和梯度进行分割，来大幅节约内存，并同时减少了激活和碎片内存的使用。现阶段的 ZeRO-2 能够将内存需求降低至八分之一。
- 模型并行性：模型并行性是指将模型的权重切分成更小的权重切片，切分方法有好几种，像层级并行、张量并行、专家并行等。模型并行主要是用在模型远大于设备显存的情况，比如模型大小为 3100GB，而显存大小为 80GB 甚至 40GB，当然处理起来也更加复杂。
- 管道并行性：将模型分成几个较小的部分或“阶段”，并行化处理，每个阶段由不同的处理单元负责。通过这种方式，不同的输入数据可以在不同阶段同时进行处理，从而提高整体的处理效率和硬件利用率。每个处理单元负责其中一个部分的前向和反向传播。当一批数据完成在一个处理单元上的计算后，会被立即传送到下一个处理单元继续接下来的计算，而当前的处理单元则可以开始处理新的输入数据。

<!-- TOC --><a name="52-"></a>
## 5.2 推理
- 最新的版本是 DeepSpeed-FastGen，利用 DeepSpeed-MII 和 DeepSpeed-Inference 的组合来提供服务。在 Llama2-70B 上做测试，推理速度提升 2.3 倍。DeepSpeed-FastGen 核心技术是 Dynamic SplitFuse，就是将长提示分解成更小、更易于管理的块，这些块可以在多个前向传递中更高效地处理。大概处理流程是这样的：
- 将长的输入提示动态分解为更小、更易管理的块来工作。这些小块可以是词、短语或句子，取决于具体实现和模型的需求。这种分解允许系统并行处理多个块，从而优化了资源使用和响应时间。
- 分解后的小块将被组织成连续的批次，这些批次可以在不同的处理单元上同时进行前向和反向传递。连续批处理技术确保了系统可以持续处理输入，而不需要等待整个长提示处理完成，从而提高了整体的吞吐量，降低了延迟。
- 在处理每个批次时，Dynamic SplitFuse 使用一种优化的令牌组合策略，这种策略调整了令牌生成的顺序和速度，以最大化处理效率和输出质量。这可能涉及到智能地预测哪些令牌或块应该优先处理，以及如何最有效地利用可用的计算资源。

<!-- TOC --><a name="53-"></a>
## 5.3 压缩
- 使用极限压缩技术，可将模型大小缩小 32 倍，而几乎没有精度损失，或者在保留 97% 的精度的同时实现模型大小缩小 50 倍。
- 量化、修剪、减少层数。

<!-- TOC --><a name="54-rlhf"></a>
## 5.4 RLHF对齐
近端策略优化 PPO：训练机器在某个环境中做出决策，核心思想是渐进式地调整决策策略，意思是在训练过程中，它会尝试轻微地修改机器的决策方式，而不是做出大幅改变。这样做的好处是可以避免一些突然的、不稳定的决策变化，让学习过程更平稳、更可靠。PPO 在优化策略时，会考虑当前策略和新策略之间的差异，并尽量保持这种差异在一个安全的范围内。就像你教小孩子骑自行车，不会突然推他们到下一个陡峭的坡道，而是逐渐增加难度，让他们能够逐步适应。

[Ten Levels of AI Alignment Difficulty](https://www.alignmentforum.org/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty)

![image](https://github.com/user-attachments/assets/3ca13da9-966d-43b4-b4ff-5bb41f58d95c)


<!-- TOC --><a name="6-"></a>
# 6. 模型轻量化

<!-- TOC --><a name="61-"></a>
## 6.1 参数剪枝
- 执行完剪枝后，发现模型并没有减少，甚至有可能会增大，那是因为在 PyTorch 中，剪枝实际上是通过在原有的模型参数上添加掩码来实现的，而不是真正地移除那些参数。这意味着剪枝操作增加了额外的状态信息，比如掩码，从而可能导致模型文件总大小增加。
- 为了确保剪枝效果永久化并减少模型的大小，你需要在保存模型前将掩码应用到原始权重并移除剪枝带来的额外属性。使用 prune.remove 函数来把 weight_orig 替换回 weight，并移除掩码。这一步确保了模型权重不再依赖于掩码，然后保存简化模型即可。
- 剪枝完成，模型的性能可能会有所下降，需要通过再次训练或者微调模型，恢复一部分性能损失，最后，再次评估模型性能，确保它仍然满足应用要求。

<!-- TOC --><a name="62-"></a>
## 6.2 量化
动态量化主要针对模型的权重进行量化，每次输入时都会重新计算量化参数。这种方法通常用于模型的线性层（全连接层）和循环层（如 LSTM）。动态量化可以在不需要重新训练模型的情况下实现，适合那些对推理速度有较高要求的应用场景。

<!-- TOC --><a name="63-"></a>
## 6.3 知识蒸馏
- 训练教师模型：首先训练一个大型的高性能的教师模型。这个模型通常是尽可能地大和复杂，以达到高准确率。
- 定义学生模型：设计一个比教师模型小得多的学生模型。学生模型的架构不必与教师模型相同，但通常需要能够学习相似类型的特征。
- 蒸馏过程：使用教师模型的输出来训练学生模型。这通常涉及调整学生模型的损失函数，使其不仅要学习真实的标签，还要学习模仿教师模型的输出。
- 评估学生模型：测试学生模型的性能，确保它能够达到预期的精度，同时具有更小的模型大小或更快的推理速度。
- 教师模型的输出通常是经过 Softmax 函数处理的概率分布，被叫做“软标签”。相对于硬标签，也就是真实标签（0 或 1 的类别标签），软标签能够提供关于各个类别相对可能性的更多信息。比如，在分类任务中，一个图像可能有 90% 的概率是猫，10% 的概率是狗，而不是简单地标记为“猫”。这种概率分布包含了数据的不确定性和类别之间相似性的信息，有助于学生模型更全面地理解数据。
```python
import torch
import torch.nn as nn
import torch.optim as optim

# 假设teacher_model和student_model已经定义并加载
teacher_model.eval()  # 确保教师模型在评估模式

# 定义学生模型
student_model = StudentModel()

# 损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.Adam(student_model.parameters(), lr=0.001)

# 数据加载器
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# 蒸馏过程
for epoch in range(num_epochs):
    student_model.train()
    for data, target in train_loader:
        optimizer.zero_grad()

        # 教师模型的输出
        with torch.no_grad():
            teacher_output = teacher_model(data)

        # 学生模型的输出
        student_output = student_model(data)
        # 计算损失：可以根据需要混合教师输出和真实标签
        loss = criterion(student_output, teacher_output)  # 以教师模型的输出作为目标
        # 反向传播和优化
        loss.backward()
        optimizer.step()

# 评估学生模型性能
student_model.eval()
# 进行测试和评估...
```

<!-- TOC --><a name="7-"></a>
# 7. 上下文长度

AI 产品的终极价值是提供个性化的交互，⽽lossless long-context 是实现这⼀点的基础。模型的微调不应该⻓期存在， 用户跟模型的交互历史就是最好的个性化过程。

⼤模型能够达到的最⾼⽔平由两个因素决定：一个是单步骤的容量，即模型在每⼀步中可以处理的信息量，对应参数量；另一个是执⾏的步骤数，也就是模型能够处理的上下⽂⻓度。

常用方法
- 稀疏注意力机制：元素只与序列中选择的部分元素建立这种关系。这种选择可以基于预定义的模式，比如局部窗口、固定模式等，也可以是通过学习得到的动态模式。
- 降采样就比较粗暴了，就是一种数据减少技术，减少输入序列，同时尽量保留重要信息，比如只选择序列中的某些部分单词，或通过合并相邻的元素，来创建一个更短的序列。

Kimi
- 模型训练方面：在传统的 Tensor 并⾏、Data 并⾏、Pipeline 并⾏基础上，增加了多项基于 Seqence 维度的并⾏策略，提升了并⾏效率。利⽤定制版的 Flash Attention、Fuse Cross Entropy、CPU offload 等技术⼤幅度降低了显存压⼒。还使⽤了创新的训练⽅法，针对性地调配了多阶段式训练⽅法，让模型保留基础能力的前提下，逐步激活⻓上下⽂的能⼒。
- 模型推理方面：⽤GQA 替换 MHA：让 KVCache 所占⽤的显存⼤⼩⼤幅度缩⼩。2Paged attention：保证显存的充分利⽤。低⽐特量化：通过 W8A8，最多可以把推理速度在上述基础上再提升⼀倍。MoE & KVCache 裁减：让显存占⽤在上述基础上再下降⼀倍。

<!-- TOC --><a name="8-"></a>
# 8. 架构设计

- PDF 处理：这块我们通过一个 Agent 去处理，包含 PDF 切分、OCR 识别等。PDF 处理可能会面临各种各样的问题，比如如果 PDF 文件里是图像嵌入的，那么就需要 OCR，一旦使用 OCR 就会涉及正确率的问题；再比如文件处理本身就是资源密集型操作，容易导致系统负载过重的情况。PDF 处理工具有很多种，比如 pdfplumber、PyPDF2、pytesseract 等，有的是纯文本 PDF 内容抽取，比如 PyPDF2，有的是 OCR 识别，如 pytesseract，作为一个通用工具，我们可以先检测 PDF 中是否包含图片，不包含图片的话直接使用 PyPDF2。
- 词嵌入：Google 的 Word2Vec，Meta 的 fastText
- 向量数据库： Meta 的 faiss
- Agent：我们把处理 PDF 的整个过程放到一个 Agent 内处理，处理的结果直接喂给大模型，大模型输出的内容可以调用 Agent 的另一个 tool 进行组装，返回给用户。这里主要涉及 prompt 组装（需要提前设定好 prompt 模版）、文本格式化、文件生成等内容。Agent 集成，可以使用像我们前面学习的 LangChain 框架，也可以自己编写集成代码。
- LLM：本地搭建或云上模型服务


<!-- TOC --><a name="9-mamba"></a>
# 9. Mamba

Transformer 模型中自注意力机制的计算量会随着上下文长度的增加呈平方级增长，比如上下文长度增加 32 倍时，计算量可能会增长 1000 倍，计算效率非常低。为什么会这样？因为 Transformer 模型在计算自注意力时，每个输入元素都要与序列中的其他元素进行比较，导致总体计算复杂度为 O(n^2∗d)，其中 n 是序列长度，d 是元素表示的维度。

<!-- TOC --><a name="91-mamba"></a>
## 9.1 Mamba
- 基于 S4 架构：S4 全称是 Structured State Spaces for Sequence Modeling，用于序列建模的结构化状态空间，是一种针对序列建模的高效模型。它结合有选择性的（Selective）状态空间模型（SSM）和深度学习技术，旨在处理长序列的依赖关系，并突破计算瓶颈。S4 使用线性递归方程和频域方法，显著提升计算效率，同时通过结构化参数设计捕捉长距离依赖。该模型在自然语言处理、时间序列预测和语音处理等任务中表现优异。S4 的核心优势在于其计算效率和捕捉远程依赖关系的能力，为长序列建模提供了一种创新且强大的解决方案。
- 高效性：Mamba 在计算效率上表现突出，特别是在处理大规模数据的时候。与 Transformer 相比，Mamba 采用了一些优化技术，比如 S4 和动态压缩，使计算复杂度更低。
- 适应性：Mamba 通过精心设计的层结构和连接方式，有效地提取和处理图像特征，同时使用加速技术提升计算效率。此外，Mamba 支持多任务学习，在图像分类、目标检测和图像分割等任务中表现优异。模型通过正则化技术和数据增强方法提高了鲁棒性和泛化能力，能够适应不同数据集。同时，在一些层与层之间的连接过程中，将上一层的输出和下一层的输入进行权重绑定，达到权重共享的目的，这样就相当于减少了一部分参数，通过这样的优化，使 Mamba 在保持高性能的同时，减少了计算和存储成本，进一步增强了其实用性。这种多功能性使 Mamba 在各类应用场景中都有出色表现。
- 内存利用：Mamba 优化了内存分配和使用，具体来说，就是引入了一种动态序列长度调整（DSLA）的新技术，允许网络根据输入序列的复杂性和长度调整其内存大小，这样就可以更有效地使用内存，减少了内存消耗
- 训练速度：Mamba 的训练速度比 Transformer 快得多，这对于需要快速迭代和测试的开发者来说非常重要。
- 性能表现：特别是在处理长序列数据和复杂任务时，Mamba 表现得特别出色。

<!-- TOC --><a name="92-mambassm"></a>
## 9.2 Mamba架构状态空间模型（SSM）
- SSM 描述的是动态系统，例如在 3D 空间中移动的物体，可以通过两个方程根据其在时间 t 的状态进行预测，其核心是两个方程：状态方程和输出方程（或者也可以称为：观测方程）。h’(t)=Ah(t)+Bx(t)，y(t)=Ch(t)+Dx(t)，目标是找到这个状态 h(t)，以便我们可以从输入计算得出输出序列。状态方程描述的是，矩阵 A 和 B 如何根据输入值和上一个状态值推导当前状态的值。输出方程描述的是，矩阵 C 和 D 如何通过状态值和输入值推导输出值的过程。矩阵 A、B、C 和 D 就是我们常说的参数，它们是可学习的。矩阵 D 实际上没有参与 SSM，直接从输入到输出，所以关于矩阵 D 的连接被称为跳跃连接。而矩阵 D 没有参与到 SSM 序列计算，所以我们说 SSM 是没有跳跃连接的。
![image](https://github.com/user-attachments/assets/783fe14f-4746-499c-a1e3-cff73ee0871a)

- 连续信号离散化：通过连续信号去计算状态是有一定的难度的，而通常我们的输入是离散的，所以我们需要将模型就行离散化。为了实现离散化，使用了一个叫 Zero-order hold 的技术，工作流程大概是这样的：每当接收到离散信号，先保留其值，直到收到新的离散信号，这样的话，使得 SSM 可以使用连续信号。离散化 SSM 使我们能够以特定的时间步长而不是连续信号来处理问题，这一点类似于我们前面讲过的循环神经网络 RNN，循环方法在这里也适用，在每个时间步，我们计算当前输入 $B_x_k​$ 如何影响之前的状态 $A_h_{k−1}$​，然后计算预测输出 $C_h_k​$
![image](https://github.com/user-attachments/assets/66c07a40-c3f6-4945-8910-0410bba174a2)

- SSM 厉害之处在于，在训练的时候，我们可以使用卷积表示，而在推理的时候，可以选择使用循环表示，这一点使 Mamba 具备了训练和推理都很高效的特性。该模型被称为线性状态空间层（LSSL），这些表示具有一个重要特性，即线性时间不变性（LTI）。LTI 指出，SSM 的参数 A、B 和 C 在所有时间步长上都是固定的。这意味着对于 SSM 生成的每个 token，矩阵 A、B 和 C 都是相同的。
- 矩阵 A 的值贯穿整个状态计算过程。因此矩阵 A 的创建自然不能随机产生，我们希望他能包含历史信息，也就是上下文信息，Mamba 使用 HiPPO 创建矩阵 A，关于 HiPPO 的解释，简单来看，就是一种通过多项式来进行历史信息循环记忆的方法。
  - Hippo: https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html 
  - Hippo 为什么能记住历史状态：https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html 

<!-- TOC --><a name="93-ssms4"></a>
## 9.3 SSM与S4
- S4 全称是 Structured State Spaces for Sequence Modeling，用于序列建模的结构化状态空间，是一种针对序列建模的高效模型。S4 基于有选择性的 SSM，也就是说 S4 是 SSM 的一种具体实现。结合刚刚的分析来看，S4 其实就是将 HiPPO 应用到上文提的卷积和循环表示，用来处理长距离依赖关系。由三部分组成：状态空间模型 SSM；HiPPO；用于创建循环和卷积表示的离散化。基于这样的架构，Mamba 拥有了多种优势，既有卷积网络的并行训练特性，又有循环网络的快速推理能力，同时还具备 Transformer 的长距离依赖，可谓是集各种技术的优点与一身。
- 选择性 SSM：选择性其实就是有选择的记录历史信息，SSM 的循环表示会创建一个非常高效的小状态，它会压缩整个历史记录，与不压缩历史记录（通过注意矩阵）的 Transformer 模型相比，它的功能要弱得多，但是 Mamba 的目标是兼具两全其美，小状态和 Transformer 状态一样强大。
- 如何进行压缩？矩阵 ABC 与输入无关，而且在不同的时间序列内，ABC 都相同。这样的话，当我们要对输入序列进行选择的时候，无法确认要丢掉哪部分信息，所以我们其实是希望 ABC 矩阵的值和输入产生关联，这样就可以对输入内容进行判断，进而进行取舍。Mamba 把输入的序列和批量大小进行合并，让矩阵 B 和 C 以及步长依赖输入。对于每个输入的序列，可以有不同的矩阵 B 和 C 对应，从而也能感知内容的不同，这种情况下，选择保留什么忽略什么就可以做到了。
![image](https://github.com/user-attachments/assets/7ab6214e-2903-4439-961f-55da3ddd4489)


缺点：复杂性高、资源需求大、迁移学习难度大、实验验证比较困难。


<!-- TOC --><a name="10-"></a>
# 10. 机器人

- RT-2 就是 Robotics Transformer-2，是谷歌 DeepMind 团队开发的一种结合大规模 Transformer 模型和强化学习的机器人技术。RT-2 利用 Transformer 模型的强大学习能力和适应能力，使机器人在复杂任务中表现更加出色。RT-2 建立在视觉 - 语言模型（VLM）的基础上，又创造了一种新的概念：视觉 - 语言 - 动作（VLA）模型，它可以从网络和机器人数据中进行学习，并将这些知识转化为机器人可以控制的通用指令。
- Q-Transformer 的核心思想是在 Transformer 模型的基础上，引入 Q-learning 等强化学习算法，用于处理时序决策任务。Q-Transformer 可以用于优化机器人的路径规划和任务执行。通过学习环境中的状态变化，机器人可以更智能地选择行动路径，避免障碍物，提高任务完成效率。还有 AutoRT、SARA-RT、RT-Trajectory、RT-H 等技术框架。

<!-- TOC --><a name="11-sora"></a>
# 11. Sora

<!-- TOC --><a name="111-transformer"></a>
## 11.1 Transformer
- 视觉转换器（ViT）：为了将 token 的概念应用于视觉数据，将图像分成 16x16 的小块（patch），每个 patch 在 Transformer 中被视为一个“token”。这种方法使模型能够学习每个 patch 在整幅图像中的关系，从而在此基础上识别和理解整幅图像。它突破了传统 CNN 模型在图像识别中固定接受域大小的限制，可以灵活地捕捉图像中的任意位置关系。
- 视频视觉转换器（ViViT）：扩展了 Vision Transformer 的概念，将其应用于视频的多维数据。视频数据更加复杂，因为它既包含静态图像信息，比如空间元素，也包含随时间变化的动态信息，比如时间元素。ViViT 通过将视频分解为时空块，并将其视为 Transformer 模型中的标记，成功实现了这一点。通过引入时空块，ViViT 能够同时捕捉视频中的静态和动态元素，并模拟它们之间的复杂关系。
- 掩蔽自动编码器（MAE）：“Masked Autoencoder”的自监督预训练方法。这个方法显著改善了传统方法在处理高计算成本和高维、海量信息大数据集时的低效率问题。具体来说，它通过对输入图像进行部分掩蔽，训练网络预测隐藏部分的信息，从而更高效地学习图像中的重要特征和结构，并获得丰富的视觉数据表征。这个过程不仅使数据的压缩和表征学习更加高效，还降低了计算成本，增强了对不同类型视觉数据和任务的通用性。这种方法与 BERT 的演化有密切关联。BERT 通过掩码语言模型（MLM）实现了对文本数据的深度上下文理解，而 He 等人将类似的掩码技术应用于视觉数据，实现了对图像更深入的理解和表示。
- 原始分辨率视觉变换器（NaViT）：这个模型旨在进一步扩展视觉转换器（ViT）对任何长宽比或分辨率图像的适用性。NaViT 旨在高效处理任何长宽比或分辨率的图像，无需事先调整即可直接将其输入模型。Sora 也将这种灵活性应用于视频，能够无缝处理各种尺寸和形状的视频和图像，从而显著提高了灵活性和适应性。

<!-- TOC --><a name="112-"></a>
## 11.2 扩散模型
使用非平衡热力学进行深度学习，通过引入扩散过程，从随机噪声（没有任何模式的数据）开始，逐渐消除这种噪声，最终创建出类似于实际图像或视频的数据。Sora 可以观察不同的视频和图像并学习。

- 2020 年 Ho 等人在其论文《Denoising Diffusion Probabilistic Models》（去噪扩散概率模型）中，以及 Nichol 和 Dhariwal 在其 2021 年的论文《Improved Denoising Diffusion Probabilistic Models》（改进的去噪扩散概率模型）中，基于 Sohl-Dickstein 等人（2015 年）的理论框架，开发了一种实用的数据生成模型，即去噪扩散概率模型 (DDPM)。
- 潜在扩散模型：2022 年，Rombach 等人在他们的论文《High-resolution image synthesis with latent diffusion models》（利用潜在扩散模型进行高分辨率图像合成）中，提出了一种方法，与直接生成高分辨率图像相比，该方法在保持质量的同时，利用潜在空间中的扩散模型，大大降低了计算成本。换句话说，它表明，通过对潜在空间中表示的数据进行编码和引入扩散过程，可以用更少的计算资源实现这一目标，而不是直接处理图像。Sora 将此技术运用到视频数据中，将视频的时间 + 空间数据压缩到低维的潜在空间中，再进行时空块的分解，这种高效的潜在空间数据处理和生成能力，对于 Sora 能够更快速地生成更高质量的视觉内容起到了至关重要的作用。这项研究为利用扩散模型进行高分辨率图像合成领域做出了非常大的贡献。
- 扩散 Transformer（DiT）：2023 年，Peebles 和 Xie 在他们的论文《Scalable diffusion models with transformers》（带有变换器的可扩展扩散模型）中提出了实现 Sora 的关键。正如 OpenAI 发布的技术报告中所述，Sora 使用的不是普通 Transformer，而是扩散 Transformer（DiT）。该结构通过 Transformer 对潜在块的操作实现了潜在扩散模型。这种方法可以更有效地处理图像块，从而能够在有效利用计算资源的同时生成高质量的图像。与 Stability AI 在 2022 年宣布的稳定扩散不同，加入这种 Transformer 被认为有助于更自然的视频生成。
- Sora 使用 Transformer 结构取代了扩散模型中常用的 U-net 组件。
  - 视频压缩网络：OpenAI 训练了一个降低视觉数据维度的网络，这个网络接受原始视频作为输入，然后进行视频压缩，也就是降低数据维度，最后输出的是在时间和空间上压缩过的表示形式。压缩并不意味着忽略原始数据的特性，而是将它们转化为一种对 Sora 来说更小、更容易理解和操作的格式。
  - 空间时间补丁：接下来，Sora 将这些压缩后的数据进一步分解为“空间时间补丁”（Spacetime Patches），这些补丁可以看作是视觉内容的基本构建块，例如照片可以分解为包含独特景观、颜色和纹理的小片段。这样不管原始视频的长度、分辨率或风格如何，Sora 都可以将它们处理成一致的格式。
  - Diffusion Transformer 模型
  - 语言理解与提示：OpenAI 将 DALL·E 3 中介绍的标题生成技术用到了视频领域，训练了一个具备高度描述性的视频标题生成模型，使用这个模型为所有的视频训练数据生成了高质量文本标题，再将视频和高质量标题作为视频 - 文本对进行训练。通过这样的高质量的训练数据，保障了文本和视频数据之间高度的对齐。

<!-- TOC --><a name="acknowledgements"></a>
# Acknowledgements

[GeekBang: AI LLM Practice](https://time.geekbang.org/column/intro/100770601)

![image](https://github.com/user-attachments/assets/f0d30f01-1892-4af9-804a-e3d0d41a9b48)

