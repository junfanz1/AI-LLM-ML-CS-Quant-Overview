# Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [1. Advanced RAG ](#1-advanced-rag)
- [2. Modular RAG ](#2-modular-rag)
- [3. Pre-Retrieval Optimizing Indexing](#3-pre-retrieval-optimizing-indexing)
   * [3.1 Multi-Representation Indexing for Enhanced Search Results](#31-multi-representation-indexing-for-enhanced-search-results)
   * [3.2 Self-Querying Retrieval (SQR): Automating Query Formulation](#32-self-querying-retrieval-sqr-automating-query-formulation)
   * [3.3 Parent Document Retrieval (PDR): Structuring Hierarchical Data](#33-parent-document-retrieval-pdr-structuring-hierarchical-data)
- [4. ​​Pre-Retrieval Query Optimization](#4-pre-retrieval-query-optimization)
   * [4.1 Multi-Query Techniques for Complex Information Retrieval](#41-multi-query-techniques-for-complex-information-retrieval)
   * [4.2 Query Decomposition for Better Precision](#42-query-decomposition-for-better-precision)
   * [4.3 Step-Back Prompting: Adjusting Queries Dynamically](#43-step-back-prompting-adjusting-queries-dynamically)
   * [4.4 Hypothetical Document Embeddings (HyDE): Simulating Context](#44-hypothetical-document-embeddings-hyde-simulating-context)
   * [4.5 Routing](#45-routing)
      + [4.5.1 Semantic Routing: Directing Queries Based on Intent](#451-semantic-routing-directing-queries-based-on-intent)
      + [4.5.2 Routing with LLM-Based Classifiers for Complex Tasks](#452-routing-with-llm-based-classifiers-for-complex-tasks)
- [5. Post-Retrieval Optimization](#5-post-retrieval-optimization)
   * [5.1 Using RAG-Fusion for Better Context](#51-using-rag-fusion-for-better-context)
   * [5.2 Cross-Encoder Reranking](#52-cross-encoder-reranking)
- [Acknowledgements](#acknowledgements)

<!-- TOC end -->



<!-- TOC --><a name="1-advanced-rag"></a>
# 1. Advanced RAG 

Advanced RAG builds on naive RAG by introducing improvements in retrieval quality. It adds pre-retrieval and post-retrieval strategies before and after the retrieval process.

<!-- TOC --><a name="2-modular-rag"></a>
# 2. Modular RAG 

Modular RAG surpasses the core functionalities of earlier RAG models by incorporating a diverse range of specialized modules. These modules collaborate to enhance information retrieval and processing, resulting in more detailed and accurate responses:
- Search module: This module adapts to specific situations, enabling direct searches across various data sources like search engines, databases, and knowledge graphs. It can even leverage code and query languages generated by the LLM for targeted information retrieval.
- RAG-Fusion: Addresses the limitations of traditional search methods with a multi-query strategy. It broadens user queries to include various perspectives, using parallel vector searches and smart re-ranking to discover clear and hidden information within the data.
- Memory module: Enhances the system by leveraging the LLM’s memory to guide retrieval. This creates an unbounded memory pool that aligns text more closely with data distribution through iterative self-improvement.
- Routing module: This module navigates through diverse data sources, selecting the optimal pathway for a query. It includes tasks like summarization, searching specific databases, or merging information streams from various sources.
- Predict module: Aim to reduce redundancy and noise by generating relevant and accurate context directly through the LLM.
- Task adapter module: This module customizes RAG for different downstream tasks. It automatically retrieves prompts for tasks that don’t need prior training data and builds task-specific retrievers by generating queries with a few examples.

Modular RAG stands out due to its remarkable adaptability. Unlike the fixed structures of naive and advanced RAG, it allows module substitution or reconfiguration to address specific challenges. This goes beyond its predecessors’ simple “Retrieve” and “Read” mechanisms. Here’s how Modular RAG offers unprecedented flexibility:
- Module substitution and reconfiguration: Modular RAG allows the swapping or rearranging of existing modules to address different needs, surpassing the limitations of fixed structures in naive and advanced RAG.
- Integration of new modules: New modules can be seamlessly integrated into the system, further enhancing its applicability across diverse tasks.
- Novel interaction flows: To improve the overall performance, modules can adjust how to communicate and exchange information among themselves. Examples include techniques that leverage the LLM to refine retrieval queries or replace traditional retrieval with LLM-generated content.

Modular RAG empowers flexible orchestration through techniques that allow the system to dynamically determine the necessity of retrieval based on the specific scenario, transcending the rigid retrieval process of earlier RAG architectures. Additionally, the flexible architecture facilitates integration with other technologies like fine-tuning and reinforcement learning. This can involve fine-tuning the retriever for better results, personalizing the generator’s outputs, or even collaborative fine-tuning across different modules.

<!-- TOC --><a name="3-pre-retrieval-optimizing-indexing"></a>
# 3. Pre-Retrieval Optimizing Indexing

<!-- TOC --><a name="31-multi-representation-indexing-for-enhanced-search-results"></a>
## 3.1 Multi-Representation Indexing for Enhanced Search Results
Multi-representation indexing involves creating and storing multiple representations of each document within the retrieval system. These representations can be derived from different techniques, such as:
- Textual analysis: Extracting keywords, named entities, or using topic modeling algorithms.
- Semantic embeddings: Utilizing pre-trained LLMs to capture the semantic meaning of the text.
- Visual features: Processing images or diagrams associated with the document.
<!-- TOC --><a name="32-self-querying-retrieval-sqr-automating-query-formulation"></a>
## 3.2 Self-Querying Retrieval (SQR): Automating Query Formulation

- Document representation: Word embeddings convert each document into a numerical representation. This allows for efficient comparison between documents.
- User query: The user submits a natural language query expressing their information need.
- LLM-driven retrieval: The LLM analyzes the query and the document representations. It then retrieves documents that best match the user’s intent.
- Refine and repeat: The user can refine their query or ask follow-up questions for a more focused search based on the retrieved documents.
<!-- TOC --><a name="33-parent-document-retrieval-pdr-structuring-hierarchical-data"></a>
## 3.3 Parent Document Retrieval (PDR): Structuring Hierarchical Data

Parent document retrieval (PDR) is a technique used in advanced RAG models to retrieve the full parent documents from which relevant child passages (snippets) are derived. This retrieval process improves the context available to the RAG model, leading to more comprehensive and informative responses, especially for complex or nuanced queries.

<!-- TOC --><a name="4-pre-retrieval-query-optimization"></a>
# 4. ​​Pre-Retrieval Query Optimization

<!-- TOC --><a name="41-multi-query-techniques-for-complex-information-retrieval"></a>
## 4.1 Multi-Query Techniques for Complex Information Retrieval

Multi-query helps by generating multiple variations of the original question, capturing different aspects of the user’s intent. This broadens the search and retrieves documents that might not contain the exact keywords but still hold valuable information.

<!-- TOC --><a name="42-query-decomposition-for-better-precision"></a>
## 4.2 Query Decomposition for Better Precision


Decomposition is a powerful technique that breaks down a large, intricate problem into smaller, more manageable sub-problems. By addressing these sub-problems independently, we can simplify the overall task and ultimately create a more comprehensive and accurate response.

<!-- TOC --><a name="43-step-back-prompting-adjusting-queries-dynamically"></a>
## 4.3 Step-Back Prompting: Adjusting Queries Dynamically

Step-back prompting addresses this by encouraging the model to:
- Abstract the question: Instead of directly attempting to answer, the model rephrases it into a more general, underlying question.
- Leverage broader knowledge: This reformulated question allows the model to tap into its wider knowledge base for relevant information.
- Improve answer accuracy: By understanding the core concept behind the question, the model can generate more accurate and informative responses.

Step-back prompting involves a two-stage process:
- Paraphrasing to a generic question: The model is prompted to rewrite the user’s question into a more general one. This step helps uncover the underlying concept or principle.
- Answering the reformulated question: The model generates a comprehensive answer to the original user query using the step-back question and retrieved information.
<!-- TOC --><a name="44-hypothetical-document-embeddings-hyde-simulating-context"></a>
## 4.4 Hypothetical Document Embeddings (HyDE): Simulating Context

HyDE leverages LLMs to generate hypothetical document embeddings that represent ideal documents for answering a given query. These embeddings, even though not corresponding to actual documents, capture the essence of the information needed. This allows the retrieval process to focus on documents containing relevant content, leading to more accurate and informative responses.

HyDE workflow:
- Query processing: The user submits a query.
- Hypothetical document generation: HyDE utilizes an LLM to create one or more “hypothetical documents” that address the query. These documents might not be factual or complete, but they capture the information a relevant document would contain. This generation process often involves prompting the LLM with instructions like “Write a short summary of a web page that answers the question...”.
- Embedding creation: Each generated hypothetical document is then converted into a numerical representation called an embedding. This embedding captures the semantic meaning of the document.
- Document retrieval: The system searches for existing documents in the collection whose embeddings are most similar to the hypothetical document embeddings. This process leverages vector similarity techniques.
- Response generation: The retrieved documents are fed into the RAG model’s generation stage, where they are used to create a response to the user’s query.
<!-- TOC --><a name="45-routing"></a>
## 4.5 Routing

Routing, in the context of LLMs, is the process of directing a user query to the most appropriate sub-model or prompt within the larger LLM architecture. This sub-model or prompt is likely to have been trained on a specific domain or task, allowing it to generate a more accurate and relevant response.

There are several ways to implement routing in LLMs. We will explore two common methods:
- Semantic routing: This method leverages semantic similarity between the user query and pre-defined sets of questions or prompts from different domains.
- Routing with LLM-based classifier: Here, a separate LLM classifier is trained to categorize the user query into a specific domain before routing it to the corresponding sub-model.
<!-- TOC --><a name="451-semantic-routing-directing-queries-based-on-intent"></a>
### 4.5.1 Semantic Routing: Directing Queries Based on Intent
Semantic routing is a data-driven approach that utilizes the semantic similarity between the user query and pre-defined prompts or questions from various domains. Here’s a breakdown of how it works:
- Pre-defined prompts and questions: We define sets of questions or prompts specific to each domain we want to handle. For example, we might have a set of questions related to personal finance, another for book reviews, and so on.
- Embedding user query and prompts: We use an embedding model to convert the user query and pre-defined prompts from each domain into numerical representations. These embeddings capture the semantic meaning of the text.
- Similarity calculation: We calculate the cosine similarity between the user query embedding and the embeddings of each pre-defined prompt set. Cosine similarity measures how similar two vectors are in a high-dimensional space.
- Routing based on highest similarity: The user query is routed to the domain with the highest cosine similarity. This indicates the closest semantic match between the query and the prompts from that domain.
- Prompt selection and response generation: The LLM uses the selected domain-specific prompt along with the user query to generate the final response.

<!-- TOC --><a name="452-routing-with-llm-based-classifiers-for-complex-tasks"></a>
### 4.5.2 Routing with LLM-Based Classifiers for Complex Tasks
It’s a technique that leverages an LLM to categorize user queries into predefined domains or topics. This classification then routes the query to the most appropriate LLM for generating a response.

<!-- TOC --><a name="5-post-retrieval-optimization"></a>
# 5. Post-Retrieval Optimization
- Refining retrieved information: The initial retrieval process might return a large pool of data, some of which might be marginally relevant or even misleading. Post-retrieval techniques like RAG-fusion and CrossEncoder reranking help refine this data. By analyzing the retrieved information and its relevance to the query, these techniques prioritize the most valuable data for the LLM to utilize.
- Enhanced focus and clarity: Retrieved information can be vast and complex. Post-retrieval techniques help identify the key points and ensure the LLM focuses on the most critical aspects for generating the final output. This leads to a more focused and clear response that addresses the user’s query directly.
- Improved factual accuracy: While pre-retrieval optimization plays a role in accuracy, post-retrieval techniques offer an additional layer of quality control. By analyzing the retrieved information and its coherence, these techniques can help identify and mitigate potential factual inconsistencies. This ensures the final output is grounded in reliable and accurate information.
- Reduced redundancy: Sometimes, the initial retrieval process might return duplicate information or data that conveys the same point in different ways. Post-retrieval techniques can identify and remove redundancy, ensuring the final output is concise and avoids repetition.

<!-- TOC --><a name="51-using-rag-fusion-for-better-context"></a>
## 5.1 Using RAG-Fusion for Better Context



This technique combines two models: A retriever that finds potentially relevant documents and a generative model that understands the query’s intent. RAG-Fusion leverages the strengths of both, often using a reranker to improve the final selection of documents for the generative model to process.


- Understanding the user’s intent: RAG-Fusion starts with a user query. Like RAG models, it aims to understand the true intent behind the question.
- Generating multiple queries: RAG-Fusion goes beyond a single query. It uses the original query to create multiple variations, essentially rephrasing the question from different angles. This helps capture the nuances of the user’s intent.
- Retrieval with embedding: The original and generated queries are converted into a numerical representation using embedding models. This allows for efficient searching within a document collection or knowledge base. Documents relevant to each query are retrieved.
- Reciprocal rank fusion (RRF): RAG-Fusion then employs a reciprocal rank fusion (RRF) technique. RRF assigns scores based on how well-retrieved documents match each query. Documents with high scores across multiple queries will likely be more relevant to the user’s intent.
- Fusing documents and scores: Finally, RAG-Fusion combines the retrieved documents and their corresponding scores. This provides a richer set of information that can be used to formulate a response.




<!-- TOC --><a name="52-cross-encoder-reranking"></a>
## 5.2 Cross-Encoder Reranking
Here, a separate model called a cross-encoder takes the query and each retrieved item as input. It then outputs a score indicating how well the item matches the user’s intent. This score reranks the initial list and presents the most semantically similar items at the top.

Traditional information retrieval systems often rely on simplistic similarity measures like TF-IDF to rank search results. While these methods work somewhat, they might not always capture the nuanced relationships between queries and documents. Here’s where Cross Encoder reranking comes in.

Suppose you search for “LangSmith.” A basic retrieval system might return documents mentioning “large language models” or "development tools.” While these documents are relevant, they might not provide a direct explanation of LangSmith itself.

Cross Encoder Reranking tackles this by employing a pre-trained model specifically designed to assess semantic similarity between text passages. This model, called a cross-encoder, goes beyond keyword matching to understand the deeper meaning of queries and documents.

<!-- TOC --><a name="acknowledgements"></a>
# Acknowledgements

[Educative: Advanced RAG Techniques - Choosing the Right Approach](https://www.educative.io/verify-certificate/pg03nJFvpmPgN4W0Zuxy07pVPro3h2)

<img src="https://github.com/user-attachments/assets/39a8411e-8eb9-4784-8fca-11c1b666b5fe" width="50%" height="50%">




