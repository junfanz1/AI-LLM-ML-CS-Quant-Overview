# Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [RAG](#rag)
- [LangChain技术框架](#langchain)
- [RAG Indexing 索引](#rag-indexing-)
   * [文档解析](#)
   * [分块（Chunking）策略](#chunking)
   * [嵌入（Embedding）技术](#embedding)
- [RAG向量数据库](#rag-1)
   * [向量数据库](#-1)
   * [四类](#-2)
   * [原理](#-3)
- [RAG检索](#rag-2)
   * [混合检索](#-4)
   * [重排序模型（Reranking Model）](#reranking-model)
- [Advanced RAG](#advanced-rag)
   * [检索前优化](#-5)
   * [检索优化](#-6)
   * [检索后优化](#-7)
   * [生成](#-8)
- [Modular RAG ](#modular-rag)
   * [Routing（路由）](#routing)
   * [Scheduling（调度）](#scheduling)
   * [Knowledge Guide（知识引导）](#knowledge-guide)
- [GraphRAG](#graphrag)
- [Acknowledgements:](#acknowledgements)

<!-- TOC end -->


<!-- TOC --><a name="rag"></a>
# RAG

![image](https://github.com/user-attachments/assets/0b1461f1-80b2-4b35-9358-cc91e75059eb)

- RAG 模型的核心思想在于通过检索与生成的有机结合，弥补大模型在处理领域问题和实时任务时的不足。RAG 标准流程由索引（Indexing）、检索（Retriever）和生成（Generation）三个核心阶段组成。
  - 索引阶段，通过处理多种来源多种格式的文档提取其中文本，将其切分为标准长度的文本块（chunk），并进行嵌入向量化（embedding），向量存储在向量数据库（vector database）中。
  - 检索阶段，用户输入的查询（query）被转化为向量表示，通过相似度匹配从向量数据库中检索出最相关的文本块。
  - 最后生成阶段，检索到的相关文本与原始查询共同构成提示词（Prompt），输入大语言模型（LLM），生成精确且具备上下文关联的回答。

<!-- TOC --><a name="langchain"></a>
# LangChain技术框架

- LangChain：提供用于构建 LLM RAG 的应用程序框架。
- 索引流程：使用 pypdf 对文档进行解析并提取信息；随后，采用 RecursiveCharacterTextSplitter 对文档内容进行分块（chunks）；最后，利用 bge-small-zh-v1.5 将分块内容进行向量化处理，并将生成的向量存储在 Faiss 向量库中。
- 检索流程：使用 bge-small-zh-v1.5 对用户的查询（Query）进行向量化处理；然后，通过 Faiss 向量库对查询向量和文本块向量进行相似度匹配，从而检索出与用户查询最相似的前 top-k 个文本块（chunk）。
- 生成流程：通过设定提示模板（Prompt），将用户的查询与检索到的参考文本块组合输入到 Qwen 大模型中，生成最终的 RAG 回答。

<!-- TOC --><a name="rag-indexing-"></a>
# RAG Indexing 索引

<!-- TOC --><a name=""></a>
## 文档解析
- LangChain 提供了一套功能强大的文档加载器（Document Loaders），帮助开发者轻松地将数据源中的内容加载为文档对象。LangChain 定义了 BaseLoader 类和 Document 类，其中 BaseLoader 类负责定义如何从不同数据源加载文档，而 Document 类则统一描述了不同文档类型的元数据。
- langchain_community 是 LangChain 与常用第三方库相结合的拓展库。各类开源库和企业库基于 BaseLoader 类在 langchain_community 库中扩展了不同文档类型的加载器，这些加载器被归类于 langchain_community.document_loaders 模块中。每个加载器都可以输入对应的参数，如指定文档解析编码、解析特定元素等，以及对 Document 类进行提取或检索等操作。目前，已有超过 160 种数据加载器，覆盖了本地文件、云端文件、数据库、互联网平台、Web 服务等多种数据源。


<!-- TOC --><a name="chunking"></a>
## 分块（Chunking）策略
- 选择适合特定场景的分块策略是提升 RAG 系统召回率的关键。


<!-- TOC --><a name="embedding"></a>
## 嵌入（Embedding）技术
- 负责将文本数据映射到高维向量空间中，将输入的文档片段转换为对应的嵌入向量（embedding vectors）。这些向量捕捉了文本的语义信息，并被存储在向量库（VectorStore）中，以便后续检索使用。用户查询（Query）同样通过嵌入模型的处理生成查询嵌入向量，这些向量用于在向量数据库中通过向量检索（Vector Retrieval）匹配最相似的文档片段。根据不同的场景需求，评估并选择最优的嵌入模型，以确保 RAG 的检索性能符合要求。
- 选择适合的嵌入模型时，需要综合考虑多个因素，包括特定领域的适用性、检索精度、支持的语言、文本块长度、模型大小以及检索效率等因素。同时以广泛受到认可的 MTEB（Massive Text Embedding Benchmark）和 C-MTEB（Chinese Massive Text Embedding Benchmark）榜单作为参考。
- SentenceTransformers（又名 SBERT）是一个用于训练和推理文本嵌入模型的 Python 模块，可以在 RAG 系统中计算嵌入向量。使用 SentenceTransformers 进行文本嵌入转换非常简单：只需导入模块库、加载模型，并调用 encode 方法即可。执行时，SentenceTransformers 会自动下载相应的模型库，当然也可以手动下载并指定模型库的路径。所有可用的模型都可以在 SentenceTransformers 模型库 查看，超过 8000 个发布在 Hugging Face 上的嵌入模型库可以被使用。在中文领域，智源研究院的 BGE 系列模型 是较为知名的开源嵌入模型，在 C-MTEB 上表现出色。BGE 系列目前包含 23 个嵌入模型，涵盖多种维度、多种最大 Token 数和模型大小，用户可以根据需求进行测试和使用。

<!-- TOC --><a name="rag-1"></a>
# RAG向量数据库

<!-- TOC --><a name="-1"></a>
## 向量数据库

向量数据库是一类专门为生产场景下的向量嵌入管理而构建的数据库。向量数据库的核心在于其能够基于向量之间的相似性，快速、精确地定位和检索数据。向量数据库可以广泛应用于 LLM RAG 系统、推荐系统、异常检测、计算机视觉、自然语言处理等多种 AI 产品生产场景中。

- 数据管理：向量数据库提供了易于使用的数据存储功能，如插入、删除和更新操作。与独立的向量索引工具（如 Faiss）相比，这使得向量数据的管理和维护更加简便，因为 Faiss 需要额外的工作才能与存储解决方案集成。
- 元数据存储和筛选：向量数据库能够存储与每个向量条目关联的元数据，用户可以基于这些元数据进行更细粒度的查询，从而提升查询的精确度和灵活性。
- 可扩展性：向量数据库设计旨在应对不断增长的数据量和用户需求，支持分布式和并行处理，并通过无服务器架构优化大规模场景下的成本。
- 实时更新：向量数据库通常支持实时数据更新，允许动态修改数据以确保检索结果的时效性和准确性。
- 备份与恢复：向量数据库具备完善的备份机制，能够处理数据库中所有数据的例行备份操作，确保数据的安全性与持久性。
- 生态系统集成：向量数据库能够与数据处理生态系统中的其他组件（如 ETL 管道中的 Spark、分析工具如 Tableau 和 Segment、可视化平台如 Grafana）轻松集成，从而简化数据管理工作流程。此外，它还能够无缝集成 AI 相关工具，如 LangChain、LlamaIndex 和 Cohere，进一步增强其应用潜力。

<!-- TOC --><a name="-2"></a>
## 四类
- 第一类是开源的专用向量数据库，如 Chroma、Vespa、LanceDB、Marqo、Qdrant 和 Milvus，这些数据库专门设计用于处理向量数据。
- 第二类是支持向量搜索的开源数据库，如 OpenSearch、PostgreSQL、ClickHouse 和 Cassandra，它们是常规数据库，但支持向量搜索功能。
- 第三类是商用的专用向量数据库，如 Weaviate 和 Pinecone，它们专门用于处理向量数据，但属于商业产品或通过商业许可获得源码。
- 第四类是支持向量搜索的商用数据库，如 Elasticsearch、Redis、Rockset 和 SingleStore，这些常规数据库支持向量搜索功能，同时属于商业产品或可通过商业许可获得源码。
- 对于需要快速开发和轻量化部署的项目，Chroma、Qdrant 是不错的选择。而对于追求高性能和可扩展性的企业级应用，可以考虑 Milvus/Zilliz。FAISS 是适合对性能有极致要求、不要求持久化和数据管理的场景。Weaviate、LanceDB 在处理多模态数据方面表现突出，适用于需要管理多种数据类型（如图像、文本、音频等）的 AI 应用。如果需要无缝集成现有数据库并进行向量搜索，PGVector、Elasticsearch、Redis 是理想的方案。而不希望管理基础设施的用户则可以选择 Pinecone 这样的全托管服务。

![image](https://github.com/user-attachments/assets/e674771e-2a34-4a7c-994a-39c1350d1dda)


<!-- TOC --><a name="-3"></a>
## 原理

- 原始数据首先被处理并转化为向量嵌入。这一步通过嵌入模型实现，模型利用深度学习算法提取数据的语义特征，生成适合后续处理的高维向量表示。余弦相似度主要用于文本处理和信息检索。
- 向量数据库的核心在于其高效的索引和搜索机制。为了优化查询性能，它采用了如哈希、量化和基于图形的多种算法。这些算法通过构建如层次化可导航小世界（HNSW）图、产品量化（PQ）和位置敏感哈希（LSH）等索引结构，显著提升了查询速度。这种搜索过程并非追求绝对精确，而是通过近似最近邻（ANN）算法在速度与准确性之间进行权衡，从而实现快速响应。
- 向量数据库的索引结构可以理解为一种预处理步骤，类似于为图书馆中的书籍编制索引，方便快速找到所需内容。HNSW 图通过在多层结构中将相似向量连接在一起，快速缩小搜索范围。PQ 则通过压缩高维向量，减少内存占用并加速检索，而 LSH 则通过哈希函数将相似向量聚集在一起，便于快速定位。

<!-- TOC --><a name="rag-2"></a>
# RAG检索

RAG 检索方式主要采用向量检索（Vector Search）。

<!-- TOC --><a name="-4"></a>
## 混合检索

又称融合检索 / 多路召回，是指在检索过程中同时采用多种检索方式，并将各类检索结果进行融合，从而得到最终的检索结果。混合检索的优势在于能够充分利用不同检索方式的优点，弥补各自的不足，从而提升检索的准确性和效率。使用 rank_bm25 作为 RAG 项目的关键词搜索技术。BM25 是一种强大的关键词搜索算法，通过分析词频（TF）和逆向文档频率（IDF）来评估文档与查询的相关性。具体来说，BM25 检查查询词在文档中的出现频率，以及该词在所有文档中出现的稀有程度。如果一个词在特定文档中频繁出现，但在其他文档中较少见，那么 BM25 会将该文档评为高度相关。

<!-- TOC --><a name="reranking-model"></a>
## 重排序模型（Reranking Model）

查询与每个文档块计算对应的相关性分数，并根据这些分数对文档进行重新排序，确保文档按照从最相关到最不相关的顺序排列，并返回前 top-k 个结果。目的是将混合检索的结果进行整合，并将与用户问题语义最契合的结果排在前列。即使检索算法已经能够捕捉到所有相关的结果，重排序过程依然不可或缺。它确保最符合用户意图和查询语义的结果优先展示，从而提升用户的搜索体验和结果的准确性。

- 向量检索主要依赖于全局语义相似性，通过将查询和文档映射到高维语义空间中进行匹配。然而，这种方法往往忽略了查询与文档具体内容之间的细粒度交互。重排序模型大多是基于双塔或交叉编码架构的模型，在此基础上进一步计算更精确的相关性分数，能够捕捉查询词与文档块之间更细致的相关性，从而在细节层面上提高检索精度。因此，尽管向量检索提供了有效的初步筛选，重排序模型则通过更深入的分析和排序，确保最终结果在语义和内容层面上更紧密地契合查询意图，实现了检索质量的提升。
- 优化检索结果。在 RAG 系统中，初始的检索结果通常来自于向量搜索或基于关键词的检索方法。然而，这些初始检索结果可能包含大量的冗余信息或与查询不完全相关的文档。通过重排序技术，我们可以对这些初步检索到的文档进行进一步的筛选和排序，将最相关、最重要的文档置于前列。
- 增强上下文相关性。RAG 系统依赖于检索到的文档作为生成模型的上下文。因此，上下文的质量直接影响生成的结果。重排序技术通过重新评估文档与查询的相关性，确保生成模型优先使用那些与查询最相关的文档，从而提高了生成内容的准确性和连贯性。

<!-- TOC --><a name="advanced-rag"></a>
# Advanced RAG

![image](https://github.com/user-attachments/assets/4676c110-61a9-4acc-8f4b-9ce8e032c64d)

https://arxiv.org/pdf/2312.10997

通过增加检索前、检索中以及检索后的优化策略，提高了检索的准确性和生成的关联性，特别是在复杂任务中表现更为出色。

<!-- TOC --><a name="-5"></a>
## 检索前优化
- 方法：滑动窗口方法，元数据添加，分层索引，句子窗口检索，查询重写，查询扩展，长短不一的内容向量化
- 案例：带有层次结构的 PDF 文档解析的索引：使用分层结构对 PDF 文档进行索引，捕捉不同章节（如业务、风险因素）和段落的内容，帮助系统更精确地进行检索。
- 将原问题重写为更清晰、更具体的查询：重点关注关键财务指标和股票价格，确保检索目标更加明确。


<!-- TOC --><a name="-6"></a>
## 检索优化
- 方法：动态嵌入，领域特定嵌入微调，假设文档嵌入，混合检索，小到大检索，递归块合并
- 案例：借助层次结构改进检索，返回相关的段落、表格和财务指标
- 假设文档嵌入：假设文档嵌入（Hypothetical Document Embeddings，HyDE）是一种创新的检索技术。HyDE 方法通过生成假设文档并将其向量化，以提升查询与检索结果之间的语义匹配度。当用户输入一个查询时，LLM 首先基于查询生成一个假设性答案，这个答案不一定是真实存在的文档内容，但它反映了查询的核心语义。然后，系统将该假设性答案向量化，与数据库中的向量进行匹配，寻找最接近的文档。例如，用户询问“拔除智齿需要多长时间？”，系统会生成一个假设性回答“拔智齿通常需要 30 分钟到两小时”，然后根据该假设文档进行检索，系统可能最终找到类似的真实文档，如“拔智齿的过程通常持续几分钟到 20 分钟以上”。通过假设文档，系统可以捕捉到更准确的相关文档。


<!-- TOC --><a name="-7"></a>
## 检索后优化
- 方法：提示压缩，重排序，上下文重构，内容过滤，多跳推理，知识注入
- 案例：使用问题与检索内容的相关性对检索到的内容进行重排序，通过重排序模型根据上下文的重要性、相关性评分等因素对已检索内容重新打分，以确保最相关的信息被优先处理。
- 知识注入：在检索后通过外部知识库或预定义的领域知识，增强生成的上下文内容。这种方式适用于对准确性要求较高的场景，尤其是在特定领域或技术场景下，系统需要补充额外的专业知识。

<!-- TOC --><a name="-8"></a>
## 生成
生成的回答包含了 Nvidia 和 Apple 的财务表现数据，如收入和波动性比较，以及提供了投资机会分析。明显提高了分析和生成的质量，但仍有效果提升的空间。

<!-- TOC --><a name="modular-rag"></a>
# Modular RAG 

![image](https://github.com/user-attachments/assets/5587aacc-e408-4818-b60b-5d3d25c76c72)

https://arxiv.org/pdf/2407.21059

- 打破了传统的链式结构，允许不同模块之间的灵活组合以及流程的适应性编排，提供了更高的灵活性和可扩展性，用于处理多样化的需求和复杂任务。Modular RAG 将 RAG 的过程细分为多个可优化的模块，以支持高度定制化和优化。Modular RAG 通过将 Advanced RAG 的优化策略自由组合，根据不同的应用场景定制化处理检索和生成任务，显著提升效率和效果。
- 在 Modular RAG 架构中，Orchestration（编排） 是区别于 Advanced RAG 最显著的部分，它通过自由的流程控制和决策来优化检索和生成的全流程。这一部分的核心思想是通过智能路由和调度，知识引导与推理路径来动态决定处理流程，从而在复杂场景下提升 RAG 系统的性能。

<!-- TOC --><a name="routing"></a>
## Routing（路由）

收到用户查询后，根据查询的特点和上下文，选择最合适的流程。具体来说，Routing 模块依赖于以下两部分

-  Query Analysis（查询分析）：首先，对用户的查询进行语义分析，判断其类型和难度。例如，一个直接问答式的查询可能不需要复杂的检索过程，而一个涉及多步推理的复杂问题则可能需要走更长的检索路径。
-  Pipeline Selection（管道选择）：根据查询分析的结果，Routing 模块会动态选择合适的流程（Pipeline）。比如针对简单的查询，可以仅用大模型的知识来回答，效率高。而针对需要领域知识及复杂推理的查询，系统会使用更多的检索步骤，结合外部文档及知识进行深度检索生成。


<!-- TOC --><a name="scheduling"></a>
## Scheduling（调度）

调度的作用是管理查询的执行顺序，并动态调整检索和生成步骤。

-  Query Scheduling（查询调度）：当系统接收到查询时，调度模块会判断是否需要进行检索。调度模块根据查询的重要性、上下文信息、已有生成结果的质量等多维度因素进行评估。
-  Judgment of Retrieval Needs（检索需求判断）：调度还通过特定的判断节点来确定是否需要额外检索。在某些情况下，系统可能会多次判断是否有必要执行新一轮的检索。


<!-- TOC --><a name="knowledge-guide"></a>
## Knowledge Guide（知识引导）

知识引导是结合知识图谱和推理路径来增强查询处理过程。

-  Knowledge Graph（知识图谱）：在处理复杂查询时，系统可以调用知识图谱来辅助检索。这不仅提升了检索结果的准确性，还可以通过知识图谱中的上下文关系来推导出更为精确的答案。例如，若查询涉及多个实体的关系或多个时间点，知识图谱能够提供更深层次的推理支持。
-  Reasoning Path（推理路径）：通过推理路径，系统可以设计出一条符合查询需求的推理链条，系统可以根据这一链条进行逐步地推理和检索。这在处理具有强逻辑性的问题时非常有效，例如跨多个文档的关系推理或时间序列推导。

<!-- TOC --><a name="graphrag"></a>
# GraphRAG

![image](https://github.com/user-attachments/assets/abf58d1d-8c72-479c-8e21-1c015b424468)


https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/

https://arxiv.org/pdf/2404.16130

https://neo4j.com/blog/graphrag-manifesto/ 

RAG 在使用向量检索时面临两个主要挑战
- 信息片段之间的连接能力有限：RAG 在跨越多个信息片段以获取综合见解时表现不足。例如，当需要回答一个复杂的问题，必须通过共享属性在不同信息之间建立联系时，RAG 无法有效捕捉这些关系。这限制了其在处理需要多跳推理或整合多源数据的复杂查询时的能力。
- 归纳总结能力不足：在处理大型数据集或长文档时，RAG 难以有效地归纳和总结复杂的语义概念。例如，试图从一份包含数百页的技术文档中提取关键要点，对 RAG 来说是极具挑战性的。这导致其在需要全面理解和总结复杂语义信息的场景中表现不佳。

- GraphRAG，通过利用大模型生成的知识图谱来改进 RAG 的检索部分。GraphRAG 的核心创新在于利用结构化的实体和关系信息，使检索过程更加精准和全面，特别在处理多跳问题和复杂文档分析时表现突出。通过这些改进，GraphRAG 在处理私有数据和复杂信息处理任务时，显著提升了问答性能，提供了比 RAG 更为准确和全面的答案。GraphRAG 能够通过知识图谱有效地连接不同的信息片段。例如，当一个查询需要整合来自不同部门的报告时，GraphRAG 可以识别并链接跨文档的相关实体，如关键指标、关键行动、关键事项等。这使得 RAG 不仅能够提供准确的答案，还能展示答案之间的内在联系，提供更丰富和有价值的结果。
- GraphRAG 通过构建知识图谱，将实体和实体之间的关系结构化地表示出来，克服了传统 RAG 的复杂推理局限性。
  - 提高答案准确度和完整性：精确的关系捕捉；多跳推理能力；微软的学术论文表明，GraphRAG 在回答业务复杂问题时，LLM 响应的准确度平均提升了三倍以上
  - 增强数据理解和迭代效率：直观的数据表示，知识图谱以图形方式展示数据
  - 提升可解释性和可追溯性
- GraphRAG 通过将知识图谱中的结构化数据与输入文档中的非结构化数据相结合，利用相关实体信息来增强LLM 的上下文理解。在处理用户查询时（可选结合对话历史记录），系统采用本地搜索方法，从知识图谱中识别与用户输入语义相关的一组实体。这些实体作为知识图谱的访问点，帮助提取更多相关信息，包括关联实体、关系以及文档片段。随后，系统对这些候选数据源进行优先级排序与筛选，以适应单个上下文窗口的预定义大小，从而为用户生成准确的查询响应。


<!-- TOC --><a name="acknowledgements"></a>
# Acknowledgements:

> [RAG快速开发实战](https://time.geekbang.org/column/intro/100804101?tab=catalog)

![image](https://github.com/user-attachments/assets/94a49f33-139b-4840-bcca-db07fe28bf65)
