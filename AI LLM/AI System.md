<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [Flowise零代码搭建 LLM 应用平台](#flowise-llm-)
- [AI推荐系统架构](#ai)
   * [策略建模](#)
   * [AI系统特征工程](#ai-1)
   * [模型工厂](#-1)
   * [数据算法](#-2)
   * [系统构建，在线模型的训练流程](#-3)
- [多智能体博弈](#-4)
- [NLP](#nlp)
- [GPT](#gpt)
- [Transformer](#transformer)
- [AI大模型系统](#ai-2)
- [斯坦福小镇](#-5)
- [知识表征](#-6)
- [模型工程](#-7)
- [Stanford Alpaca](#stanford-alpaca)
- [工业级AI大模型系统在线制备流程](#ai-3)
- [DeepSeek](#deepseek)
- [面试题](#-8)

<!-- TOC end -->

<!-- TOC --><a name="flowise-llm-"></a>
# Flowise零代码搭建 LLM 应用平台

```bash
$ npm install -g flowise
$ npx flowise start --FLOWISE_USERNAME=user --FLOWISE_PASSWORD=1234
```

- Folder with Files 组件，负责将相关知识文档上传。
- Recursive Character Text Splitter 组件，用来给上传的文档内容做断句切片。
- OpenAI Embeddings 组件负责将断句后的内容切片映射成高维 Embedding。
- In-Memory Vector Store 组件用来将 Embedding 存入数据库中，供给 LLM 作为外部记忆。
- Conversational Retrieval QA Chain  组件则会根据问题，获得外部知识，在 LLM 思考形成回答后返回给用户问题答案。
![image](https://github.com/user-attachments/assets/d243cf3b-e90d-4e18-b4bf-984f1819837c)

真正的领域微调需要基于定制化的模型，使用高性能的训练框架进行大规模分布式训练，结合强化学习和 MoE（混合专家模型）。因为在商业系统中，绝对不允许出现差错，对模型性能有严格的要求。

<!-- TOC --><a name="ai"></a>
# AI推荐系统架构

<!-- TOC --><a name=""></a>
## 策略建模
AIRC 系统
- 策略建模
- 数据工程
- 模型工程

电商系统通常把 GMV  指标作为北极星指标。为了提升这个指标，算法工程师们会想尽办法，在用户进入产品时把他最可能购买的商品放在最显眼的位置。电商系统的本质是对商品进行排序，优先展示排序靠前的内容。将用户对每个商品的期望收益进行排序，将排名靠前的商品，展示给用户，早期的 AIRC 系统采用的就是这种方式。
要深度模型的建模，对于这么大的数据规模，在线实时打分排序是不现实的，所以，AIRC 系统往往会拆分为召回和排序两个模块，这是在线系统中常用的效率权衡方法。
- 召回模块的核心目的，就是用时间复杂度较低的算法排除大量的“错误答案”，减少排序算法的压力。比如搜索引擎，常常会通过搜索词中的关键字，在倒排索引中拉取内容，大幅度地降低排序规模，让用户更快得到搜索结果。这个方法叫做关键词召回，它只是众多召回策略里最简单的一种。常见的召回策略还包括年龄、性别这类用户画像召回方法，只要对内容对应的标签建立倒排索引就可以了。选择一个好的召回策略，通常需要对业务有深入的洞察，针对业务平台的属性来设计特定的策略，比如基于性别的召回策略，就利用了类似“女性不太会买渔具，男性不太会买假睫毛”这类生活经验。在召回阶段，我们需要放弃某些低潜力的商品，所以所有的召回策略，都可能导致最优结果的流失。还是沿用性别例子：如果夫妻共用一个账号，性别策略很可能会起到反作用。所以后来发展出了向量召回技术，该技术基于对比学习和图神经网络，刻画用户和商品之间的空间距离，以此作为召回顺序的依据。
- 排序模块负责对召回结果进行精确的打分排序。点击行为的预测，这类任务当中，常见的有 Wide&Deep、DeepCross 等模型，你可以暂且把它当成预测概率的黑盒。
- 召回和排序是性能上的一个取舍，这种取舍在排序的内部会再一次出现。为了满足发展需求，排序阶段也会进一步拆分为粗排、精排和重排。一般来说，较前阶段用到的排序算法要更快，相应的模型也更简单。
- 面对快速、大量的流量请求，我们如何准确地控制分布呢？这类问题是控制论算法的范畴，最好的方法就是用以 PID 和强化学习为代表的控制算法。控制类算法是 AIRC 系统当中的重要部分。
- 博弈场景：广告业务上会分成，流量卖方 SSP 系统和流量买方系统 DSP 两大部分，卖方的目标是帮媒体最大化流量变现收益。而买方系统，则是要帮广告主低价买到适合自己的流量。博弈论中的拍卖算法。在这个场景下，卖方往往会制定拍卖策略，根据估价给出“拍卖底价”，不让买方“捡漏”。这样呢，买方才会给出较高的报价。
- 工业级的系统最大的特点是要考虑线上风险。你需要和陌生对手，甚至自己的恶意用户，去进行竞争博弈。这也就催生出了风控模块。风控模块一般包含内容安全系统和反作弊系统。

<!-- TOC --><a name="ai-1"></a>
## AI系统特征工程
- 特征处理的过程是对数据进行微观和宏观投影的过程，所以虽然叫特征处理，但特征本身其实没有变化，变的只是你观察的维度。
- 最简单的方法就是保证它们之间在数值上是正交的，确保每个值只占据空间中的一个维度，简而言之就是用各个维度的单位向量表示一个分类，这种编码方式叫独热编码（one-hot encoding）。将上述语料中所有的单词做独热编码，映射到高维空间中，得到单词的高维向量表达。不过，这时独热编码所投影出来的全部单词之间，都是正交的关系。所以接下来，我们需要用相邻单词之间的字面距离，来描述它们的空间关系。这里使用了对比学习的方法，利用了“你的基友 A 和基友 B 之间更相似的”这类假设，去训练一个模型，来刻画单词之间的相似度。那你可能会问，该如何去衡量，语料中词与词的关系呢？这里有两种方法，第一种是跳字模型 Skip-gram，第二种是连续词袋模型 CBOW（Continuous Bag of Words）。

<!-- TOC --><a name="-1"></a>
## 模型工厂

Google 的 Wide&Deep 模型，这是点击率预测的经典模型。

模型由宽度和深度两部分组成。宽度部分用于处理与“是否点击”有直接联系的特征，原理与传统的逻辑回归相同。深度部分则可以更好地利用那些看似与标签无关，但组合起来会很有用的特征，是一个多层的神经网络（MLP）。本质上，它是将宽度和深度两个模型融合成了一个模型，来发挥各自的优势。
![image](https://github.com/user-attachments/assets/f68d8c3b-d11d-422b-bba8-b27c5bc08370)

损失函数 L 是模型给出的“预测”和标准“答案”之间的差距，损失越小则说明模型效果越好。所以“解方程”的过程就是要“试”出损失最小的“解”。工业场景中我们会使用更复杂的损失函数，比如交叉熵，它可以对预测值和真实值之间的分布做相似性的预估。
梯度下降来求解的整个过程是这样的。首先，我们从需要拟合的已知数据中取一小批数据（X，Y），并将它们带入损失函数。然后，随机给损失函数赋予一个解 W。接下来，计算梯度，梯度会指向这批数据上将损失函数减小的最小解的方向。之后，沿着这个方向迈出一步，也就是调整你的模型参数 W。最后，我们再取一批数据，基于更新后的解重复这个过程，直到用完所有已知数据。最终，你将得到一个针对你的已知数据拟合后的点击率模型。这个模型可以用来预测未知数据的点击率。

推荐系统：用图结构来表示用户和物品的关系，构建它们之间的关系图，用户和物品是图的节点，它们之间的交互行为是图的边，边的权重则是它们交互行为的频率。
DeepWalk 算法的核心思想，我们来看看具体的建模过程。首先随机选择一个初始节点，根据与它相连边的权重分配概率，进行节点之间的随机跳转，也就是所谓的随机游走。随机游走的过程中会生成许多路径，这个路径则是你想要的序列。得到序列后，自然可以使用 Word2Vec 建模，唯一不同的是对象由单词变成了用户和商品。
只是一种权宜之计，最终还是会发展成端到端的模型，因为分阶段的方法通常会导致信息的丢失。所以，下节课我还会带你学习一种端到端的图神经网络算法——GraphSAGE。

控制论中的经典算法，比如 PID 控制器，该控制器由比例、积分和微分这三个部分构成。
- 比例部分反映了偏差与目标之间的比例关系，能在偏差产生时立即提供反馈，实现有效的控制。
- 积分部分表示偏差随时间的累积情况，主要用于消除静态误差，例如在误差积累较长时间时，逐渐增加修正幅度。
- 微分部分则捕捉偏差信号的变化趋势，特别用于快速扩大偏差时，可以及时抑制其进一步放大的趋势。

应用在 AI 系统中的控流场景，这里我们以短视频应用为例。短视频的 AI 系统，通常会限制每个视频在 24 小时内的流量，以避免某些视频过度占据流量，挤压中小创作者的发展空间。因此，当某个视频的曝光量达到一定值后，就会被限制流量。不过，短视频 AI 系统还有一个目标——让各种使用习惯的人都能接触到优质内容。所以，平台还需要确保内容在一天中各时刻的流量均匀分布。这样就能避免在某个时段，比如下午使用产品的用户看不到优质内容。在这种场景下，就很适合使用 PID 控制器来控制流量。

当收到流量请求的时候，我们要先算出实际曝光量和预期曝光量的差距值，把这个差距值和之前几个时刻的差距一起传给 PID 控制器，并将计算出输出值作为对当前视频的调控反馈。如果输出值越高，那该视频在这次请求中，被限制的概率就越大。通过这个机制可以动态地调整当前视频的曝光量，因为这完美地利用了 PID 控制器的特点——“风浪越大，控流越强烈”。翻译一下就是真实流量和预期流量间的误差越大，误差增长越快，误差积累时间越长，AI 系统就会越强力地限制流量。这样即使面对突然出现的流量洪峰，也能做到动态控制。

PID 算法也存在一些局限性，比如调整加权参数通常需要高昂的实验成本，尤其是在在线商业系统中。因此，目前各大厂正在逐渐切换到强化学习的方案上来。强化学习不依赖于预先标记的数据，而是通过与环境的交互来进行自主学习，根据奖励信号的反馈进行实时的策略更新


<!-- TOC --><a name="-2"></a>
## 数据算法
系统需要两种能力：第一种是对数据做“身份对齐”的能力，比如对同一人在多设备、多账号产生的数据做身份识别。第二种则是挖掘某个人群的“潜在用户”的能力，比如识别符合某个品牌产品调性的用户。

GraphSAGE
![image](https://github.com/user-attachments/assets/b2a45085-29c0-4265-9671-27b576a46cf6)


<!-- TOC --><a name="-3"></a>
## 系统构建，在线模型的训练流程
全量模型训练
- 先看第一步数据拼接，拼接的过程因模型而异，例如 CTR 模型需要将内容曝光和内容点击的日志拼接起来，生成是否点击的预测样本；而向量召回算法则需要基于用户、物品以及它们之间的交互数据进行建图，供给后续的算法生成空间投影模型。
- 在完成了数据拼接之后，第二步则是针对模型的特点来进行特征投影
- 使用样本数据开始进行模型训练。这里可以使用常见的模型训练框架，如 PyTorch 和 TensorFlow 来训练得到模型文件。
- 在得到模型文件之后，则要完成模型的离线验证，这也是模型上线前的最后一步。我们通常会选择最近 N 天的真实样本数据来评估模型的各项指标（N 的大小取决于模型的重要程度），判断是否达到准入门槛。如果达到要求，则进行上线。
- 最终上线时，我们只需要更换并重新加载模型服务

增量模型训练（和 AI 大模型的微调大同小异）
- 样本进行多级分类，通过“三级火箭”的方式发布上线。第一级火箭使用全量（例如过去一年）的样本。第二级火箭是模型的短期增量训练，使用第一级训练出的模型和最近一天的样本数据，训练一个最新的二级模型。二级模型通常每天训练一次，来更新前一天的二级模型。第三级火箭指的是，把在二级模型部署到线上后，24 小时内产生的增量数据，实时地喂给在线模型进行训练和更新，来确保在线模型的实效性。
![image](https://github.com/user-attachments/assets/6b075013-a0a8-443f-9dcc-807d09d124da)

数据存储索引Memory&Index
- 在线倒排索引：ElasticSearch。增量索引是一个技术难点。通常需要通过对诸如 MySQL 的增量二进制日志进行繁琐地解析和处理才能完成。
- 基于语义相关性的向量召回（embedding based retrieval）。在这里，你可以运用前几节课学到的知识，将用户和物品进行联合建模，得到高维投影函数，然后将用户和物品投影到高维空间，获得它们在高维空间的坐标，也就是 Embedding。

我们已经成功准备了数据、索引和模型。可以利用这些离线素材构建一个相当不错的内容推荐引擎了。通过使用前两节课学到的图神经网络，即可对用户和物品进行高维映射，并将它们存入向量引擎中。只要投影模型质量良好，当收到客户端请求时，你完全可以使用用户向量来召回空间距离相近的物品，然后从中选择其中之一返回，以获得良好的推荐效果。

排序服务
- 每个因子背后都使用了一个甚至多个模型来满足打分的需求，比如上面的排序分中涉及到了“XX 率”预测模型和控制因子用到的 PID 控制器方法。所以这本质上是一个混合专家系统（Mixture of Experts, MoE），该系统通过不同的专家模型解决各个领域的任务，共同实现业务目标。

![image](https://github.com/user-attachments/assets/c4ce7e99-5eea-461d-9b2f-8248b34d3451)

<!-- TOC --><a name="-4"></a>
# 多智能体博弈
- 集成到 ChatGPT 上的应用都是参与博弈的智能体，所有生态应用都会想尽办法让 ChatGPT 选它，所以它们会给出自己的“最优方案”，这意味着各个应用需要考虑其他应用的行为，才能在激烈的竞争中占据上风。
- 实时竞价问题RealTime Bidding
- 市场竞价预估Bid Landscape Forecasting


<!-- TOC --><a name="nlp"></a>
# NLP
- ResNet 的解题思路是怎样的呢？它通过设计特殊的直通通道，引入了许多旁路直接连接到后续层。这就相当于让领导和员工直接见面，避免传话过程中信息的丢失。这个设计让模型可以绕过深层网络中的梯度爆炸和消失问题，更好地学习复杂的图像特征。
- RNN 引入了循环结构，如下图所示，在 RNN 中，每个单元都具有一个隐藏状态来存储之前的信息，并将其传递给下一个节点。
- 在处理较长序列时，RNN 会展开成多层，导致同一权重参数在各个单元中复制。在反向传播过程中，这些参数的梯度可能在复制时成倍地增长或减少，最终甚至出现梯度爆炸或消失的问题。LSTM 是一种特殊的 RNN，其内部包含三个关键的门控制单元：遗忘门、输入门和输出门。这些“门”控制了信息的流动和保留，使网络能够有选择地记住或遗忘相关信息，从而有效解决了梯度消失和爆炸问题。
- “Seq2Seq”（Sequence-to-Sequence）任务，并为此提出编码器 - 解码器的架构。
- Attention 就是在做这个事情，它允许后面的人询问（Query）他前面所有人知道的内容（Key & Value），甚至后面的人还能知道之前哪个人说的话对他来说是“更重要的”，是不是感觉这太过“作弊”了？没错，注意力机制的思路和原先的循环结构相比，简直是开了挂，这正是它的强大之处。具体来说，类似于在 LSTM 中我们引入 Ct​ 这条记忆链路，使得模型能够更好地建模长距离依赖的关系。如下图所示，Attention 则是为解码器阶段的每个单元，单独准备了一个自己的 C，它会基于当前单元内容和输入之间的关系进行“私人定制”，让解码器的每个单元都能获得定制化的全局信息。
- Word2Vec找到了一种无监督学习的方法，对大规模的语料库进行预训练，学习其中的语义，这绕开了对大规模有标签数据集的依赖，打破了 NLP 预训练数据上的困境。Word2Vec 也有明显的短板，比如无法处理词在不同语境中的多义性。举例来说，“苹果”可以指水果，也可以指手机。Word2Vec 无法理解这些差异，可能导致产生错误的结果。怎么帮助模型消除歧义呢？
- ElMo（Embeddings from Language Models），它是 Google AI 在 2018 年提出的一项技术。ElMo 是一种基于上下文生成词向量的方法，能够通过综合考虑“前到后”和“后到前”两个序列单词的含义，动态调整生成的词向量，让词语的表示变得更加准确。尽管 ELMo 使用双层双向 LSTM 网络结构进行语言模型训练，但这两个双向模型是完全独立训练的，最终只是将它们的输出连接在一起，不是一个端到端的方法，这可能会导致一些信息的丢失

<!-- TOC --><a name="gpt"></a>
# GPT
- GPT 系列一直采用了 Decoder Only 的 Transformer 架构,提出了一个两阶段的方法：首先，在第一阶段使用一个庞大的语言模型进行预训练；然后，在第二阶段，使用特定下游任务的数据集做模型微调。
- BERT 与 GPT-1 在预训练过程中有明显差异。BERT 采用了类似 ELMo 的双向语言模型，同时利用上文和下文信息进行建模预测。BERT 在模型和方法方面的创新主要体现在 Masked 语言模型和 Next Sentence Prediction。Masked 语言模型使用掩码标记替换部分单词，并要求模型预测这些被替换的单词。Next Sentence Prediction 则要求模型判断两个句子之间的关系。因为在预训练阶段中同时使用这些技巧和各类任务的数据，所以  BERT  具备了多任务学习的能力。BERT 系列从一开始就采用了 Encoder Only 的 Transformer 架构，这一架构能够同时利用上文和下文信息，为 BERT 带来了显著的性能提升。所以它在各类 NLP 任务上表现出色，赢得了工业界的广泛认同，取得了巨大的成功，将 NLP 预训练模型技术推向了不可或缺的地位。
- GPT-1 在训练过程中则会根据海量数据，统筹考量不同上下文语境下生成的文本在语法和语义上的合理性，来动态调整不同词的生成概率，这个能力主要由它所使用的 12 个 Transformer 共同提供。
- GPT-2 零样本学习
- GPT-3 in-Context Learning。“少样本学习”（Few-Shot Learning）的概念。这和传统意义上模型微调的“少样本学习”是不一样的。GPT-3 所提出的方式是，允许下游使用者通过提示词（prompt）直接把下游任务样本输入到模型中，让模型在提示语中学习新样本的模式和规律，这种方法的学名叫做 in-context learning。
- ChatGpt 排序打分Learning to Rank：对偏序关系进行建模，借助偏序关系的传递性，进一步对全局的偏序关系建模。你可以借助这个方法的损失函数直观理解它的原理。
![image](https://github.com/user-attachments/assets/48ef5ff9-361c-4286-b5fd-0e5a7499eff8)
其中，yw​ 代表排序排在 yl​ 的所有句子。损失函数的值等于排序列表中所有排在前面项的 reward 的总和，减去排在后面项的 reward 的总和。这其实就是好答案和坏答案之间的距离。因此，我们只要最小化损失函数，就可以得到更好的排序结果。
- 随着模型规模的增长，刚开始模型效果下降，但当模型规模足够大时，效果反而会提升的现象称为“U 形曲线效应”。因为如果把参数规模作为横坐标，把模型性能作为纵坐标，那么就会出现一个 U 型曲线。在包含多种子任务，而且模型规模较大的情况下，就会经常出现“U 型曲线效应”。


<!-- TOC --><a name="transformer"></a>
# Transformer
- Transformer 会并行地处理所有输入的内容，所以各个并行单元会无状态地处理每个输入。因此，我们需要在最开始就给每个输入的嵌入向量一个位置编号，这样模型才能通过输入判断它在整体中的位置。
- 自注意力机制会针对每个带有位置编码的输入向量，去计算和其他位置的关联程度，从而捕捉输入内部的上下文关联信息，形成一个注意力权重的分布作为后续层的输入，指导模型的学习过程。自注意力机制的主要目标，则是确定输入词与词之间的内部关联性。比方说，在 “道可道非常道” 这句话中。我们希望 Transformer 可以通过自注意力机制，学到句中“非常道”指的是原句中的第一个“道”字而不是第三个。这种机制可以让 Transformer 学习理解这句话中更多隐含的信息和深意。
- 观测多个平行宇宙中舰队的航行情况的能力，正是 Transformer 中的多头注意力（Multi-Head Attention）模块所具备的。Transformer 模型会使用多组同构的自注意力（Self-Attention）模块，并行学习出多组不同的权重，每组权重表示了它根据输入信息所学习的不同自注意力权重。最终通过将多组自注意力计算的结果拼接在一起，通过线性变换得到多头自注意力模块的联合输出。总之啊，多头注意力机制类似于赛马机制，它有助于减少模型初始化的随机性对模型效果的影响。所以即使只留下一个注意力头也能使用，但这会导致模型的稳定性和多样性无法得到保障，进而造成模型的性能下降。
- 授予各个舰队一定的权限，允许它们跨级沟通，以提高信息传递的效率。实际上，这就是之前我们学过的残差结构。在 Transformer 的每个子层中，都使用了残差连接和层归一化来稳定训练过程。
- 集体智慧决策的方式，通过设立总参谋部，分层汇总各个平行宇宙的舰队情况。这就是 Transformer 中多层感知机，也就是 MLP 的作用了。从下图中你可以看出，在 Transformer 中，每个“头”都会输出到 MLP 中进一步的汇总信息，用来增加 Transformer 模型的拟合能力，提升决策的效率和效果。
- 舰队的升空启航，进入宇宙空间。这就好像 Transformer 模型中把文本输入数据转化成了高维空间向量的过程。随后，为了确保每艘舰船在航行中，能够准确地报告自己的位置，并且进行有效的通讯，我们为每位成员分配了独一无二的编号，这也是 Transformer 中位置编码的作用。随着太空军司令的一声令下，舰船们各自启航，寻找目标。这实际上就是自注意力机制所承担的任务，它负责在茫茫的宇宙中寻找目标。同时，为了提高寻找过程的效率，太空军司令还拥有了一种“超能力”，那就是窥探平行宇宙中，各种可能的寻找路径。这就是多头注意力机制的巧妙之处，它可以极大地加速寻找的过程。最后，司令汇总了所有舰船的探索结果，通过加权汇总的方式确定了目标星球的位置，就像 MLP 层在模型中的作用一样。

![image](https://github.com/user-attachments/assets/28f3a4b8-2328-4d51-88f9-0a1596b37175)


<!-- TOC --><a name="ai-2"></a>
# AI大模型系统
- 现在工业界在 AIGC 系统的链路就是：检索 -> 生成 -> 检索的形态。两次检索的目标是不同的。第一次检索的输入是用户问题，目的是为大语言模型提供外部记忆；而第二次检索的输入则是生成内容，目的是为生成的内容提供引用信息，增加生成内容的可信度，此外，我们还可以通过这次检索，来优选生成模型输出的多个结果。
- 排序：GlobalE&LocalE（投其所好）我们可以根据 ACL 22 的这篇杰出论文 “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity” 提出的方法，使用熵来决定示例的排序。简单来说，我们可以对一个示例样本的序列，计算它概率的熵值。熵值越大，就表示该序列的质量越高。使用他们提出的全局熵（Global Entropy: GlobalE）和局部熵（Local Entropy: LocalE），跟随机排序的方法相比，可以分别平均提高 13% 和 9.6% 的性能。这种方法相较于“就近示例”方法更加复杂，但在理论上具有更强的可解释性，更适用于对稳定性要求较高的工业级系统。
- Automatic Prompt Engineer（自问自答）APE 方法的主要思想是，根据用户要完成任务的具体内容，自动生成一个更适合大语言模型的提示词模板，主要包含以下几个步骤：首先，你需要提供示例，包括任务描述和任务示例，一般来说任务示例是一组输入输出对。大语言模型会根据提供的示例，生成一些备选的“提示语模板”。之后，你需要使用大语言模型作为打分模型，给每个提示语模板打分，选出分数最高的提示语模板作为这个任务的模板。最后，是改进模板，这里要使用迭代蒙特卡洛搜索的方法，针对刚刚生成好的提示语模板，生成语义相似的指令变体，改进最佳模板。这个改进的模板，就是我们用来完成最终任务的提示语模板了。APE 方法可以提高模型的回答质量，将提示语工程的工作自动化掉，提高用户产品体验。这样用户在和大语言模型对话的时候，只要给出自己要完成的任务，APE 就会自动帮他选择更合适的沟通方式来与大语言模型对话了。
- Self-Consistency Sampling（一题多解）大语言模型通常会在解码器的每个时间步，选择概率最高的词汇作为输出。这种策略在只有一次回答机会的情况下是非常有效的。但是，如果我们希望模型在回答某个问题时，充分考虑他所学过的所有知识，这种方法就不太适用了。为了优化这种方法，Google 提出了自一致采样（Self-Consistency Sampling）的方法。自一致采样方法包括以下三个步骤。第一步，使用思维链（CoT）提示语言模型，分步骤地解决给定问题。第二步，从语言模型的解码器中随机采样，生成一组不同的推理路径。第三步， 在众多最终答案中，选择最一致的答案，作为最终的推理结果。这个思想，和我们在数学考试中，通过“一题多解”来验证答案的正确性，其实是一样的。
- Generated Knowledge Prompting 反刍知识。在大语言模型回答复杂问题的时候，往往需要用到涌现能力，而涌现能力呢，对模型的参数量有很大的需求，这会花费大量的算力成本。所以，为了不滥用模型的“涌现”能力，在一些情况下，我们可以先帮助模型，去唤醒一些它的“深度记忆”，让它知道该用这些学过的知识来，回答问题。需要注意的是，这里的“记忆”是指模型在训练过程中获得的记忆，而不是提示词提供的外部记忆。这个方法的具体步骤是先让大语言模型生成一些，跟问题内容相关的“知识”，然后再把这个知识做为输入示例样本反哺给大语言模型，辅助回答目标问题。
- Self-Ask 本质上是通过给大语言模型几个的示例样本（Few-Shot），帮助模型理解在什么情况下，它需要向外界索要更多的外部知识。这种通过大语言模型自我判断的，多轮思考方法的步数不是特别可控，所以该方法在聊天对话类大模型应用中不太常见。不过，在具身智能和多智能体博弈的场景则经常会用 Self-Ask 和它的各类变种来实现智能体的自治。

<!-- TOC --><a name="-5"></a>
# 斯坦福小镇
- 记忆流这个构建具身智能的关键方法。记忆流可以用于保存智能体经历的记忆事件列表，其中包括自然语言描述和时间戳等信息。记忆流的检索过程类似于人类的回忆过程，会利用时近性、重要性和相关性等因素来计算记忆对象的得分，然后选择与当前情境最相关的记忆。通过记忆流可以帮助智能体理解和适应复杂的情境。通过回顾过去的经历，智能体可以更好地理解当前的情境，并做出更合理的决策。同时提高智能体的性能，更有效地管理记忆信息。记忆流的本质是通过类似“注意力”机制的方法，大幅提升智能体的回忆效率，帮助智能体存储和检索经历过的记忆事件。
- 针对记忆流回忆能力的局限性，论文中引入了第二种类型的记忆，这种记忆的名字叫反思（reflection）。反思区别于观察事件记忆，它是一种更高层次的认知能力，包括对经验的分析、评价和总结。不断把反思存储在记忆流中，并建立反思与相关记忆的连接，便可以逐渐生成一颗智能体的反思树。树的叶子节点代表观察事件记忆，而非叶子节点代表反思记忆，随着节点高度的增加，它所代表的思维，也变得更加抽象和高级。本质上也是一种记忆聚合的方式，是重要的提示语压缩方法。
- 规划能力，它描述了智能体未来的行为序列，这个行为序列可以帮助它在时间维度上保持行为的一致性。一个规划包含地点、开始时间和持续时间。智能体规划的方法可以分为两步，分别是全天规划和细节规划。与反思一样，计划也存储在智能体的记忆流中，这允许智能体在回忆时同时参考观察、反思和计划的记忆。智能体可以根据需要随时更改其计划。
- 斯坦福研究人员利用观察、记忆、反思、计划构成的思维闭环，在 AI 小镇中开展了社会实验。在实验中，他们观察到了涌现出的智能体之间的“社会行为”，例如信息传播、关系形成和协作。具体而言，作者测量了两条特定信息在游戏中的传播情况。在模拟开始前，知道 Sam 参选市长的智能体的比例为 4%，知道 Isabella 举办情人节派对的智能体比例为 4%。在模拟结束后，这两个比例分别增长到 32% 和 48%。这表明，在没有用户干预的情况下，智能体能够将信息传播给其他智能体。为了衡量关系的形成情况，作者还测量了智能体是否知道其他智能体的存在。在模拟开始前，智能体之间的关系密度为 0.167。在模拟结束后，关系密度增长到 0.74。这表明，在模拟过程中，智能体之间形成了新的联系。此外，作者还在 Isabella 组织的情人节派对中研究了智能体协作的情况。在派对开始前，Isabella 邀请宾客、收集物料并寻求帮助以装修咖啡馆。在情人节这天，12 个受邀的智能体中有 5 个出现在了派对上。这表明，智能体能够在集体活动中相互协作。
- Smallville 仿真游戏环境是一个虚拟世界，使用了 Phaser 框架构建。这个环境包括了视觉环境、地图和碰撞信息。生成式 AI 智能体可以访问这些信息，并通过后端服务器与仿真环境交互。仿真世界中的每个智能体的信息都被存储在后端服务中的一个 JSON 数据结构中。该数据结构包含智能体的 ID、位置、方向、状态和记忆等信息。在每个仿真时间步骤中，仿真服务器都会解析 JSON 数据结构，检查生成式 AI 智能体是否有任何数据更新，如果有则同步到仿真环境中。智能体的自主行为由记忆驱动，在最早期赋予它的记忆叫做种子记忆，后续随行为产生的记忆会随着智能体在仿真世界中的经验变化而不断更新。
```bash
$ git clone https://github.com/joonspk-research/generative_agents.git
$ pip install -r requirements.txt
```
- 通过对比使用不同提示语工程的智能体在竞争中呈现的“能力”，来判定你的提示语工程方法是否更优秀。你甚至可以用类似 AutoML 的思想，让你的提示语引擎自动进化。

<!-- TOC --><a name="-6"></a>
# 知识表征
- 嵌入表征方法，和我们前面学过的 Work2Vec，BERT 以及各类的图神经网络算法等等没有区别。只不过 BERT 模型在计算两个句子的语义相似度时，需要将它们同时按下面的形式输入到模型。这种方式叫做 Cross-Encoder。不过这种两两比对的推理开销，在真实的业务环境中几乎是不能接受的。因此，在外部记忆表征检索中用到的嵌入表征方法通常是指 sentence-bert（SBERT） 这类投影表征方法。SBERT 本质上也是通过我们学过的对比学习方法，用相似句子的表征向量之间的距离更近这个原则，来微调 BERT 模型。

最大内积搜索 (MIPS)
- 向量索引是一个可以用于快速检索相似向量的存储引擎，其中最有名的当属早期工业界广泛使用的 Faiss
- 向量检索的根本目标是找到与查询向量最相似的 K 个向量（KNN），或者找到距离查询量小于某个距离阈值的所有向量（RNN）。但工业场景中数据量往往很大，如果采用精确最近邻算法，会导致检索效率太低。所以我们通常选择牺牲一定准确性来提高速度，采用近似最近邻算法（ANN）。
- Faiss 中就使用了 PQ 算法，PQ 算法是一种近似最近邻算法，它将高维数据映射到低维空间进行检索。PQ 算法的基本思想是将原始向量空间分解为多个低维向量空间的笛卡尔积，然后对每个低维向量空间进行量化。这样，每个原始向量就可以由多个低维量化编码表示。
- 向量数据库没什么神秘的了，只不过是通过 ANN 的方法在检索数据库的一些相似向量而已。其中一个挑战是超大规模索引的精度和效率问题。随着数据量的不断增加，在亿级、甚至十亿级时，向量索引的构建和检索成本也会随之增加，这可能会影响向量检索的性能和准确性。另一个挑战是高维数据的处理问题。随着维度的增加，向量检索的计算复杂度也会呈指数级增长，这可能会导致查询效率下降、存储成本升高。此外，向量检索还面临分布式数据分片构建和快速合并检索、流式索引的在线更新、多路召回等等挑战。
- 阿里巴巴开源的 HA3 ，实现一个工业级知识系统

<!-- TOC --><a name="-7"></a>
# 模型工程
- 怎么低成本获取大量的指令微调数据。Self-Instruct 是一种数据增强方法，其目标是减少对人工标注人员的依赖。总的来说，Self-Instruct 从一组初始示例样本开始，以 LLM 的自我引导方式生成新的指令和示例。
![image](https://github.com/user-attachments/assets/d46bca84-cfd4-4efc-a2ff-5d9550b6cb71)

- Alpaca 是 LLaMA-7B 的微调版本，它采用了 Self-Instruct 方式生成的数据进行了指令微调。具体来说，它利用了 OpenAI 的 text-davinc-003 模型，用 175 对人工标注的种子数据，构建了 52K 条指令微调数据，使用 OpenAI API 的成本仅仅不到 500 美元。斯坦福的研究团队基于 LoRA 训练框架和 Llama 开源预训练模型，对这些生成增强数据进行了微调训练，得到了 Alpaca 模型，事实表明通过这种方法得到的模型，可以在指定任务上达到 SOTA 的效果。
- 用了 LoRA（低秩适应）技术复制了 Alpaca 的结果。LoRA 的实现思想很直观：我们首先冻结一个预训练模型的矩阵参数，然后选择使用 A 和 B 矩阵来代替这些参数。在下游任务的训练中，我们只对 A 和 B 进行更新即可。该方法会在原始的预训练模型右侧添加一个侧通道，进行降维和升维的操作，以模拟内在维度的概念。在训练的过程中，需要保持预训练模型的参数不变，只对降维矩阵 A 和升维矩阵 B 进行训练。模型的输入输出维度保持不变，在输出时，将 BA 矩阵与预训练的参数相叠加即可。这里我想提醒你注意的是，我们需要使用随机高斯分布来初始化矩阵 A，同时使用零矩阵初始化矩阵 B，这样可以确保在训练开始时，这个侧通道矩阵是一个零矩阵。
- LoRA 通过把原模型的权重拆成小块矩阵，然后只训练这些小块，而不动原始权重。这个方法极大地减少了计算和内存的开销，让微调变得更划算。
- LoRA 技术并不仅仅适用于大语言模型，它可以应用在深度模型的各个模块，通过减少可训练参数的数量来提高效率。举个例子，比如在 Transformer 模型中的在 Self-attention（自关注） 模块中通常包含四个权重矩阵（wq、wk、wv、wo），而在 MLP 模块（多层的神经网络）中通常包含两个权重矩阵。LoRA 技术允许将适应下游任务的注意力权重限制在自关注 Self-attention 模块中，并冻结 MLP 模块，以简化和提高参数效率。有了 LoRA 技术的加持，我们在训练大规模深度学习模型时，就可以明显地降低 GPU 的资源开销。

<!-- TOC --><a name="stanford-alpaca"></a>
# Stanford Alpaca
Alpaca 的开源实现与工业界的需求紧密契合，可以说达到了工业级的入门标准
- 数据生成过程：Alpaca 对 self-instruct（自我教导）数据生成的方式进行了升级，提高了效率，降低了花费。这些升级包括用 text-davinci-003（代替 davinci）来生成指令数据，并且创建了一个新的提示词模版 prompt.txt。此外，他还采用了更大胆的批量解码方式，每一次会生成 20 条指令，大幅度降低了生成数据的费用。同时，数据生成流程也变得更简单了，取消了分类指令和非分类指令之间的差异，每个指令只生成一个实例，而不再生成 2 到 3 个实例。这些优化成功生成了包含 52000 个实例的指令数据集，而成本只有不到 500 美元。初步研究还显示，与之前的 self-instruct 发布的数据相比，这样生成的这 52000 条数据更加多样化。
- 全量参数训练（需要 8 张 A100 的显卡）：用上一步中生成的数据，来训练 Llama 模型。最终生成的数据存储在 alpaca_data.json 文件中，其中的数据集包含了 52000 条指令，每个指令由以下字段组成。
  - instruction：一个字符串，描述模型应该执行的任务，这 52000 条指令都是独一无二的。
  - input：是一个可选的字段，用来表示指令的可选上下文或输入。大约 40% 的示例包含这个字段，例如，当指令是 “总结以下文章” 时，该字段的输入则是文章原文内容。
  - output：一个字符串，由 text-davinci-003 生成的、作为指令数据的输出。
- LoRA 低成本训练，搭建起你自己的专属领域大模型
  - 除了训练代码，该项目还发布了已微调好的模型和 LoRA 权重以及推理的脚本。为了让微调的过程节省资源并且高效，该项目使用了 HuggingFace 的 PEFT 和 Tim Dettmers 的 bitsandbytes 来优化性能。
  - 模型训练代码的核心部分，这里我们挑主要的代码逻辑讲解。我们使用了 Hugging Face Transformers 库中的 LlamaForCausalLM 类来加载基础模型，这里通过 base_model 参数来设置。接着，我们为 LoRA 方法创建配置，然后把它传到模型的设置中。然后，这里的代码检查了是否有恢复训练的检查点。如果有，它会加载已有的模型权重。然后，这里创建了一个 Hugging Face Transformers 的 Trainer 对象，用于管理和执行训练过程，其中设置了训练参数、数据集和数据收集器。最后，这里开始模型的训练，如果有提供的 checkpoint（检查点），则从该检查点恢复训练。可以直接使用 generate.py 从 Hugging Face 模型库读取基础模型和 LoRA 权重，然后运行一个 Gradio 接口，用于对指定输入进行推理。

<!-- TOC --><a name="ai-3"></a>
# 工业级AI大模型系统在线制备流程
- 样本制备：数据增强的过程主要涉及到第 25 节课中学到的 self-instruct 工作的各种变种，比如 alpaca。我们可以利用这些方法来发挥中小型公司的“后发优势”，缩小自家大语言模型和其他先进模型之间的差距。
- 模型制备：LoRA+MoE（Mixture of Experts）技术，有效降低参数量。这样，在某些小型的专家模型上，我们仍然可以在局部使用三级火箭的方式来确保更新的实时性。
- 提示语引擎
  - 工具的存储和使用：第一部分与 OpenAI 的 function 功能类似。它为生态用户提供了一种方法，使他们能够将其能力接入到我们的平台中，以便大语言模型能够准确调用这些能力，通常的实现方法类似于传统微服务的注册中心。第二部分是基于向量表示的 API Bank，它类似于 OpenAI 提供的插件功能。一旦大语言模型根据语义相似性查找到了相关的插件向量，识别到用户的聊天意图与这些插件相关，则会组合使用这些插件来满足用户的需求。
  - 示例的存储，我们知道示例是 ICL 的最本质特征，它可以帮助大模型通过 few-shot 理解下游任务场景是 AIGC 最重要的优势之一。所以我们需要有一个高质量的示例库，这个示例库一般来说是分领域设计的，每个领域示例库中会首先由领域专家来进行设计，生成一些针对领域特定问题的种子示例。随后，用这些种子示例，配合大语言模型，从无标签的领域数据中解析出更多的高质量示例，补充到领域示例库中。


<!-- TOC --><a name="deepseek"></a>
# DeepSeek
- DeepSeek V3 这一系列优化策略的核心在于“稀疏激活”机制，它不仅减少了单次推理所需的计算量，还赋予了大模型更强的适应性和可扩展性。相比传统稠密模型，混合专家模型的动态路由机制使计算资源的使用更加灵活，使得开发者可以在有限的计算资源下，仍然能够运行参数量更大的模型，不必因计算成本或硬件限制而受制于推理效率的下降。混合专家模型通过门控网络实现动态路由，使每个输入 token 仅调用最相关的专家进行计算，减少了推理时的资源消耗。这种“稀疏激活”机制不仅降低了显存占用，还提升了特定任务的适应能力和泛化效果。DeepSeek V3 进一步优化了这一机制，在门控网络中引入可学习的偏置项，避免计算负载集中在少数专家上，实现更均衡的计算资源分配，提升整体推理效率。
- 混合专家模型的优势：与其让一个超大的稠密网络对所有 token 做统一计算，不如把网络拆分为多个“专家模块”，通过门控机制在推理过程中只启用与该 token 最相关的模块。这样就能实现“稀疏激活”：大模型整体依然保持巨量参数，但每一次推理需要真正访问和计算的参数规模被极大缩减。DeepSeek V3 正是这一思路的典型应用。
- DeepSeek V3 的每一层，都存在多个专家节点，其中包含两大类：路由专家（图中蓝色）和共享专家（图中绿色）。路由专家的主要功能是根据输入 token 的特征来选择激活的参数。具体而言，当一个 token 进入该层时，会先经过一个门控机制（Gate），通过计算与不同路由专家的分数，选出得分最高的若干个专家进行计算，这种方式也被称作 Top-K 路由。由于每次只激活部分路由专家，模型虽然规模庞大，但单次推理不必激活所有参数，单卡显存上限要求和计算开销因此大幅降低。之所以能够实现“专才导向”的稀疏激活，是因为不同路由专家通常针对各自擅长的领域或功能进行了优化，比如自然语言理解、数学推理、代码生成等。当 token 被分配到合适的专家节点后，会在该专家完成相应的计算，随后再与全局网络进行融合。这样，每个 token 只需依托最适合它的专家，而非从头到尾激活全量参数，就达到了稠密模型推理类似的目的。
- DeepSeek V3 还设置了一部分共享专家 (Shared Experts)，以获取和整合不同上下文或任务场景下的共通知识。这些共享专家在模型内部相当于“公共模块”，帮助减少那些原本可能重复出现在路由专家中的冗余参数。通过把共享知识集中到这些共享专家中，既能提升模型对广泛背景信息的适应能力，也能进一步节省路由专家间的重复开销。如果我们把传统稠密模型比作“所有人都排队到同一个大厅去办理各种业务”，MoE 架构就像是给大厅划分出若干独立的“专业柜台”，并设立了一个“门控调度中心”指导每位来访者去最合适的柜台办理业务。这样既能提高办理效率，也节省了资源，使模型能够在相同或有限的硬件条件下容纳更多参数，继续享受大规模带来的性能提升。
- 如何确保专家节点的合理利用，避免计算负载的极端倾斜，是混合专家模型落地应用时必须解决的问题。DeepSeek V3 通过在门控网络中引入可学习的偏置项，使训练过程能够自适应地调整专家的选择概率，优化 token 在推理阶段的分配方式。具体来说，在模型训练时，门控网络会学习每个专家的工作负载，并通过引入轻微的偏置，让某些使用率较低的专家获得更多机会被选中参与计算，从而避免所有计算任务集中到少数几个专家上。这种方法不仅能够平衡各个专家的计算压力，还能在长期训练过程中，让所有专家节点都能获得充分的参数优化，确保它们在实际推理任务中都能发挥作用。
- 后训练指的是在模型已经拥有基础语言和推理能力后，再通过针对性的数据和任务，强化或微调模型的表现。目标是把预训练阶段意外找到的有效提示语，变成模型在任何场景下都能稳定触发的“技能”。为此，研究人员会利用多样化的提示语工程方法，构造出针对性的训练数据，让模型不断练习如何在不同任务与上下文中应用这些提示语。例如，输入端可能是用户的指令，输出端则是模型通过若干工具使用步骤组合出来的思维过程，这些真实或模拟的场景有助于模型学会灵活激活相关推理知识。此外，专家反馈在这个过程中也起到了至关重要的作用。每当模型在较难的推理任务上出现偏差或不稳定，研究团队都会分析其原因并再次进行针对性调整。随着不断迭代和打磨，模型与提示语之间的“默契”便逐渐形成，一句简单的“Let’s think step by step”也能在关键时刻引导模型沿着正确的推理路径前进。
- 预训练：为模型打下语言和初步推理的基础，并偶然获得一些提示语。后训练：有针对性地放大和固化这些提示语的优势，使其在复杂任务中依旧稳定高效。只有将这两个阶段紧密结合，才能让提示语从一次“偶然发现”真正成为大幅提升推理表现的“关键武器”。
- DeepSeek V3 对后训练阶段的重点调整之一是指令微调对齐。为达成这一目标，需要收集大量真实场景下的指令数据，涵盖问答、任务规划、信息检索等多个领域。每条指令背后都隐藏着用户的真实意图。经过专家的严格标注和校验后，模型在微调时不仅仅停留于“字面匹配”，还能深入到更深层次的语义理解。在具体的微调过程中，DeepSeek 采用了类似多任务联合训练的方法。具体而言，每一类“好用”的提示词可以视为一种任务，DeeSeek 针对这些高频的任务生成了大量的微调数据，确保模型能够在面对不同任务时，自动提炼出共同的语义特征和推理路径。实时反馈机制则确保了模型在每次回答出现偏差时，都能被系统记录并在后续训练中得到修正。
- 指令微调对齐后，DeepSeek V3 更进一步，通过强化学习（RL）的方式实现模型在持续交互中的“自我进化”。我们为模型设计了一套细致的奖励体系，对答案的逻辑性、准确性以及与指令契合度进行打分——输出多步推理、逻辑清晰的答案得高分，偏离用户意图或不连贯的回答则被“扣分”。在这个过程中，模型被放置在一个模拟的真实场景环境中，与虚拟用户进行多轮交互。每一次交互结束后，系统都会根据任务完成情况和用户反馈，对模型进行即时评估。这样一来，模型就可以通过不断的试错，逐渐优化自己的推理路径和响应策略。
- DeepSeek V3 使用了一项关键技术 GRPO 来进一步强化模型的自我进化能力。GRPO，其实可以看作是对传统强化学习对齐方法 RLHF 的一次重大升级。它不仅在策略更新过程中加入了奖励平滑与调控机制，更注重从长远反馈中提取信息，让模型在不断试错的过程中保持稳定性和高效性。
- RLHF 方法，也就是 OpenAI-o1 Policy 采用的方案。在这个方法里，模型要经过多个环节优化。用户输入一个问题后，策略模型（Policy） 会先给出一个回答，这个回答会被参考模型（Reference Model） 和奖励模型（Reward Model） 评估，然后再通过额外训练的价值模型（Value Model） 进一步优化答案。虽然这种方式能提升回答质量，但问题在于，价值模型本身需要单独训练，这不仅增加了计算成本，而且优化流程变得更复杂。而 DeepSeek V3 采用的是 GRPO 方法，它的做法就简洁多了！它的策略是：让模型一次性生成多个候选答案（比如 Output 1、Output 2……Output N），然后让冻结的参考模型（REF Model） 和奖励模型（Reward Model） 直接对这些答案进行评估，最终选出最优答案。这样一来，不需要额外训练价值模型，整个优化流程轻量化了，计算开销也减少了不少！通过 GRPO，DeepSeek V3 不用训练额外的价值模型，但却能让模型更稳定、更聪明，在动态交互过程中不断提升推理能力。
![image](https://github.com/user-attachments/assets/c32469db-5285-4ed9-8272-cb9618ed9509)

DeepSeek V3 的两个重要对齐机制
- 首先是指令微调对齐，通过收集海量真实场景的指令数据，再经过专家的严格标注，让模型不仅能够捕捉到指令的表层含义，更能深刻理解用户真实意图。然后，我们讲到了强化学习对齐，这里核心在于通过设计精细的奖励体系，鼓励模型输出逻辑严谨、推理完整的答案。
- 最终，DeepSeek V3 在这套多重对齐机制的加持下，不仅在静态数据层面实现了对指令的深层理解，也在动态交互中不断优化自身表现。那些看似简单的提示语，也从一次“偶然发现”蜕变成了模型推理的强大武器，让语言模型的能力得到进一步解放。
参数扩展与数据扩展同样重要，两者相辅相成，才能让语言模型从“会用语言”进化到“深度理解与推理”。

DeepSeek V3 做了哪些工程化优化。
三大改良：RoPE、MTP、MLA 这三项改良，实际上对应了大模型在长文本处理、生成速度和注意力计算成本上最核心的痛点。RoPE 让模型在处理极长序列时不至于出现严重的梯度衰减，并为推理过程中的 KV Cache 提供了更高效的配合。MTP 有效地缩短了生成文本时的等待周期，提升了应用场景中用户对大模型的交互体验。MLA 则在不牺牲太多模型性能的情况下，显著降低了多头注意力的计算和内存消耗，为大规模模型落地创造了更友好的环境。
- MLA：多头隐式注意力，降低大模型算力开销
  - DeepSeek V3 的 MLA 思路就相对折中些。它直接给注意力过程加了一道中间环节，也就是让模型先学到一组紧凑的隐式向量。这些隐式向量可以看作是对原始 Key/Value 的可学习压缩。换句话说，模型并不再为序列中的每个 token 都生成一份完整的 Key/Value，然后再两两交互，而是把那些信息聚合到一块小巧的潜在空间里，再在这里执行多头注意力的计算。
  - LoRA 的低秩空间表示，这里是同样的道理，我们用一个隐空间配合线性变换还原出完整的 MHA，是不是非常像？这样一来，在 Prefill 和 Decode 阶段，就不需要面对每个 token 都和所有其他 token 进行全连式的注意力运算，也不用在内存里保存非常庞大的 K/V，对于推理尤其重要的 KV Cache，也能轻装上阵。虽然在直觉上，这会让人担心会不会丢失一些精细的信息，但事实上，MLA 的核心就是让那部分“压缩”是可学习的，并且依然保留多头机制。这意味着不同 Head 还是可以去关注隐式向量里不同的维度或者特征，从而在保留丰富上下文的同时，大大降低长序列带来的算力和存储负担。
  - MLA 让我们看到了另一种可能——与其在 n² 上做大量减法，不如在序列表示形成的时候就把 Key/Value 学习得更高效、更紧凑，把注意力范围局限在潜在空间里，不再死板地让每个 token 都要跟所有其他 token 做全交互。更可喜的是，这种思路还保留了多头的灵活性，保留了深度学习可端到端训练的属性，也就为日后继续迭代改进留下了很大空间。从 MHA、GQA 到 MQA，再到 MLA，这条演化脉络让我们看到一个趋势：优化多头注意力，最根本的点还是减少重复存储和大规模交互，让模型学会用更紧凑、更聪明的方式表达上下文依赖。
![image](https://github.com/user-attachments/assets/e249fe33-5a02-4e62-82e7-9f82231be728)

- RoPE：让长文本位置编码更灵活、更有效
  - Transformer 之所以能够并行处理序列数据，是因为它的自注意力机制无需像 RNN 那样逐步读入。然而，这种并行特性也带来了一个问题：模型需要“显式”知道每个 token 在序列中的位置，否则就无法判断先后顺序。最早的 Transformer（“Attention Is All You Need”论文）通过正余弦绝对位置编码来解决这个问题。在固定长度的序列里，这种编码表现尚可，但当序列大幅拉长，尤其在超过训练长度时，绝对位置编码经常出现梯度消失和外推性不足等问题。
  - DeepSeek V3 采用了 RoPE（Rotary Position Embedding）。它基于旋转矩阵的思想，将位置信息融入词向量时，不再只关注“词在句子中的绝对位置”，而是更注重“词与词之间的相对位置”。这样一来，模型在处理超长文本时，其内部的注意力计算不容易随着序列变长而出现梯度大幅衰减，还能更好地把握远距离的语义关联。RoPE 和 MLA 的融合体现在键的生成过程中。如下图所示，RoPE 被应用于键的一部分，而另一部分键则与潜在键值缓存相关联。通过这种方式，位置信息和缓存机制得以协同工作。RoPE 确保了模型能够理解长文本中的位置关系，而 MLA 则通过缓存减少了计算量，提升了效率。
- MTP：一次性多 token 预测，缓解自回归生成的时间“黑洞”
  - 自回归“逐步预测”，虽然能保证文本连贯与准确度，但会拉长整体生成时间。无论是企业级应用还是用户在线聊天，都迫切希望输出速度能更快、更即时。
  - DeepSeek V3 引入了多 token 预测（MTP），让模型在一次前向传播中可以同时输出多个后续 token，从而大幅减少了迭代次数。同样，MTP 并不是 DeepSeek V3 独家原创。此前，业界也有一些并行预测或分块生成的方法，试图让模型一次性输出更多 token。不过，DeepSeek V3 的贡献在于更深入地将并行预测与自回归语言模型的训练目标结合起来，让模型在一次解码循环中就能取得尽可能高的准确度和连贯性。MTP 想做的就是：不再“一次只拿一个词”，而是在一次前向传播里，直接预测出多个后续 token。这样对于相同长度的文本，我们就可以显著减少推理迭代次数，换句话说，少循环几百次乃至上千次，对在线服务而言就是实打实的效率提升。
  - DeepSeek V3 的方法通过将多 token 预测融入损失函数，帮助模型在生成多个 token 时，仍能保持语言的一致性与流畅性。同时，如何平衡输出 token 的数量，也是一个至关重要的策略。如果一次性输出太多 token，错误或偏差可能会被放大，影响结果的整体质量；如果输出的 token 太少，又无法有效提高推理效率。因此，模型需要根据任务场景和硬件资源，在速度和生成质量之间找到最佳平衡。
  - 实际应用中，可以通过结合投机采样算法进行优化：模型先生成多个候选 token 作为草稿，然后通过后续的验证过程，筛选出符合质量标准的前缀。这样不仅能够提升生成效率，还能在确保质量的同时最大化加速推理过程。
![image](https://github.com/user-attachments/assets/7c574442-7660-412b-b727-835d78a5552a)

<!-- TOC --><a name="-8"></a>
# 面试题
构建一个程序，支持根据文档中出现的内容，搜索本地所有文本文档，你觉得应该分为哪几个步骤？（提示：倒排索引）
- 通过系统 API 遍历所有本地文本文件，遍历过程中使用 restfulAPI 写入 < 文件地址，全文 > 到 elasticsearch 构建倒排索引。通过 restfulAPI 调用 elasticsearch 接口查询倒排，获得文件地址列表。
让程序猜测你现在要使用哪个文件，把排名前十的文件推荐给你，要求将时延控制在 100ms 内，你会怎么做？（提示：多路召回）
- 开发一个后台进程订阅系统日志，解析文件使用时段，文件使用时系统活跃进程和当天是星期几，分别对其建立倒排索引（使用时段 / 活跃进程 / 星期几三路召回）。在用户打开程序时立刻获取当前所处时段，系统活跃进程和当天是星期几，随后触发各个并行查询，对各个倒排索引查询，查询时每路只返回前 N 个文件即可；最后对多路返回结果根据文件最后修改时间进行归并排序。
独热编码是如何处理分类特征的？为什么需要进行正交的空间投影？高维空间刻画特征距离的意义和作用。
- 对于有 N 个不同取值的分类特征，独热编码会创建一个长度为 N 的二进制向量，其中只有一个位置为 1，表示该分类特征的取值。在没有得到特征之间的彼此关系时，让特征之间正交可以消除相互影响，避免模型得到错误信息。富含语义关系的空间中的各个特征更接近于它们在真实世界之间的关系，模型可以利用这些信息更好地完成任务。
预训练模型和大模型之间的关系是什么？
- 预训练模型（pre-training model）首先通过一批语料进行训练，然后在这个初步训练好的模型基础上，再继续训练或者另作他用。为了最大化模型复用的效果，往往使用参数量较大的模型作为预训练模型的网络结构。
由于 RLHF 只是单纯地根据已有的回答进行排序，是否会出现“自己吃自己”的循环，也就是用来训练模型的数据来自于模型自己生成的，“近亲繁殖”训练的模型水平是否会受影响？
- 会受影响。因此，OpenAI 一直没有停止使用 SFT 进行增量训练。
RLHF 和 SFT 的关系是什么？
- SFT 后的模型可以生成多个答案作为 RLHF 的输入，RLHF 使回答更符合人的预期。其中更重要的是 SFT，因为如果没有 SFT，RLHF 将成为无源之水。
提示语工程只是自说自话的试错游戏，你觉得他说得对吗？
- GPT-2 以来就一直具备的一种能力，那就是通过与用户交互和提示语进行上下文学习（In-context Learning）。在 GPT-2 中，提示语主要被用来向模型传递“指令”信息，使模型了解其正在进行的任务。而在 GPT-3 之后，提示语的内容逐渐演变成为了包含“示例”和“指令”的形式。其中，“示例”部分负责为模型提供任务场景的相关样例，帮助其掌握其中的规律；而“指令”部分被沿用下来，目的仍然是让模型明确自己的任务目标。因此，提示语工程的本质其实是一种试图充分利用大语言模型上下文学习能力的方法。通过最佳的格式和最有效的示例及指令为模型提供指导，使其能更好地理解和解决给定问题。在这个过程中，不仅需要考虑具体任务数据集的特性，而且也需要关注大语言模型本身的行为和表现。为此，我们会运用一系列基于统计或基于监督学习的提示语工程技术，以提高模型的性能表现。所以提示语工程并不是自说自话的试错游戏，而是非常依赖于实际数据和模型特性的数据驱动工作。
- 提示语工程的本质，还是（领域）知识注入。好的提示词工程如果没有富含增量信息的知识作为外部记忆注入，如果没有提供与增量信息相匹配的推理逻辑，那就是没有注入知识。那就是一个死循环，或者说瞎尝试。面对 AI，拼的还是人类自身的知识 / 经验的积累 + 对 LLM 的 ICL 特点的深刻理解。
Alpaca 增强数据微调模型的上限是什么？
- Alpaca 或者说 self-instruct 的各种变种，本质上是在做模型对齐，如果你使用 GPT-4 来生成增强语料，则是在对齐目标任务领域上，你的模型与 GPT-4 之间的能力，所以上限就是你所选择对齐的那个模型。
LoRA 微调的过程中，可能会存在哪些问题？
- 可能存在 LoRA 的参数空间过大，无法完全测试，所以要引入一些超参数搜索的方法，比如 AutoML 的策略。
通过 AutoML 的方法自动化 LoRA 的调参过程
- https://arxiv.org/pdf/2305.16597v1
LoRA 方法和向量检索中的经典 ANN 算法 PQ 之间有何联系？
- 两个方法都是低维映射的矩阵来近似替代原矩阵，来提高工作效率。

![image](https://github.com/user-attachments/assets/862e0a90-09c2-4bca-9fed-97faa20fd2fd)


